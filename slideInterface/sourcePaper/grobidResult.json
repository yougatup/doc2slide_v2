{"authors": "Hyungyu Shin; Eun-Young Ko; Joseph Jay Williams; Juho Kim", "pub_date": "", "title": "Understanding the Effect of In-Video Prompting on Learners and Instructors", "abstract": "Online instructional videos are ubiquitous, but it is difficult for instructors to gauge learners' experience and their level of comprehension or confusion regarding the lecture video. Moreover, learners watching the videos may become disengaged or fail to reflect and construct their own understanding. This paper explores instructor and learner perceptions of invideo prompting where learners answer reflective questions while watching videos. We conducted two studies with crowd workers to understand the effect of prompting in general, and the effect of different prompting strategies on both learners and instructors. Results show that some learners found prompts to be useful checkpoints for reflection, while others found them distracting. Instructors reported the collected responses to be generally more specific than what they have usually collected. Also, different prompting strategies had different effects on the learning experience and the usefulness of responses as feedback.", "sections": [{"heading": "INTRODUCTION", "text": "In online learning environments, an important task for instructors is to inspect learners' level of comprehension and learning experience regarding their lecture videos. Such inspection enables instructors to gain insights into what to teach and how to teach in future instruction. Online learning platforms such as Coursera and edX collect data from multiple sources and provide dashboard interfaces for instructors to support this task. Available data streams include video clickstream logs, responses to in-video quizzes, submissions for assignments and exams, platform interaction data, discussion forum posts, and course reviews. Figure 1: An example of in-video prompting supported by a video learning interface. Once the playhead reaches the red bar, the prompt appears. This example asks a question about the learner's comprehension on a specific part of the video. While common, it's not the only possible prompting type. This paper explores the design space of in-video prompting, and presents how learners perceive the general idea of in-video prompting and different prompting designs.\nResearch shows that learner-generated data such as artifacts in discussion forums and learner surveys are important in monitoring online courses [20] and preparing future iterations of the course [27]. Discussion forums and review websites are popular communication channels where learners share their comprehension and evaluation of the course. Through discussion forums, instructors can check how learners understand concepts and correct misconceptions by intervening in discussions among learners [15]. Review websites address various dimensions of a course, such as the level of difficulty, quality of contents, and instruction delivery. From these channels, instructors can assess how learners perceive their material and course design, which could serve as useful feedback.\nHowever, existing channels for collecting learners' responses and input are insufficient as a source of feedback. Posts on discussion forums could give insights into the level of comprehension, but only a small portion of learners participate in the discussion forums [3]. Posts on review websites capture subjective learning experiences, but their granularity is at the course level, which is not specific enough for instructors to identify which specific parts of the lecture need to be improved.\nTo address these challenges, we investigate in-video prompting as a channel for collecting learners' feedback on lecture Table 1: The design space of in-video prompting questions, segmented by comprehension-experience orientation and the level of specificity. Comprehension-centered/Experience-centered questions ask about learners' comprehension and learning experience. General/Specific questions determine whether the questions refer to lecture content or not. Table 1a shows example questions for each prompting strategy, and Table 1b shows sample responses for each prompting condition.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Comprehension-centered Experience-centered", "text": "General Describe what you have learned so far.\nDescribe something unsatisfying about the lecture so far.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Specific", "text": "Describe how to calculate the standard deviation. You may assume you want to calculate the standard deviation of five numbers.\nDescribe something unsatisfying about calculating the standard deviation. You can consider how clear the explanation was, how fast the explanation was, and what information was missing.\n(a) Sample questions for each prompting condition.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Comprehension-centered Experience-centered", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "General", "text": "Population standard deviation and how it is calculated and what it means if it is a larger number.\nThe instructor's handwriting is a little bad.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Specific", "text": "The standard deviation is calculated by taking the square root of the variance.\nExplanation of change from squared units to units was slightly too fast.\n(b) Sample responses to each question presented in Table 1a, submitted by crowd workers in our study.\nvideos while minimizing disruption in learners' experience.\nIn-video prompting presents reflective questions to learners while they are watching the video, in order to get specific comments on their level of comprehension or learning experience. Figure 1 shows an example video learning interface equipped with in-video prompting. Unique properties of invideo prompting are that (1) all learners who watch the video encounter the prompts, which is likely to yield a high response rate, (2) prompting at an inopportune time might distract learners from learning, and (3) since the prompts are given in the middle of learners' video watching session, they can ask specific questions to learners about the material just covered in the video.\nLittle research has investigated the effect of in-video prompting and its design space. A common type of in-video prompting is in-video quizzes [13], which are commonly multiple choice questions or short-answer questions that pop up during playback to maintain engagement and check understanding. But the types of possible in-video prompts are not limited to quizzes. For example, they could allow a more detailed insight into learners' interpretation of video content. Also, carefully designed prompts could enhance learning, as research shows that reflective prompts improve learning outcomes [5,24].\nThe design of questions used for prompting is important in determining the type and quality of comments that instructors collect and students' learning experience. Depending on what goal the instructor wishes to achieve with in-video prompting, questions could focus on either revealing learners' level of comprehension, or understanding their subjective learning experience. As the learning experience is affected by the questions, we also need to understand how learners perceive in-video prompting. To understand the effect of invideo prompting on both learners and instructors, we pose the following research questions:\n\u2022 RQ1. What are learners' perceptions of in-video prompting?\n\u2022 RQ2. How do different in-video prompting strategies affect the learning experience?\n\u2022 RQ3. How useful are learner responses to in-video prompts as feedback to instructors?\nWe focus on two dimensions in exploring the design space of in-video prompting: the type of information collected from learners, and the specificity of the question. Table 1a illustrates the combinations of these two dimensions (the comprehensionexperience orientation and the level of specificity) with example questions. The comprehension-experience orientation represents which information learners need to submit: comprehension-centered questions ask about the level of comprehension, while experience-centered questions ask about the learning experience. The level of specificity represents whether the questions refer to lecture content or not. General questions prompt for reflection on the lecture content without referencing specific context, whereas specific questions ask about the lecture content in detail. Table 1b shows examples of learner responses for each prompting questions.\nTo answer the three research questions, we conducted a series of studies and interviews. To understand the learners' perspective, we conducted two studies with crowd workers to explore both the effect of prompting in general (RQ1) and the The contributions of this paper are as follows:\n\u2022 Results from a study on learners' perception of in-video prompting, organized into a list of pros and cons.\n\u2022 Results from a study on learners' perception of four different prompting strategies, which span general versus specific, and comprehension-centered versus experience-centered.\n\u2022 Results from interviews with instructors and instructional designers about the usefulness of learner responses as feedback on lecture videos.\nThe rest of paper is organized as follows. First, we survey related work. Second, we present instructor perspectives on learner feedback on lectures. Third, we present two studies designed to understand how in-video prompting affects learners. Fourth, we report results from interviews with instructors and instructor designers to understand how learner responses to in-video prompting serve as feedback. Finally, we conclude with the discussion of the effect of in-video prompting.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "RELATED WORK", "text": "In-video prompting involves promoting learner reflection and providing feedback to instructors. We briefly review prior research on these topics, such as methods for collecting feedback in both offline and online settings and the effect of reflective prompting on learners.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Utilizing Learners' Interaction Data", "text": "Existing online learning platforms have been collecting learners' interaction data in both passive and active ways. Clickstream and in-video dropout data are passively collected in that the data is naturally collected regardless of learners' intention.\nAlthough research has investigated presenting [11] and analyzing passive data [4,26], the conclusions that can be drawn are limited as feedback because users' true intentions behind traces are unknown. Forum posts are actively collected in that learners explicitly give the data to the platforms. Agrawal et al.\n[1] and Wen et al. [22] have investigated analyzing discussion forum posts to get meaningful insights from them. The granularity of forum posts is at the course-level, which makes it hard for instructors to figure out which specific parts of the lecture need to be improved. Singh et al. [19] and Lee et al. [14] facilitate discussion among learners within a lecture video.\nAlthough instructors can get more specific learner comments, it has limitation as feedback in that instructors cannot control the types of learner comments. In this paper, we explore invideo prompting, which yields specific and controlled learner comments, as a channel for collecting more useful feedback.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Providing Feedback on Online Lecture Videos", "text": "Vidcrit [17] supports asynchronous video review and sharing feedback on videos, but is designed for reviewers who indicate problems or offer suggestions, not for learners. Mudslide [9] attempts to collect spatially contextualized 'muddy' points in a video with specific explanations, which enables instructors to figure out the common points of confusion with reasons.\nWe see Mudslide as a case of in-video prompting, which uses spatial anchoring at the end of the video as its prompt. In this work, we aim to understand the effect of in-video prompting broadly by exploring two specific design dimensions described in Table 1a.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Collecting Qualitative Comments in Offline Classroom", "text": "In a physical classroom, classroom assessment techniques (CAT) can be used to collect qualitative comments [8]. A technique described in CAT is called One-Minute paper, which asks questions to students at the end of the class. The questions include \"What are the most important concepts you have learned today?\", and \"What are the most confusing points?\".\nIt not only provides students with a better learning experience [21] and higher scores on tests for some cases [7], but also enables instructors to improve teaching with formative evaluation. Our work seeks to explore the design space of invideo prompting and to help design effective online prompts that provide benefits to both learners and instructors like the One-Minute paper.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Effect of Reflective Prompting on Students", "text": "Many studies have demonstrated that students can learn more when they are prompted to explain the meaning of what they are learning. For example, prompting students to explain what they understand from biology texts enhances the accuracy of students' mental models about the circulatory system, arguably by helping students generate inferences and spot gaps in their understanding [5]. Williams et al. [24] suggest that explaining why a fact is true drives learners to discover underlying patterns or principles. On the other hand, there are many known cases where prompts to reflect do not enhance learning, and many more that are likely unreported. There are even cases where prompts to reflect can hurt learning, by causing learners to overgeneralize [25], ignore details [23], or rely on incorrect prior knowledge rather than observed facts [6]. These contradictory finding underscore the importance of exploring the design space of how prompts to reflect impact learners.\nIn particular, it is important to understand the strengths and weaknesses of different kinds of prompting strategies, and how these are perceived by learners.\nIn the context of learning online video, platforms like Coursera allow instructors to insert in-video multiple choice quizzes. While there have been studies of how they affects learners' video navigation [13], there is less evidence about the causal impact on learning. In addition, in-video quizzes must be designed for the specific content of a video, while work on reflective prompting tends to use general prompts that aren't tied to specific content. There has been relatively little research on the effects of adding general prompts to online videos, as existing work focuses more on different formats of video presentation [16]. In addition, educational studies of prompting have focused more on learning outcomes than on learners' subjective experiences. And as most studies of prompting have been conducted in laboratory settings or physical classrooms, little is known about what instructors might learn by being able to rapidly view students' responses to prompts. In this work, we attempt to fill this gap in literature.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "INSTRUCTOR PERSPECTIVES ON LEARNER FEEDBACK ON LECTURE VIDEOS", "text": "To better understand instructors' perspectives on learner feedback on lecture videos, we conducted interviews with three instructors on campus and a web-based survey with five MOOC instructors outside of campus. We aimed to understand (1) what learner feedback instructors collect in their current practice, and (2) how useful learner feedback is in improving instruction. Among the interviewees, one instructor had experience in teaching a MOOC, and two instructors had experience in the flipped classroom method. Each interview session took about an hour, and the expected completion time for the survey was 15 minutes. We summarize the main findings below.\nLack of learner comments. Instructors reported having few learner comments to work with in the first place. In the interview, the MOOC instructor said there were approximately 10 questions per each video. The flipped classroom instructors pointed out that they do not typically ask for feedback because students tend to passively consume the lecture video and they did not expect students to be willing to provide useful comments.\nLack of Specificity. Even in the context where learners leave comments in forums, instructors responded that the comments are not specific enough to understand what causes the confusion or problem. Two of the survey respondents expressed that they would like to receive comments specifically anchored to the lecture content, such as \"I would like more examples of this algorithm\", and \"How do Japan and China name Japanese invasions of Korea described in slide 17?\". One of the survey respondents expressed the need for feedback on her instruction delivery, such as the tone of speech and sentence length.\nFrom the interviews and the survey, we confirmed that there are few learner comments, and the comments are generally not specific enough to be used as feedback. We anticipate that in-video prompting can address these challenges by actively asking questions while learners are watching lecture videos. However, it is hard for learners to provide a large amount of specific feedback. Questions that drive our investigation in in-video prompting include: How should we design questions in prompts to collect useful feedback? What is the effect of prompting on the learning experience? How distracting is it for learners to answer the questions while watching a video? How useful are the collected responses as feedback to instructors?\nTo answer these questions, we conducted a series of studies.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "INVESTIGATING IN-VIDEO PROMPTING", "text": "We aim to understand the effect of in-video prompting on both learners and instructors. Our investigation is organized as follows. We first present two studies designed to understand the effect of in-video prompting on learners. Then we consider the usefulness of collected responses as feedback from instructors' view as well as from instructional designers' view. Finally, we wrap up the studies by discussing the complexity of considering viewpoints of multiple stakeholders when designing prompting strategies.\nWe conducted two studies to understand the effect of in-video prompting on learners. The first study explored learners' qualitative experience of receiving in-video prompts. Learners watched videos with and without reflective prompts, and answered open-ended questions about their experience. The second study investigated how the type of prompt might influence learners' perceptions, comparing specific versus general prompts, and prompts focused on comprehension versus sharing one's experience with the instructor. The second study also collected quantitative measures of learners' experience.\nLearners received prompts to reflect at the beginning, middle, and end of each video. Prompts at the beginning could prepare people for learning [18], prompts in the middle can maintain engagement with mid-video prompts, and prompts at the end help in review the material as a whole [10].", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "STUDY 1. LEARNER PERCEPTIONS OF PROMPTING", "text": "The objective of the first study was to gain a qualitative understanding of how learners perceive the addition of in-video reflective prompts, by asking them to compare learning experiences with and without prompts.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study Design & Procedure", "text": "Participants watched two 8 minute lecture videos, one with reflective prompts, and one without any prompts. The videos were from Khan Academy, titled \"Population standard deviation\" and \"Logarithms\", respectively. The order of presentation and pairing of prompting condition with topic were counterbalanced.\nAfter participants watched both videos, they were asked openended questions about their experience in either condition, to compare their experience with respect to enjoyment, cognitive goal, and the perceived benefits to learning. 1 The prompting condition was counterbalanced over all four prompting strategies (see Table 1a), which we discuss in more depth in Study 2.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Participants", "text": "We recruited 100 participants (55 male, mean age 35.1) on Amazon Mechanical Turk, paying $6 for an hour-long study.\nRecruiting crowd workers enables us to (1) obtain a more general population compared to lab settings and (2) have greater experimental control to collect more extensive data about learning experiences, even though the motivation and background knowledge of crowd workers may be different from those of learners in online learning platforms. Participants said they enjoyed the interactivity and felt they have learned more. This sentiment is echoed by a participant: \"I think prompts make videos more hands-on and interactive and deliver a more educational experience.\"\nDistract from the learning process. Prompting might break the flow of concentration. Forcing learners to respond to the questions even if they are following the lecture very well can lead to a negative learning experience. A participant noted, \"it might cause you to lose focus on the material in the video by breaking your chain of thought because you are basically being interrupted.\"\nProvide no feedback on responses. Learners wish to receive feedback on their response to make sure that they properly respond to the questions. Without feedback, learners may be less motivated to respond to prompts. A participant commented, \"there is no feedback so even if I answer the prompt question and I'm confident, I may be wrong.\"\nCause anxiety. Some learners feel worried about giving inappropriate or inadequate responses. They feel the responses are being monitored, which makes learners frustrated when they struggle to come up with an appropriate response: \"I felt frustrated that it appeared difficult for me to explain what I learned thus far.\"", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "STUDY 2. EFFECTS OF DIFFERENT PROMPTING", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "STRATEGIES", "text": "Study 2 investigated how different kinds of prompts might be perceived by learners, and collected quantitative measures of how learners perceived prompts.\nWe investigated two dimensions of prompting questions: the comprehension-experience orientation and the level of specificity. The comprehension-experience orientation represents which information the question seeks to reveal. Comprehension-centered questions (\"Describe what you have learned so far.\") asked learners to reflect on the contents of the lecture and their current comprehension. These questions promote self-explanation in learners, which previous research suggests could be beneficial for their learning [24,5]. Responses to comprehension-centered questions allow instructors to identify how well learners are following the lecture. Experience-centered questions (\"Describe something unsatisfying about the lecture so far.\") reveal learning experiences during the lecture video. By directly asking what makes learners unsatisfied, instructors can collect actionable feedback on their instruction.\nThe level of specificity determines whether the prompting question refers directly to lecture content. General questions (\"Describe something unsatisfying about the lecture so far.\") do not refer to the content of the lecture. These questions could be shown anywhere in a given video. Specific questions (\"Describe how to calculate a mean.\") refer to concepts in the lecture video. Instructors should consider what to ask at specific moments, which requires more effort for instructors to build the prompting questions. Table 1a shows the design space of prompting questions that we cover, and examples of each.\nIn addition, we investigated whether perceptions of prompt types might vary based on learners' prior knowledge of the video content.\nWe investigated the following research questions.\n\u2022 RQ2a. How does the effect of in-video prompting vary according to the kind of prompting strategy?\n\u2022 RQ2b. How do learners with different levels of achievement perceive each prompting strategy?", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Study Design", "text": "The study used a between-subjects design, where each student was randomly assigned to one of the four prompting strategies described in Table 1a. Each participant watched a lecture video from Khan Academy on \"Population standard deviation\". We gave prompts at the beginning, middle, and end of the video. We denote each condition using four-letter codes; Co-Ge for the comprehension-centered and general condition, Co-Sp for comprehension and specific, Ex-Ge for experience and general, and Ex-Sp for experience and specific.\nTo understand the differences between prompting strategies, we included quantitative measures of learners' experiences.\nAfter watching the video, learners were asked to rate their agreement on a seven points scale with six statements about their experience. For example, a learner would be asked to rate on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree) whether \"Prompting helped me to pay attention to the lecture\" (Q1). The six statements about experience with prompting are listed in Figure 2. These asked about the extent to which learners agreed or disagreed that prompting helped them pay attention to the video, understand, grasp the most important ideas, was enjoyable, interrupted their learning process, or made them worried about giving inappropriate responses. These statements were chosen by using the qualitative dimensions identified in Study 1.\nParticipants also answered two questions about their cognitive load while watching the video [12]. They rated on a sliding scale from 0 to 100: \"How much mental demand did you experience watching this lecture?\" and \"How much effort did it take you to watch this lecture?\".", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Participants", "text": "We recruited 200 participants on Amazon Mechanical Turk, paying $3.5 for a 35-minute long study. Participants' mean age was 36.8 (SD = 11.5; min = 20; max = 72) with 102 male.\nFor each subgroup by prompting strategy (Co-Sp, Co-Ge, Ex-Sp, Ex-Ge), the mean ages were 39.2, 36.1, 37.3, and 36.6, respectively.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study Procedure", "text": "Participants were asked to (1) take a pretest with six problems, (2) watch a video with prompting, (3) respond to a survey related to the video watching experience, and then (4) take a posttest. The session ended with a final survey asking for participants' general experiences in free-form text.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Different prompting strategies had different effects on participants' perceived learning experience. As shown in Figure 2, learners rated comprehension-centered prompts as more helpful than experience-centered prompts on as measured by judgments on the positive questions Q1-Q4 (2-way ANOVA, F(1, 196) = 10.04 and p < 0.005, F(1, 196) = 33.11 and p < 0.0001, F(1, 196) = 33.00 and p < 0.0001, and F(1, 196) = 16.67 and p < 0.0001 for Q1, Q2, Q3, and Q4 respectively). Participants in the comprehension-centered conditions mentioned that they could take a breather, have time to reflect on what has been learned, and assess their understanding on their own. One participant remarked, \"It was a way to help ensure I understood the whole process by breaking it down with questions to answer, instead of trying to absorb it all at once and remember everything after.\"\nParticipants rated the experience-centered prompts as more distracting (2-way ANOVA, F(1,196)=17.13 and p<0.0001 for Q5). Participants in the experience-centered conditions mentioned that it was hard to learn from the video while at the same time trying to give comments on how the instructor could improve the video. One participant remarked, \"Prompting had me thinking about many things at once. This caused me to lose focus.\" However, there is also a bright side of experiencecentered prompts, especially regarding the learners' emotional experience. Participants said that they liked being able to leave a subjective comment to the instructor and it made them feel involved in the lecture.\nAlthough participants could take advantage of comprehensioncentered prompts in their learning, being asked about the contents of the lecture in depth irritated some learners. On the other hand, experience-centered prompts ask about participants' subjective opinion regarding the lecture and therefore it can be thought to be less pressure for learners. Our survey results show that, however, there is no such difference between comprehension-centered and experience-centered prompting strategies (Q6).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "The effect of learners' prior knowledge", "text": "The qualitative comments suggest that learners differ in their opinions about the value of prompting. To understand the effect of each prompting strategy for learners with unequal prior knowledge, we divided each group into two subgroups based on participants' pretest scores and discovered how the experience differed for each subgroup. As the cut-off point to separate the two groups we used a score of 2 out of 6, the median pretest score of all participants. Table 3 shows the number of participants in each subgroup.  prompting conditions (ANOVA, p<0.05 for both conditions). This result corresponds to the findings from earlier research [2] that high-performing students are good at providing an answer to generic questions.\nFigure 3b shows how each subgroup perceived anxiety for different prompting conditions. The result indicates that the low-score group felt more anxiety than the high-score group under specific prompting condition and the opposite pattern is observed for the general prompting conditions. This outcome may seem incongruous with the previous finding that the highscore group exhibit lower cognitive load with general prompts.\nHowever, study data collected from this experiment could provide alternative explanations for this result. First, a number of participants in the low-score group provided simple responses under general prompting conditions (e.g., \"population standard deviation\" for the prompt \"Describe what you have learned so far\"), leaving less chance of being wrong. In addition, with the experience-general prompts, many high-score participants mentioned it was hard for them to find unsatisfying points in the lecture, increasing their anxiety regarding their responses.\nThe result indicates that the high-score group and the lowscore group perceived a different level of cognitive load and anxiety. The low-score group reported higher cognitive load than the high-score group and the differences were especially large for general prompting conditions. Regarding the level of anxiety, the low and high-score group behaved differently for the specific and general conditions.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "LEARNER RESPONSES AS FEEDBACK TO INSTRUC-", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "TORS", "text": "We conducted a series of interviews with instructors and instructional designers to understand the usefulness of learner responses as feedback. Instructors and instructional designers are important stakeholders in in-video prompting, as they are the ones who author the prompts and potentially benefit from the collected learner responses. This section presents results from the interview study with instructors.  The low-score group reported higher cognitive load than the high-score group when they encountered general prompts. (b) The low-score group reported higher anxiety than the high-score group when they were given a comprehension-centered and specific prompt. Error bar:+/-1 standard error of the mean.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Procedure", "text": "We had 1-hour long interviews with instructors asking how useful the collected responses to in-video prompts from learners might be as feedback. We recruited 3 instructors who had experience in publishing lecture videos. Two instructors had published 8 hours and 10 hours of lecture video in total, respectively. The other instructor had led a flipped classroom for 8 semesters. After explaining the prompting strategies, we presented the learner responses as well as the corresponding questions collected in Study 2 and asked how they would make use of the responses as feedback. Instructors explored the learner responses for 10-15 minutes. After the exploration, we asked questions about using the responses as feedback.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Specificity of learners' responses. Instructors found that the collected responses are helpful as feedback because the responses were specific. They noted that asking questions in the middle of a video yields more specific responses. An instructor remarked, \"If instructors ask for comments at the end of the lecture, it is hard for learners to give comments on specific points of the lecture because they are likely to have forgotten details.\"\nNeeds for organizing learners' comments. Instructors expressed needs for clustering the responses, based on criteria such as whether the comments are about visuals, pronunciation, or lack of information. An instructor said, \"I thought it would be easier to read the comments if they were organized.\"\nComprehension-centered vs. Experience-centered. Instructors responded that experience-centered questions yield more actionable feedback than comprehension-centered questions. The responses from experience-centered questions (e.g., \"The instructor's handwriting is a little bad\") tend to point out the problems on instructional delivery, but the responses from comprehension-centered questions (e.g., \"The standard deviation is calculated by taking the square root of the variance.\") describe their comprehension. Instructors were not sure whether the responses from comprehensioncentered questions reflect learners' true level of comprehension. An instructor noted, \"If learners' responses are not good in comprehension-centered questions, it is hard to know whether the learner doesn't know, or the learner is just tired.\"\nInstructors had different opinions on which questions help learning more. Two instructors said asking comprehensioncentered questions is more helpful for learners because they promote reflection on the lecture while describing learning experience is not quite relevant to learning. One of the instructors said, \"I think learners feel like unnecessarily responding to experience-centered questions, and feel like checking their understanding when they respond to comprehensioncentered questions.\" However, another instructor mentioned experience-centered questions help learning more: \"I think experience-centered responses are going to inherently be more specific than comprehension-centered. could serve as content-related feedback (e.g., \"if the entire equation was done in meters, I would have a better and easier understanding of how it works\"), such as the clarity of a particular explanation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "INSTRUCTIONAL DESIGNERS' VIEW ON RESPONSES", "text": "To understand the usefulness of the responses as feedback from an educational point of view, we interviewed instructional designers.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Procedure", "text": "Similar to the interview study with instructors, we presented the responses as well as corresponding questions. Then we explained the prompting strategies and the instructional designers explored the responses. We conducted 1-hour long interviews with three instructional designers who are managing online courses on campus, including those on Coursera.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Instructional designers commented that the responses could serve as feedback to instructors, identified problems in our current prompts, and suggested ways to improve the design of prompting strategies.\nFeedback to instructors. Instructional designers said that the responses are useful as self-checklists for instructors, but to improve the lecture, the responses should be coupled with instructional design components that the instructor should consider in the lecture. Also, they remarked that high-level feedback such comments on learning objectives and organization of presentations is more useful for instructors.\nImportance of specific questions. Upon inspecting the current question prompts, instructional designers identified issues in the questions. They mentioned some questions were not concrete enough and too superficial to diagnose issues with the lecture. Designers suggested including more specific questions such as \"Is the pitch of the instructor's voice appropriate?\".\nMixing multiple prompting strategies. Instructional designers suggested using multiple prompting strategies even in a single video. They said asking experience-centered questions multiple times during a single lecture video could distract learners and damage the learning experience. Presenting experience-centered questions at the end and comprehensioncentered questions in the middle of the video could be effective for both learners and instructors.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION", "text": "In this research, we attempt to understand the effect of invideo prompting, an under-explored yet complex topic. Invideo prompting has potential to make video-based learning more interactive and even increase learning, while providing instructors with valuable data. Our goal is not to show that a particular prompting strategy is better than others or to conclude that prompting should be designed in a particular way for all videos, but rather (1) to contribute an in-depth understanding of differing perspectives on in-video prompting between learners, instructors, and instructional designers, and\n(2) to explore the design space of in-video prompting and the trade-offs involved. We discuss several issues in in-video prompting that involve handling trade-offs and making design decisions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Trade-off Between Learners and Instructors", "text": "One observation from our studies was that learners and instructors might prefer different prompting strategies for different reason. Learners perceive comprehension-centered prompting as enjoyable, less interrupting, and helping them learn. However, instructors generally found that the responses from experience-centered questions to be more actionable, which makes it easy for instructors to figure out the points that need to be improved. With this trade-off in mind, prompting strategies should be carefully designed to maximize the benefits for both stakeholders.\nOne way to address the trade-off is to design a hybrid prompting strategy, in which multiple prompt types are presented to a learner while watching a video, as suggested by instructional designers we interviewed. However, several issues could arise.\nIt might be the case that drawbacks of both prompt types could be observed. Moreover, presenting both prompt types makes learners respond to two different types of questions in a lecture video, which may increase their cognitive load. Further study is needed to determine whether and how this hybrid prompting strategy helps learners.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Trade-off Between Questions and Responses for Instructors", "text": "Designing specific prompts is more expensive than designing general prompts because specific prompts needs to be coupled with lecture content. For the responses, however, specific prompting tends to yield more specific responses, which instructors might find useful. Instructors should choose the level of specificity of the questions, but it is not a straightforward task. Also, specific questions could yield responses that are too narrow, which could compromise the diversity of responses.\nIt is important to balance the level of specificity to get diverse but insightful responses. One way to meet the balance is to design multiple specific questions. For example, we currently consider content-related specificity, but we also can consider instructional delivery-related specificity, such as the pace of lecture, the tone of voice, and the speed of speech. By considering multiple types of specificity in parallel in their prompt design, instructors could potentially receive diverse and specific responses. It is hard to say that one prompting strategy always outperforms another. As discussed above, there is a trade-off along the dimensions of prompting strategies for different stakeholders. However, there still remains much room for improvement in designing effective prompts.\nThe effects of in-video prompting highly depend on (1) who the learners are, (2) how difficult or well-structured the video is, and ( 3) what the prompts ask about. For example, as we observed in our study with crowd workers, prompting for negative feedback will generate only meager responses if the video is already well-structured, while learners may have a hard time writing their response. Likewise, requiring too detailed knowledge in in-video prompting may frustrate lowperforming learners, who are already prone to drop out. In an online learning environment, where thousands of videos meet millions of learners, thoughtfully designed and adjusted invideo prompting has potential to provide significant benefits to both learners and instructors. Future work could explore the feasibility of dynamically adjusting the prompting strategy for each video (content-specific), or generating a personalized prompt plan based on learners' course interaction (learnerspecific).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "LIMITATIONS AND FUTURE WORK", "text": "This work aims to provide guidance to instructors and researchers about how learners perceive in-video prompts, and when it might be worth including them, and in what form.\nDue to the exploratory nature of this work, however, it leaves several points unanswered.\nWe did not investigate how in-video prompting affects learning outcomes since our goal is not to yield a clear conclusion about how effective in-video prompting is. As research shows that reflective prompting could yield learning gains [5,24], future work could address the relationship between prompting strategies and learning outcomes.\nThis paper only covers a subset of dimensions in designing prompting strategies: comprehension-experience orientation and the level of specificity. For the prompt positions in our studies, the rationale followed the affordance of time-anchored video prompting; the positioning of reflective prompts afford different levels of specificity, e.g., a prompt at the end of a video could ask to reflect more generally, whereas a prompt in the middle could refer to the specific concept just covered. We recognize that there might be other important design factors not addressed in this work. Future work could address other dimensions such as frequency of prompting and mode of learner responses.\nWe collected responses from crowd workers, which could bias results, e.g., due to selection bias or non-representativeness. It is not clear whether our findings might generalize to learners in online learning platforms. Moreover, the crowd workers responded to all the prompting questions partly of monetary reward. Future work should perform a deployment study to test the effect of in-video prompting in real-world educational settings.\nDesigning personalized prompting strategies could be an interesting future work. By leveraging learners' data, we could choose the most beneficial prompting strategies. For example, if the learner is already good at the subject covered by the lecture, the instructor could provide prompting with more challenging questions.\nFuture work could also investigate the design of the instructor dashboard. Interview results suggest that instructors have needs for efficiently organizing and exploring the collected learner responses to easily grasp the overall perception of comments. Automatically organizing, aggregating, and visualizing learner responses could be an interesting future direction to explore.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "CONCLUSION", "text": "This paper investigated the effects of in-video prompting on both learners and instructors. In-video prompting enables instructors to collect specific comments from learners. To understand how in-video prompting affects the learning experience, we conducted two studies with crowd workers. Results showed that different prompting strategies have different effects on the learning experience. Learners perceive that learning-centered questions are less interrupting, more enjoyable, and more helpful for learning. Interviews with instructors revealed that in-video prompting gives specific comments to them and that responses from experience-centered questions are more actionable. Instructional designers emphasized the importance of coupling question design with instructional design components for more useful feedback.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "YouEDU: addressing confusion in MOOC discussion forums by recommending instructional video clips", "journal": "", "year": "2015", "authors": "Akshay Agrawal; Jagadish Venkatraman; Shane Leonard; Andreas Paepcke"}, {"title": "Supporting self-explanation of argument transcripts: Specific v. generic prompts", "journal": "", "year": "2006", "authors": "Niels Vincent Aleven; Kevin Pinkwart; Collin Ashley;  Lynch"}, {"title": "Studying learning in the worldwide classroom: Research into edX's first MOOC", "journal": "Research & Practice in Assessment", "year": "2013", "authors": "Lori Breslow; E David; Jennifer Pritchard; Glenda S Deboer; Andrew D Stump; Daniel T Ho;  Seaton"}, {"title": "DropoutSeer: Visualizing learning patterns in Massive Open Online Courses for dropout reasoning and prediction", "journal": "", "year": "2016", "authors": "Yuanzhe Chen; Qing Chen; Mingqian Zhao; Sebastien Boyer; Kalyan Veeramachaneni; Huamin Qu"}, {"title": "Eliciting self-explanations improves understanding", "journal": "Cognitive science", "year": "1994", "authors": "T H Michelene; Nicholas Chi; Mei-Hung Leeuw; Christian Chiu;  Lavancher"}, {"title": "The role of anomalous data in knowledge acquisition: A theoretical framework and implications for science instruction", "journal": "Review of educational research", "year": "1993", "authors": "A Clark; William F Chinn;  Brewer"}, {"title": "The one-minute paper: Some empirical findings", "journal": "The Journal of Economic Education", "year": "1998", "authors": "F John; Anthony L Chizmar;  Ostrosky"}, {"title": "Classroom Assessment Techniques. A Handbook for Faculty", "journal": "", "year": "1988", "authors": "Patricia Cross; Thomas A Angelo"}, {"title": "Mudslide: A spatially anchored census of student confusion for online lecture videos", "journal": "ACM", "year": "2015", "authors": "Juho Elena L Glassman; Andr\u00e9s Kim; Meredith Ringel Monroy-Hern\u00e1ndez;  Morris"}, {"title": "Retrieval practice produces more learning than elaborative studying with concept mapping", "journal": "Science", "year": "2011", "authors": "D Jeffrey; Janell R Karpicke;  Blunt"}, {"title": "Data-driven interaction techniques for improving navigation of educational videos", "journal": "ACM", "year": "2014", "authors": "Juho Kim; J Philip; Carrie J Guo; Shang-Wen Daniel Cai;  Li; Z Krzysztof; Robert C Gajos;  Miller"}, {"title": "The instructor's face in video instruction: Evidence from two large-scale field studies", "journal": "Journal of Educational Psychology", "year": "2015", "authors": "F Ren\u00e9; Jeremy N Kizilcec; Charles J Bailenson;  Gomez"}, {"title": "Effects of in-video Quizzes on MOOC lecture viewing", "journal": "", "year": "2016", "authors": "Geza Kovacs"}, {"title": "Using time-anchored peer comments to enhance social interaction in online educational videos", "journal": "ACM", "year": "2015", "authors": "Yi-Chieh Lee; Wen-Chieh Lin; Fu-Yin Cherng; Hao-Chuan Wang; Ching-Ying Sung; Jung-Tai King"}, {"title": "When to jump in: The role of the instructor in online discussion forums", "journal": "Computers & Education", "year": "2007", "authors": "Margaret Mazzolini; Sarah Maddison"}, {"title": "Does a presentation's medium affect its message? PowerPoint, Prezi, and oral presentations", "journal": "PloS one", "year": "2017", "authors": "Selen Samuel T Moulton;  T\u00fcrkay;  Stephen M Kosslyn"}, {"title": "VidCrit: Video-based Asynchronous Video Review", "journal": "ACM", "year": "2016", "authors": "Amy Pavel; B Dan; Bj\u00f6rn Goldman; Maneesh Hartmann;  Agrawala"}, {"title": "Inventing to prepare for future learning: The hidden efficiency of encouraging original student production in statistics instruction", "journal": "Cognition and Instruction", "year": "2004", "authors": "L Daniel; Taylor Schwartz;  Martin"}, {"title": "The Video Collaboratory as a Learning Environment", "journal": "ACM", "year": "2016", "authors": "Vikash Singh; Sarah Abdellahi; Mary Lou Maher; Celine Latulipe"}, {"title": "Monitoring moocs: which information sources do instructors value", "journal": "ACM", "year": "2014", "authors": "Kristin Stephens-Martinez; Marti A Hearst; Armando Fox"}, {"title": "Mental aerobics: The half-sheet response", "journal": "Innovative Higher Education", "year": "1985", "authors": "L Richard;  Weaver; W Howard;  Cotrell"}, {"title": "Sentiment Analysis in MOOC Discussion Forums: What does it tell us", "journal": "", "year": "2014", "authors": "Miaomiao Wen; Diyi Yang; Carolyn Rose"}, {"title": "The role of explanation in discovery and generalization: Evidence from category learning", "journal": "Cognitive Science", "year": "2010", "authors": "J Joseph; Tania Williams;  Lombrozo"}, {"title": "Revising Learner Misconceptions Without Feedback: Prompting for Reflection on Anomalies", "journal": "ACM", "year": "2016", "authors": "Joseph Jay Williams; Tania Lombrozo; Anne Hsu; Bernd Huber; Juho Kim"}, {"title": "The hazards of explanation: Overgeneralization in the face of exceptions", "journal": "Journal of Experimental Psychology: General", "year": "2013", "authors": "Joseph Jay Williams; Tania Lombrozo; Bob Rehder"}, {"title": "Smart Jump: Automated Navigation Suggestion for Videos in MOOCs", "journal": "", "year": "2017", "authors": "Han Zhang; Maosong Sun; Xiaochen Wang; Zhengyang Song; Jie Tang; Jimeng Sun"}, {"title": "Ask the Instructors: Motivations and Challenges of Teaching Massive Open Online Courses", "journal": "ACM", "year": "2016", "authors": "Saijing Zheng; Pamela Wisniewski; Mary Beth Rosson; John M Carroll"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "FigureFigure 2 :2Figure3aillustrates self-reported cognitive load of each group for four different prompting conditions. The high-score group perceived significantly less cognitive load under the general", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Self-reported cognitive load, out of 100. *: the difference between low and high-score group is significant with p<0.05.(b) Average scores for Q6: Prompting made me worried about giving inappropriate responses. *: the difference between low and high-score group is significant with p<0.05.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Perceived experience for Low and High group in each prompting condition. (a)The low-score group reported higher cognitive load than the high-score group when they encountered general prompts. (b) The low-score group reported higher anxiety than the high-score group when they were given a comprehension-centered and specific prompt. Error bar:+/-1 standard error of the mean.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "of different prompting strategies(RQ2). We collected open-ended responses about the pros and cons of prompting from learners and found evidence that learners generally prefer comprehension-centered prompting to experience-centered prompting. We conducted interviews with instructors and instructional designers to understand the usefulness of the collected learner responses as feedback on how the learners were doing, and if the course iterations needed change (RQ3).", "figure_data": "It turns out that collected responses are generally more spe-cific than what they have usually collected. Instructors foundresponses to experience-centered questions provided more ac-tionable feedback than those from comprehension-centeredquestions. Instructional designers provided insights into de-signing better prompting strategies."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Main pros and cons of in-video prompting from learners' perspective (number of mentions in parenthesis)To analyze participants' comments, we first separated comments consisting of more than one point into multiple comments so that each comment contains only one idea. We then categorized each comment into two groups based on whether it was positive or negative. Out of 231 comments, 130 were positive and 101 were negative. For comments in each group, one researcher conducted open coding to categorize each response. Afterwards, another researcher verified the coded labels and resolved conflicts with discussion. Table2describes the reported pros and cons of in-video prompting from learners' perspective. The extracted categories of participants' comments are as follows: Prompting splits the knowledge to digest in a more fine-grained way. By forcing learners to stop and think at checkpoints, the amount of knowledge to absorb at once is reduced: \"With prompts, the learning is broken down into stages and you have to think and reiterate what you've learned so far, which makes it easier to remember.\"Help grasp key concepts. Prompting helps learners to grasp the most important concepts of the lecture. Participants perceived the moment of prompting as important checkpoints, which makes learners think about the most important concepts they have learned so far. A participant pointed out, \"Having a prompt helps to indicate exactly what the key concept was so you can take a moment and decide if you fully understand it.\" Provide interactivity. Prompting is an interactive activity.", "figure_data": "ProsConsConcentration-Enhance learners' concentration. (42)-Distract from the learning process. (59)-Encourage reflection. (57)Learning process-Split the lecture into small pieces. (16)-Provide no feedback on responses. (9)-Help grasp key concepts. (10)Emotional responses -Provide interactivity. (5)-Cause anxiety. (17)Qualitative ResultsEnhance"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The number of participants in each group and sub-", "figure_data": "group. Co-Sp: comprehension-centered and specific, Co-Ge: comprehension-centered and general, Ex-Sp: Experience-centered and specific, and Ex-Ge: experience-centered andgeneral. High: participants with high pretest scores; Low:participants with low pretest scores.Total High LowCo-Sp502228Co-Ge502129Ex-Sp502327Ex-Ge503119"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "CHI 2018 PaperCHI 2018,[ . . . ]When we are talk-ing about learning a particular topic, it's just going to beway more useful to talk about the thing that we can both seeclearly between the two of us [ . . . ]. That's going to be a moreproductive discussion than what's going on inside my head.\"Specific vs. General. Instructors observed that differentlevels of specificity in prompts result in different types offeedback. For experience-centered questions, instructors saidresponses to general questions could serve as feedback oninstruction delivery (e.g., \"It was good, just a little slow\"),such as the speed of speech, but responses to specific questions"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "CHI 2018 PaperCHI 2018,Through the interviews, we observed that the meaning of use-ful feedback is different between instructors and instructionaldesigners. Instructors found it useful to get actionable feed-back, whereas instructional designers found it useful to getmore high-level feedback.This leads us to think about what good feedback is for in-structors and instructional designers. We demonstrated howcollecting actionable comments from learners is possible, butmaking sense of them, finding patterns in them, and derivinghigh-level points require extra work. Future work could inves-tigate collecting and processing the responses of learners togenerate more high-level, aggregate feedback to instructors."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}], "doi": "10.1145/3173574.3173893"}