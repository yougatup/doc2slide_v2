{
  "abstractText": {
    "page": 0,
    "region": {
      "x1": 53.31999969482422,
      "x2": 295.7413635253906,
      "y1": 462.90985107421875,
      "y2": 567.9660034179688
    },
    "text": "ABSTRACT We report the results of a crowdsourced experiment that measured the accuracy of motion outlier detection in multivariate, animated scatterplots. The targets were outliers either in speed or direction of motion, and were presented with varying levels of saliency in dimensions that are irrelevant to the task of motion outlier detection (e.g., color, size, position). We found that participants had trouble finding the outlier when it lacked irrelevant salient features and that"
  },
  "figures": [{
    "caption": "Figure 3: Flow diagram illustrating the sequence of screens in the study interface. Participants could replay the animations twice. Blank screens were place in-between replays.",
    "captionBoundary": {
      "x1": 53.79800033569336,
      "x2": 295.63787841796875,
      "y1": 245.99038696289062,
      "y2": 273.5479736328125
    },
    "figType": "Figure",
    "imageText": ["250", "msPlay", "(2|3)", "500", "ms", "Submit", "500", "ms", "Select", "and", "Play", "(1)", "Blank", "(1s)", "Frame", "2", "Frame", "2", "End", "Frame", "1", "Frame", "1"],
    "name": "3",
    "page": 5,
    "regionBoundary": {
      "x1": 83.65276336669922,
      "x2": 265.0,
      "y1": 92.0,
      "y2": 232.0
    }
  }, {
    "caption": "Figure 2: Snapshot of the interface for the speed task. The direction task asked \"Select the point thatmoves in themost deviant direction.\"",
    "captionBoundary": {
      "x1": 317.9549865722656,
      "x2": 558.190185546875,
      "y1": 320.6463928222656,
      "y2": 348.2039794921875
    },
    "figType": "Figure",
    "imageText": [],
    "name": "2",
    "page": 1,
    "regionBoundary": {
      "x1": 317.0,
      "x2": 559.0,
      "y1": 92.0,
      "y2": 309.0
    }
  }, {
    "caption": "Figure 4: Distribution of accuracy for each condition. For each stimulus, accuracy is calculated over 20 judgments. There are 10 stimuli per condition, one for each scene.",
    "captionBoundary": {
      "x1": 53.564998626708984,
      "x2": 558.1805419921875,
      "y1": 315.1123962402344,
      "y2": 331.71002197265625
    },
    "figType": "Figure",
    "imageText": ["se", "cr", "ea", "ze", " in", "+", "si", "se", "cr", "ea", "ze", " in", "−", "si", "se", "cr", "ea", "ze", " in", "+", "si", "se", "cr", "ea", "ze", " in", "−", "si", "Tasks", "Saliency", "Deficient", "Saliency", "Charged", "y", "group", "ur", "ac", "A", "cc", "Condition", "1.00", "0.75", "0.50", "0.25", "0.00", "io", "n", "re", "ct", "−", "di", "ee", "d", "−", "sp", "lo", "r", "−", "co", "ze", "−", "si", "n", "b", "a", "s", "e", "li", "n", "e", "−", "c", "h", "a", "rg", "e", "d", "−", "po", "si", "tio", "b", "a", "s", "e", "li", "n", "e", "−", "d", "e", "fi", "c", "ie", "n", "t", "+", "po", "si", "tio", "n", "+", "si", "ze", "+", "co", "lo", "r", "+", "sp", "ee", "d", "+", "di", "re", "ct", "io", "n", "io", "n", "re", "ct", "−", "di", "ee", "d", "−", "sp", "lo", "r", "−", "co", "ze", "−", "si", "n", "b", "a", "s", "e", "li", "n", "e", "−", "c", "h", "a", "rg", "e", "d", "−", "po", "si", "tio", "b", "a", "s", "e", "li", "n", "e", "−", "d", "e", "fi", "c", "ie", "n", "t", "+", "po", "si", "tio", "n", "+", "si", "ze", "+", "co", "lo", "r", "+", "sp", "ee", "d", "+", "di", "re", "ct", "io", "n", "direction", "speed"],
    "name": "4",
    "page": 6,
    "regionBoundary": {
      "x1": 59.609352111816406,
      "x2": 552.7369384765625,
      "y1": 96.91120910644531,
      "y2": 298.15350341796875
    }
  }, {
    "caption": "Figure 5: Estimates for the effect of irrelevant salient features on the odds of a speed (top) and direction (bottom) outlier being identified. Binary covariates andmultiplicative coefficients. Red denotes statistical significance (p < .05).",
    "captionBoundary": {
      "x1": 53.79800033569336,
      "x2": 295.6379089355469,
      "y1": 616.2993774414062,
      "y2": 654.8150024414062
    },
    "figType": "Figure",
    "imageText": ["Direction", "Task", "e", "ia", "bl", "V", "ar", "Estimate", "(odds)", "0", "1", "2", "3", "4", "position", "speed", "size", "color", "size_increase", "(Intercept)", "Speed", "Task", "e", "ia", "bl", "V", "ar", "Estimate", "(odds)", "0", "2", "4", "6", "direction", "position", "color", "size", "size_increase", "(Intercept)"],
    "name": "5",
    "page": 6,
    "regionBoundary": {
      "x1": 59.915714263916016,
      "x2": 291.0,
      "y1": 348.69927978515625,
      "y2": 602.5113525390625
    }
  }, {
    "caption": "Figure 6: Left: estimates for the effect of feature saliency on the number of times a non-target is selected (erroneously). Right: interaction plot depicting the modulation of the effect of speed and direction by irrelevant features.",
    "captionBoundary": {
      "x1": 53.79800033569336,
      "x2": 559.3278198242188,
      "y1": 419.46539306640625,
      "y2": 436.06402587890625
    },
    "figType": "Figure",
    "imageText": ["Errors", "−", "Significant", "interaction", "terms", "Direction", "task", "+", "1", "SD", "Mean", "−", "1", "SD", "t", "co", "un", "direction", "4", "3", "2", "1", "−1", "0", "1", "2", "−1", "0", "1", "2", "−1", "0", "1", "2", "saliency_color", "saliency_xy1", "saliency_xy2", "Errors", "Direction", "task", "e", "ia", "bl", "V", "ar", "Estimate", "(count)", "0.8", "1.0", "1.2", "1.4", "1.6", "(Intercept)", "direction", "saliency_xy2", "direction:saliency_xy1", "saliency_speed", "saliency_color", "direction:saliency_color", "direction:saliency_size", "saliency_xy1", "direction:saliency_speed", "saliency_size", "direction:saliency_xy2", "Errors", "−", "Significant", "interaction", "terms", "Speed", "task", "+", "1", "SD", "Mean", "−", "1", "SD", "t", "co", "un", "speed", "3.0", "2.5", "2.0", "1.5", "1.0", "−3", "−2", "−1", "0", "1", "2−3", "−2", "−1", "0", "1", "2", "saliency_direction", "saliency_xy1", "Errors", "Speed", "task", "e", "ia", "bl", "V", "ar", "Estimate", "(count)", "1.0", "1.2", "1.4", "1.6", "(Intercept)", "speed", "saliency_direction", "saliency_xy1", "speed:saliency_direction", "saliency_xy2", "speed:saliency_xy1", "saliency_size", "saliency_color", "speed:saliency_size", "speed:saliency_color", "speed:saliency_xy2"],
    "name": "6",
    "page": 8,
    "regionBoundary": {
      "x1": 68.70411682128906,
      "x2": 545.1566162109375,
      "y1": 96.71009826660156,
      "y2": 409.5339050292969
    }
  }, {
    "caption": "Figure 7: Distribution of number of views divided by task and condition.",
    "captionBoundary": {
      "x1": 317.9549865722656,
      "x2": 558.468017578125,
      "y1": 617.7274169921875,
      "y2": 634.3259887695312
    },
    "figType": "Figure",
    "imageText": ["group", "Saliency", "Charged", "Saliency", "Deficient", "correct", "FALSE", "TRUE", "nt", "C", "ou", "Number", "of", "Views", "80", "60", "40", "20", "0", "80", "60", "40", "20", "0", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "1", "2", "3", "direction", "speed", "se", "cr", "ea", "ze", "in", "−", "si", "io", "n", "re", "ct", "−", "di", "ee", "d", "−", "sp", "lo", "r", "−", "co", "ze", "−", "si", "n", "si", "tio", "−", "po", "ge", "d", "ch", "ar", "lin", "e−", "ba", "se", "se", "cr", "ea", "ze", "in", "+", "si", "io", "n", "re", "ct", "+", "di", "ee", "d", "+", "sp", "lo", "r", "+", "co", "ze", "+", "si", "n", "si", "tio", "+", "po", "ie", "nt", "de", "fic", "lin", "e−", "ba", "se"],
    "name": "7",
    "page": 8,
    "regionBoundary": {
      "x1": 323.3960876464844,
      "x2": 548.8931274414062,
      "y1": 450.4962463378906,
      "y2": 600.0904541015625
    }
  }, {
    "caption": "Table 1: Feature ranges. When speed is the task, the target is assigned an outlying distance value andmean or salient value for the other features.When direction is the task, the target receives an outlying direction. The color spectrum is defined by matplotlib’s Viridis colormap.",
    "captionBoundary": {
      "x1": 317.6260070800781,
      "x2": 559.91650390625,
      "y1": 94.7542724609375,
      "y2": 160.79705810546875
    },
    "figType": "Table",
    "imageText": ["x,", "y", "[0,", "500]", "px", "250", "variable", "color", "[", ",", "]", "size", "(area)", "[100,", "600]", "px", "350", "600", "size", "increase", "[1,", "2]", "multiplier", "1.5", "2", "distance", "[25,", "100]", "px", "62.5", "100", "150", "direction", "[-81,", "171]", "degree", "45", "171", "225", "Sa", "li", "en", "t", "va", "lu", "e", "O", "ut", "li", "er", "va", "lu", "e", "≈", "M", "ea", "n", "≈", "U", "ni", "t", "ge", "Feature", "R", "an"],
    "name": "1",
    "page": 4,
    "regionBoundary": {
      "x1": 322.0,
      "x2": 552.0,
      "y1": 176.0,
      "y2": 284.6689453125
    }
  }],
  "sections": [{
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 53.24800109863281,
        "x2": 295.0179748535156,
        "y1": 584.8275756835938,
        "y2": 711.5759887695312
      },
      "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI 2019, May 4–9, 2019, Glasgow, Scotland UK © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5970-2/19/05. . . $15.00 https://doi.org/10.1145/3290605.3300771"
    }, {
      "page": 0,
      "region": {
        "x1": 317.70599365234375,
        "x2": 559.8948364257812,
        "y1": 462.8301696777344,
        "y2": 564.9769897460938
      },
      "text": "visual channels contribute unevenly to the odds of an outlier being correctly detected. Direction of motion contributes the most to accurate detection of speed outliers, and position contributes the most to accurate detection of direction outliers. We introduce the concept of saliency deficit in which item importance in the data space is not reflected in the visualization due to a lack of saliency. We conclude that motion outlier detection is not well supported in multivariate animated scatterplots."
    }]
  }, {
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 317.6960144042969,
        "x2": 558.2056884765625,
        "y1": 602.55712890625,
        "y2": 621.0180053710938
      },
      "text": "•Human-centered computing→ Empirical studies in visualization;"
    }],
    "title": {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 392.6675109863281,
        "y1": 587.69287109375,
        "y2": 594.1190185546875
      },
      "text": "CCS CONCEPTS"
    }
  }, {
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.1893310546875,
        "y1": 658.5281982421875,
        "y2": 708.218994140625
      },
      "text": "Rafael Veras and Christopher Collins. 2019. Saliency Deficit and Motion Outlier Detection, in Animated Scatterplots. In CHI Conference on Human Factors in Computing Systems Proceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3290605.3300771"
    }, {
      "page": 0,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 1"
    }],
    "title": {
      "page": 0,
      "region": {
        "x1": 317.6230163574219,
        "x2": 415.48681640625,
        "y1": 647.784423828125,
        "y2": 653.4240112304688
      },
      "text": "ACM Reference Format:"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.7414245605469,
        "y1": 111.05318450927734,
        "y2": 691.406982421875
      },
      "text": "In this paper, we investigate questions related to the independence of visual dimensions in animated scatterplots. We often seek to encode data in as many visual variables as possible, and this strategy has been extended to scatterplots with the use of color, size, and motion. Here we question the accuracy of the basic task of motion outlier detection in the complex scenes formed by animated multivariate scatterplots. Does the saliency of non-motion features impact the detection of motion outliers? Can we put motion outliers in a state where they are hard to detect by simply changing their color, size, or position? If so, in visualizations where observing change is a relevant task the variations in data point saliency will hinder or amplify the local perception of change, turning the encoding unreliable. The perception literature has abundant studies on the performance of search tasks in static and moving scenes [7, 8, 21, 33, 35]. However, psychology studies are difficult to comprehend by non-experts, and their low level makes it difficult to extract implications for visualization design. Nonetheless, these controlled experiments produced general results that support useful rules of thumb; for instance, targets among uniform distractors are much easier to detect than when the distractors have high variance [7]. This rule captures well the results of “pre-attention” experiments with single and conjunction static features (e.g., color), and with motion components (speed and direction). Detection of speed and direction outliers in displays where no other features compete is considered efficient, and the effects of speed on direction and vice-versa are well studied [25]. However, detecting speed and direction targets in scenes where many other channels are used is not well studied. In the second edition of his book, Ware warned that studies on perceptual independence among three or more visual channels were rare [34]. Almost 15 years later, our understanding of these interactions and their implications to visualization is insufficient, and fewer are the studies that involve motion in visualization. Progress recently has been made in revising rankings of encoding effectiveness [17, 22]. While these have great practical application, they do not seek to explain the fundamental phenomena driving performance results. Among the powerful concepts that may help us unveil the roots of problems in the visual mapping of data is visual saliency. In this paper, we contribute an experiment aimed at measuring the gap in motion outlier detection accuracy between salient and non-salient outliers. We simulate animated scatterplots that contain either a speed outlier or a direction outlier. Then we vary the number of static features that, in addition to motion, are salient in these outliers."
    }, {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.8978271484375,
        "y1": 367.3511657714844,
        "y2": 493.40899658203125
      },
      "text": "We find that motion outliers that have additional salient features are much more likely to be correctly identified than non-salient outliers. Our results show that motion is not immune from the interference of other dimensions and suggest that motion outlier detection is unreliable in multivariate animated scatterplots. We proceed to define the notion of saliency deficit: a state where the saliency profile in a visualization scene impairs the effectiveness of a visualization task; and suggest that saliency deficit models can help the automatic identification of saliency-boosting opportunities in visualizations."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 53.79800033569336,
        "x2": 146.90223693847656,
        "y1": 96.18885803222656,
        "y2": 102.614990234375
      },
      "text": "1 INTRODUCTION"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 317.5870056152344,
        "x2": 559.89892578125,
        "y1": 524.2591552734375,
        "y2": 602.4959716796875
      },
      "text": "In this research, we are interested in the role saliency plays in motion outlier identification. While this question has wideranging applications, we constrain our investigation to animated scatterplots. In this section, we will review the related work in perception for information visualization, the use of animated scatterplots, and the recent trend of developing empirical perception models for visualization."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 411.9559326171875,
        "y1": 509.39483642578125,
        "y2": 515.8209838867188
      },
      "text": "2 RELATEDWORK"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 317.6260070800781,
        "x2": 559.8985595703125,
        "y1": 633.3461303710938,
        "y2": 711.5830078125
      },
      "text": "Visual attention research investigates the limits of attention of the human visual system and has produced a number of theories that explain the mechanisms of visual information processing (see Healey and Enns [13] for a review). Feature integration theory proposes that scenes are initially processed as many separable basic dimensions (e.g., color, motion, orientation), which are later integrated to"
    }, {
      "page": 1,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 2"
    }, {
      "page": 2,
      "region": {
        "x1": 53.46900177001953,
        "x2": 305.3608093261719,
        "y1": 96.1091537475586,
        "y2": 497.135986328125
      },
      "text": "form more complex objects [32]. Without focused attention, features remain separated. As a consequence of this mechanism, searches for basic features occur in parallel and are fast, while searches for conjunction features, which involve more than one dimension (e.g., a red circle in a scene with red squares and blue circles), occur serially and thus slow down as the number of objects present in the scene increase. Visual search experiments usually ask participants to determine whether a target is present in a scene with distractors, and the number of distractors is manipulated. Reaction times (RT) and accuracy are recorded, and results are summarized as the slope of the linear relationship between the response and the number of distractors. Parallel searches have slope close to 0. Frequently, the term “popout” is used to describe the easy identification of targets in these searches. While many experiments corroborate feature integration theory, other experiments found that some conjunction searches are too efficient to be serial searches. For instance, motionshape targets can be detected in parallel, suggesting the existence of a motion filtering process, which effectively subsets the scene, reducing the search task to a simple feature search on moving items [21, 33]. Aiming at explaining these problematic cases, the theory of guided search posits that the goals of the viewer play a large role in visual search, with activation maps (“heatmap” representations of the visual space storing the likelihood of locations containing a target) being constructed with bottom-up and top-down information. Top-down processes are cognitive, driven by users tasks and goals, while bottom-up processes are driven by sensory information. Guided search theory suggests that the difference in performance between single feature and conjunction tasks is due to the amount of guidance that bottom-up processes can provide [35]. Thus top-down guidance is the reason “fast” conjunction searches exist."
    }, {
      "page": 2,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.7423400878906,
        "y1": 502.5851745605469,
        "y2": 712.3289794921875
      },
      "text": "The impact of color on motion discrimination is well studied. Both hue and luminance have been shown to independently enable apparent motion of simple objects when they are displayed in different positions in successive frames, prompting debate as to whether or not color and motion are processed by separate pathways [23]. Croner and Albright [6] found that hue saliency and luminance saliency aid the discrimination of motion direction; that is, participants detect more accurately targets moving in the same direction among distractors moving in random directions when the targets have distinct hue or luminance, which may suggest that color segmentation of the scene occurs prior to motion discrimination, a process opposite to the motion filtering mentioned above. The statistical saliency model (SSM) [25] seeks to explain motion popout phenomena with a simple statistical measure that quantifies the saliency of targets with respect to the distractors in the scene. The SSM explains the following"
    }, {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.6441040039062,
        "y1": 96.1091537475586,
        "y2": 269.98797607421875
      },
      "text": "asymmetries inmotion popout phenomena: a) searching for a moving target among still distractors is easier than searching for a still target among moving distractors; b) searching for a fast target among slow targets is easier than the opposite; c) adding variability in speed when searching for a unique motion direction has little effect, while adding variability in direction when searching for a unique speed makes the search task more difficult. The SSM is compelling because calculation of the saliency of objects is trivial and efficient, and because it has been shown to explain search results in experiments where dimensions other than motion are examined. We review this model in more detail in Section 3. We enumerate the following challenges in transferring the existing perception knowledge to the problem addressed in this work:"
    }, {
      "page": 2,
      "region": {
        "x1": 318.27398681640625,
        "x2": 559.893798828125,
        "y1": 281.59014892578125,
        "y2": 431.5580139160156
      },
      "text": "1 In the perception experiments cited above, targets are chosen arbitrarily. In our experiment, targets are outliers in the statistical sense. We ask whether outlierness as a statistical property is preserved through the visual mapping. 2 Motion outlier detection in scatterplots is not a conjunction task. While the conjunction of motion and other dimensions is well studied, our problem is defined as a basic feature search in the presence of many irrelevant dimensions. 3 The dimensions in our stimuli encode continuous data attributes, while in perception studies they are often discretized to some degree (e.g., moving / still, fast / slow, bright / dim) [6, 21, 23, 33]."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 366.9331970214844,
        "y1": 618.4818725585938,
        "y2": 624.9080200195312
      },
      "text": "Perception"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 317.5870056152344,
        "x2": 559.8988037109375,
        "y1": 464.78515625,
        "y2": 710.39501953125
      },
      "text": "Scatterplots are one of the most effective visualizations because they employ position along a common scale, which was found to be the representation with which people can most accurately perform visual judgments [14]. Less important dimensions are commonly mapped to color, size, and shape. Gleicher et al. demonstrated that people can accurately compare means in multiclass scatterplots despite the addition of one discrete irrelevant cue (shape) [12]. This work shows that people can comfortably extract a summary statistic confined to a single dimension in the presence of an irrelevant dimension. Here, we investigate whether another summary statistic (outlierness) can be extracted frommotion in correlated scatterplots with more than one irrelevant dimension (color, size). A key difference is that our scatterplots do not feature discrete dimensions that would enable the visual segmentation of the scene. Szafir et al. argue that ensemble coding allows us to visually extract statistical information from scatterplots, such as outliers and statistical summaries, but acknowledge that attentional control may be problematic when multiple variables are encoded simultaneously, although the empirical"
    }, {
      "page": 2,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 3"
    }, {
      "page": 3,
      "region": {
        "x1": 53.54899978637695,
        "x2": 295.7342224121094,
        "y1": 96.1091537475586,
        "y2": 198.25701904296875
      },
      "text": "basis is still lacking [31]. Robertson et al. [24] found that animated scatterplots were not superior to static trend visualizations in analytical tasks (error rates) focused on trajectories. Huber and Healey [15] devised precise discriminability lower limits for motion (in displays with no competing visual channels): a target-distractor difference of a least 20 degrees is necessary for direction oddballs to be detected accurately; for speed, the difference needs to be at least 0.43 degrees of visual angle. Our outliers satisfy these conditions (Section 4)."
    }, {
      "page": 3,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.74066162109375,
        "y1": 203.70619201660156,
        "y2": 473.2250061035156
      },
      "text": "Albeit designed to devise guidelines for notification design, Bartram et al.’s study of visual cues came to conclusions that relate to visualization design. Subjects were asked to perform a task in a window while glyphs overloaded with various encodings were scattered in the periphery [1]. The authors measured how accurately subjects could detect change in the glyphs. Motion was found to be the most reliable cue, better than changes in shape and color. They concluded that motion “does not seem to interfere with existing color and form coding” and that motion detection is effective even in visual periphery and with small amplitudes. Etemadpour et al. [9, 10] used motion as a solution to clutter on the assumption that motion does not suffer interference from other channels. They reported a large improvement in the accuracy of ranking cluster density when motion was used as an encoding for cluster density. The improvements were relative to scatterplots where density was not explicitly encoded (implicitly encoded as position); plus, density is necessarily correlated to position, which makes motion-position a double encoding for density. Similarly, animated scatterplot matrices that encoded density with flickering were found superior to conventional ones in density judgement tasks [5]."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 418.66107177734375,
        "y1": 449.921875,
        "y2": 456.3480224609375
      },
      "text": "Animated Scatterplots"
    }
  }, {
    "paragraphs": [{
      "page": 3,
      "region": {
        "x1": 53.499000549316406,
        "x2": 295.7337951660156,
        "y1": 502.5851745605469,
        "y2": 712.3289794921875
      },
      "text": "The statistical saliency model (SSM) [25] is a model of visual search based on the intuition that the visual system is interested in unusual things. Rosenholtz represents a visual scene in an appropriate feature space and then computes the saliency of a target as the number of standard deviations between its feature value and the mean of distractors. For a 1-D feature, this corresponds to a simple z-score, while for a higher number of dimensions, the saliency value is given by the Mahalanobis distance. Their model can be seen as a formalization of Duncan and Humphreys’ [8] rule of thumb that states that search is easier when target-distractor similarity decreases, or when distractor-distractor similarity increases. The use of search tasks and reaction times as proxies for attention relies on the premise that search for salient items should be faster than search for items that do not draw attention. Rosenholtz’s study of visual search is directly relevant to motion outlier detection in visualization, and to ranking,"
    }, {
      "page": 3,
      "region": {
        "x1": 317.5870056152344,
        "x2": 559.8980102539062,
        "y1": 96.1091537475586,
        "y2": 556.9119873046875
      },
      "text": "indirectly, if we assume that ranking points defaults to finding the most outlying point in increasingly narrow search spaces. For our purposes, however, the existing empirical validation of the SSM is limited. First, the scenes used to test it are usually distractor arrays of constant density (as in a uniform grid) [7]; second, no more than two features (speed and direction of motion) are varied. In information visualization displays, especially scatterplots, the x and y positions of points are commonly correlated, forming point clouds with varying density and levels of occlusion, and the points may be overloaded with multiple visual encodings, such as color, size, and shape [31]. A subsequent paper demonstrates how the SSM predicts asymmetries in colour search in the presence of non-neutral backgrounds [28]. The model is also the foundation for the feature congestion model of visual clutter [27], where separate pixel-level saliency maps of color and contrast luminance are linearly combined to produce clutter maps for raster images. The maps can be further aggregated to produce a scalar measure of overall display clutter. Critically, it is not clear how low-level dimensions should be composed for the calculation of saliency in complex visualizations. In Rosenholtz’s study of motion outlier detection [25] it was suggested that the Mahalanobis distance should be calculated on the 2D space formed by speed and direction of motion, whereas in the feature congestion model saliency is calculated as a linear combination of 1D saliencies. It is likely that the latter is the appropriate method in a scene where motion and static features are varied, in which case we need to learn the dimension coefficients. The pixel-level saliency maps employed in the feature congestion model and in many other saliency models [16] are not compelling for visualization applications because they operate after rendering, a late stage of the visualization pipeline, and because they are commonly tuned for natural images [3]. Recently, saliency models for data visualization were proposed [4, 20] that owe their performance mostly to accurate predictions of fixations on text elements (e.g., labels) in static visualizations."
    }, {
      "page": 3,
      "region": {
        "x1": 317.5870056152344,
        "x2": 559.8943481445312,
        "y1": 562.3611450195312,
        "y2": 592.7769775390625
      },
      "text": "In the next section, we will explain howwe created stimuli with salient and non-salient targets following SSM’s definition of saliency."
    }],
    "title": {
      "page": 3,
      "region": {
        "x1": 53.79800033569336,
        "x2": 117.32208251953125,
        "y1": 487.7208557128906,
        "y2": 494.1470031738281
      },
      "text": "3 SALIENCY"
    }
  }, {
    "paragraphs": [{
      "page": 3,
      "region": {
        "x1": 317.47698974609375,
        "x2": 559.8983154296875,
        "y1": 622.1371459960938,
        "y2": 712.3289794921875
      },
      "text": "We designed an experiment to find whether saliency predicts the accuracy of motion outlier detection tasks in animated multivariate scatterplots. In particular, we investigate whether saliency in irrelevant dimensions influences accuracy. Irrelevant dimensions are those that are not part of the task; for instance, when participants are instructed to find the fastest point, all dimensions (color, size, etc.) but speed are irrelevant."
    }, {
      "page": 3,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 4"
    }, {
      "page": 4,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.7410888671875,
        "y1": 96.1091537475586,
        "y2": 389.53900146484375
      },
      "text": "The experiment is split into two tasks, a direction task and a speed task. The former asks participants to select the point with the most deviant direction, and the latter asks them to select the fastest point. Throughout this paper we will refer to visual channels as dimensions, and to specific values in these dimensions as features. We’ll also call direction and speed the relevant dimensions in their respective tasks. Each animated scatterplot (a scene) we produced has 12 conditions, where only the target is varied: a baseline where the target has no irrelevant salient features, plus five instances where it holds a single irrelevant salient feature (position, color, size, direction/speed, or size increase); a second baseline where the target has five irrelevant salient features at once, plus five instances where one irrelevant feature is held out. Thus, half the stimuli follows a one-at-a-time design, and the other half follows a hold-one-out design.We call these condition groups saliency-deficient and saliency-charged. We use the following notation to refer to individual conditions: in the saliencydeficient group, + conditions refer to the added irrelevant salient feature. For example, +position refers to a stimulus where the only irrelevant salient feature is position. In the saliency-charged group, - conditions refer to the removed irrelevant salient feature. For example, -position refers to a stimulus where only position is not salient. In all stimuli, the target has outlying value in the relevant dimension."
    }, {
      "page": 4,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.7341613769531,
        "y1": 394.9881591796875,
        "y2": 521.0460205078125
      },
      "text": "The reader may question why we do not vary dataset size, correlation, or the parameters of the sampling distribution. When distribution and dataset size are manipulated, the fundamental quantity that is being varied is the saliency of the target. For instance, a scene with more point spread results in less target saliency, and the same with a more crowded scene. As our goal is to find the effect of saliency on accuracy and we are already varying saliency by manipulating visual features, varying the factors in question would be redundant. Therefore, we see no reason in increasing the complexity of the experiment by adding these additional variables."
    }, {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.7400817871094,
        "y1": 526.4951171875,
        "y2": 604.7319946289062
      },
      "text": "We generated ten different scenes per task, across 12 scene conditions, for a total of 120 stimuli per task. We collected 20 judgments of each stimulus for a total of 2400 judgments collected for each task, 200 per task-condition. We are interested in measuring the differences in error rates between the saliency-deficit and the saliency-charged baselines, and the impact of introducing or removing features."
    }],
    "title": {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 448.19134521484375,
        "y1": 607.2728271484375,
        "y2": 613.698974609375
      },
      "text": "4 EXPERIMENTAL DESIGN"
    }
  }, {
    "paragraphs": [{
      "page": 4,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.7410583496094,
        "y1": 650.0321044921875,
        "y2": 704.3590087890625
      },
      "text": "Wewrote a procedure for generating realistic stimuli inspired by animated scatterplots of the Gapminder data. The Gapminder plots map an often correlated pair of variables to the x and y coordinates, use size to encode a time-varying quantitative variable (usually population), and map a categorical"
    }, {
      "page": 4,
      "region": {
        "x1": 317.70599365234375,
        "x2": 559.8946533203125,
        "y1": 298.5511779785156,
        "y2": 328.9670104980469
      },
      "text": "variable (continent) to color. In our scenes, we simulate instead a continuous variable mapped to color because it allows fine-grained control of the saliency."
    }, {
      "page": 4,
      "region": {
        "x1": 317.656005859375,
        "x2": 559.890625,
        "y1": 334.41717529296875,
        "y2": 532.2049560546875
      },
      "text": "A scene has 50 data points and is composed of two frames that are linearly interpolated to produce the animation. We decomposed motion into distance, which determines how much the point moves in the 2D plane (Euclidean distance), and direction. We sampled the features for the initial frame and calculated the positions in the final frame based on sampled values for distance and direction. x1 and y1 are sampled from a multivariate normal distribution with correlation 0.7. The values for color, size increase, and distance are sampled from independent normal distributions. Direction (angle) is sampled from a beta distribution (α = 9.55, β = 10) that has shape similar to a normal, but produces values that are more concentrated around the mean. This pattern was chosen to preserve the correlation of the plot; that is, the point cloud, as a whole, should be moving in a well-defined direction. Due to the animation duration being constant for all points, distance is effectively a measure of speed."
    }, {
      "page": 4,
      "region": {
        "x1": 317.5870056152344,
        "x2": 559.893310546875,
        "y1": 537.6551513671875,
        "y2": 711.5330200195312
      },
      "text": "After all points are sampled, a target is selected according to the condition. If position is salient, then we select the point with the highest Mahalanobis distance (i.e., the most distant from the center of the point cloud); otherwise, the point closest to the center is selected. If color is salient, we assign to the target the maximum color in the color range; otherwise, we assign it the mean color. This pattern is followed for all the other irrelevant visual dimensions. All targets are outliers detectable through the interquartile range method (Tukey’s fences, k=1.5); thus, an analyst using boxplots to analyze the distributions of speed and direction would clearly identify the target as an outlier (positioned beyond a boxplot’s whiskers). We produced outliers by assigning to targets a constant value outside the sampled distribution range. On average, direction and speed outlier"
    }, {
      "page": 4,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 5"
    }, {
      "page": 5,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.7410583496094,
        "y1": 296.68017578125,
        "y2": 602.0650024414062
      },
      "text": "values were 3.11 and 3.82 standard deviations from the mean. For comparison with Huber and Healey’s discriminability thresholds, in average, the trajectory of speed outliers was 0.95 degrees of subtended visual angle longer (40% higher) than that of the next fastest point on a 113ppi laptop screen (e.g., Macbook Pro 13in.) at typing distance (20in.). The difference between direction outliers and the next most deviant points was 52 arc degrees (43% higher), in average. Table 1 lists the dimension ranges for the sampled points, as well as the mean and salient values. We use the inverted version of matplotlib’s Viridis colormap [30], where higher values are darker (bright points on awhite backgroundwould not \"pop out\"). Viridis was found to have superior performance, measured in time and accuracy of relative similarity judgments, in comparison with other popular colormaps [19]. We chose the direction range again respecting the principle that the plot trend shouldn’t be overly disrupted. The size range was chosen so as not to cause too much occlusion. In addition, the render order on the screen (from largest to smallest) also reduced occlusion. We inspected the stimuli to make sure that the targets were not occluded. Size increase is a multiplier of the initial size. Figure 1 displays a scene for the direction task in the saliency-deficient baseline condition. The target moves in an outlying direction but has average values for speed, color, size and position. All stimuli are provided in the supplemental materials."
    }],
    "title": {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 86.74564361572266,
        "y1": 635.1678466796875,
        "y2": 641.593994140625
      },
      "text": "Stimuli"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.5885925292969,
        "y1": 633.203125,
        "y2": 711.4400024414062
      },
      "text": "We presented the stimuli embedded in the Mechanical Turk interface (Figure 2). The page presented the first frame of the animation until the play button was pressed. After the end of the animation, the visualization was stationed in the second frame, allowing participants to select the target and submit the response or replay the animation up to two times before submission. The animation duration was 500 milliseconds."
    }, {
      "page": 5,
      "region": {
        "x1": 317.47698974609375,
        "x2": 559.896728515625,
        "y1": 96.1091537475586,
        "y2": 258.031982421875
      },
      "text": "When play was pressed the second or third time, the points faded to a blank screen then reappeared in their first frame positions before the animation took place. This sequence is illustrated in Figure 3. The variable number of views was introduced as a measure to mitigate errors due to interruptions, as these can be a problem in crowdsourced studies where we have no control over the environment. The number of views was capped at three to prevent the task from becoming too easy to the extent no differences can be detected between the conditions. Trials were published as two separate groups of HITs on Mechanical Turk (speed and direction). Within each group, trials appeared in random order. Participants were not limited in the number of tasks they could complete. We recorded time, accuracy and number of views."
    }, {
      "page": 5,
      "region": {
        "x1": 317.656005859375,
        "x2": 559.8978271484375,
        "y1": 263.4821472167969,
        "y2": 377.5840148925781
      },
      "text": "Participants were instructed to find the fastest point (\"find the fastest point\") in the speed task and the most deviant point (\"find the point that has the most unique trajectory compared to the rest\") in the direction task. Therefore, the task is to “find the maximum”, with all targets being outliers. This mitigates the risk of participants not comprehending the outlierness concept or the study being affected by different notions of what an outlier is. Participants had the opportunity to perform test trials, as it is common on MTurk, but these trials did not provide feedback."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 53.79800033569336,
        "x2": 100.36505126953125,
        "y1": 618.3388671875,
        "y2": 624.7650146484375
      },
      "text": "Procedure"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.47698974609375,
        "x2": 559.8908081054688,
        "y1": 421.2901611328125,
        "y2": 511.48199462890625
      },
      "text": "We collected 4800 observations from 67 participants, who performed an average of 71.6 tasks (sd = 42.4). The median completion time was 10.3s. Figure 4 displays the accuracy distribution per task-condition. Accuracy is calculated per stimulus (a scene-condition pair) as the ratio correct/incorrect. In the following sections, we examine the odds of a participant selecting the outlier and which features contributed most to incorrect selections."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 441.2769775390625,
        "y1": 406.4258728027344,
        "y2": 412.8520202636719
      },
      "text": "5 EXPERIMENT RESULTS"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.47698974609375,
        "x2": 559.7504272460938,
        "y1": 555.1881103515625,
        "y2": 705.156005859375
      },
      "text": "We used the R package lme4 [2] to fit a pair of generalized linear mixed models (GLMM), one for each task (speed and direction). We specified the models with a binary response variable (correct = [true, false]) and five binary covariates [salient, non-salient]: position, color, size, speed/direction, and size increase. This model is also known as a binomial logistic regression. In order to account for scene-specific and participant-specific effects, we inserted the variables scene and subject as random effects. As such, the random impact from scenes that happen to be more or less difficult, or participants that are more or less accurate, is reduced. Figure 4 shows the data, and Figure 5 shows the model estimates. Below we discuss the main findings."
    }, {
      "page": 5,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 6"
    }, {
      "page": 6,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.0432434082031,
        "y1": 680.68212890625,
        "y2": 711.0980224609375
      },
      "text": "Motion outlier detection is not well supported. We observed a mean accuracy lower than 25% in the condition baselinedeficient in both tasks. This condition is where the motion"
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.89794921875,
        "y1": 347.7291564941406,
        "y2": 414.010986328125
      },
      "text": "outlier does not have salient features other than motion. Low accuracy suggests subjects were mostly unable to separate motion from other dimensions in order to correctly identify the motion outlier. In other words, motion detection in multivariate scatterplots suffers interference from irrelevant dimensions."
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.8950805664062,
        "y1": 434.77716064453125,
        "y2": 584.7459716796875
      },
      "text": "Accuracy depends on the saliency of irrelevant features. We recorded higher accuracy in most conditions where the motion outlier had irrelevant salient features. In particular, subjects achieved averages of 78.5% and 58.5% accuracy in the baseline-charged condition, in the speed and direction tasks, respectively. Removing one salient feature at a time generally caused a drop in accuracy; conversely, adding one salient feature generally increased accuracy, but not by much, especially in the direction task, which suggests that in crowded displays motion outliers can only reliably be extracted if they have multiple salient features. More generally, animated scatterplots may reliably support only the detection of global outliers."
    }, {
      "page": 6,
      "region": {
        "x1": 317.656005859375,
        "x2": 559.8980102539062,
        "y1": 605.5121459960938,
        "y2": 707.6589965820312
      },
      "text": "Direction plays the largest role in the speed task. The fitted model indicates that direction saliency accounts for an increase of 4.7 times in the odds of correct speed outlier detection, which corresponds to a shift in probability from 0.19 (intercept) to 0.52. This result is somewhat aligned with previous findings that direction variability degrades searching for a unique speed. Targets with salient direction might have allowed subjects to segment the scene, cancelling some of the noise that impacts accuracy."
    }, {
      "page": 6,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 7"
    }, {
      "page": 7,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.5928649902344,
        "y1": 96.92113494873047,
        "y2": 222.1669921875
      },
      "text": "Position plays the largest role in the direction task accuracy. Position is estimated to account for an increase of 3.2 times in the odds for the direction task, which is equivalent to a shift in probability from 0.10 (intercept) to 0.26. This result is not trivial: while targets in salient positions (surrounded with blank space) are more visible, they are arguably more difficult to compare, due to their distance from other points. In addition, this result highlights the effect of clutter on this task. Our sampling process produces a point cloud with a high-density center. Points with low spatial saliency are located in these cluttered regions."
    }, {
      "page": 7,
      "region": {
        "x1": 53.499000549316406,
        "x2": 295.7410888671875,
        "y1": 233.59413146972656,
        "y2": 311.83099365234375
      },
      "text": "Size and color have small influence in the direction task. Both size and color contributed modestly to the outcome. We found no evidence of a difference between the odds estimate for these dimensions, as their confidence intervals largely overlap. In general, we observe precedence of spatial attributes (position, speed, and direction) over form attributes (color and size)."
    }, {
      "page": 7,
      "region": {
        "x1": 53.569000244140625,
        "x2": 295.15167236328125,
        "y1": 323.2571716308594,
        "y2": 473.2250061035156
      },
      "text": "Size makes no difference in the speed task. We found that size and size increase did not alter the odds of correct detection in the speed task (these variables have odds ratio approximately 1). This is in contrast to a small, but significant effect in the direction task. It is possible that this can be explained by larger points being perceived as moving slower, which would degrade the performance relative to the baseline; however, ourmodel did not point to a negative effect. It is also plausible that the distribution of values mapped to size did not produce enough saliency. Weber’s law predicts a non-linear relation between area change and perceived area change, which may have caused points with maximum area to appear closer to the mean and less salient."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 422.3374328613281,
        "y1": 540.3238525390625,
        "y2": 546.75
      },
      "text": "Channel Contributions"
    }
  }, {
    "paragraphs": [{
      "page": 7,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.74066162109375,
        "y1": 502.5851745605469,
        "y2": 712.3289794921875
      },
      "text": "When examining the incorrect choices of participants one would normally expect that the points they selected are close to the target in speed or direction; that is, more incorrect selections should be recorded for faster or more deviant points. This expectation was contradicted by the low correlations we observed between task dimensions and selection counts: 0.23 for speed and 0.28 for direction. The correlations were calculated on the subset of non-target points with selection count greater than 0. This suggests that incorrect selections are not necessarily due to the proximity to the outlier value in the target dimension; that is, irrelevant dimensions may be leading participants to make mistakes. To find which dimensions play a role in the number of times a non-target point is selected we fit generalized linear models (GLM) to the subset of 1,530 non-target points that were selected at least once. Since the observed response variable—selection count—is skewed and lies in the interval (1, ∞) we set the models with a Gamma response variable."
    }, {
      "page": 7,
      "region": {
        "x1": 317.656005859375,
        "x2": 562.3646850585938,
        "y1": 96.1091537475586,
        "y2": 163.885009765625
      },
      "text": "The covariates are saliencymeasures (SSM) on speed/direction and on all other dimensions. We use the saliency measure here because unlike targets, which were made either salient or not, non-target features lie within a saliency spectrum. Likewise, we split position saliency into saliency in the first frame (xy1) and in the second frame (xy2)."
    }, {
      "page": 7,
      "region": {
        "x1": 317.5870056152344,
        "x2": 559.8939819335938,
        "y1": 167.8401641845703,
        "y2": 293.89801025390625
      },
      "text": "We included terms for interactions of all saliencymeasures with speed/direction. In order to make the estimates comparable and easier to interpret we standardized all covariates (zero-mean and unit-variance). In Figure 6, the effects are multiplicative; that is,y = β0×β1x1×β2x2×β12x1x2..., where β0 is the intercept, βi are fixed effects, βi j is an interaction term, and xi are dimension values. The interaction plots in Figure 6 depict the curve that represents the relationship between speed/direction and the response variable (count), and how this curve is changed as a function of the interacting variable. Below we report the main findings."
    }, {
      "page": 7,
      "region": {
        "x1": 317.5867614746094,
        "x2": 559.3900146484375,
        "y1": 307.0331726074219,
        "y2": 492.8659973144531
      },
      "text": "Position and direction saliencies boost the effect of speed. In the speed task, the model estimates reveal, not surprisingly, that speed is a confuser and that the interactions of speed with direction saliency and position saliency in the first frame are significant. The interaction terms are positive: the misleading effect of speed increases as a function of the saliency of these irrelevant dimensions. In Figure 6 (top right), this is shown as an increase in slope: when the values of either direction saliency or position saliency increase by one standard deviation, the effect of speed on the response becomes steeper. In practice, this indicates that fast points moving from blank regions and in unique directions tend to bemistaken for true speed outliers. This result is alignedwith the channel contributions observed in the previous section: position and direction have the highest impact on the odds of a target being correctly identified."
    }, {
      "page": 7,
      "region": {
        "x1": 317.9549560546875,
        "x2": 559.8907470703125,
        "y1": 506.8131103515625,
        "y2": 644.0140380859375
      },
      "text": "Position saliency in the first frame and color saliency boost the effect of direction. In the direction task we found that the misleading effect of direction saliency is boosted by position saliency in the first frame. In Figure 6 (bottom right) this is seen as a slope increase when the value of saliency_xy1 increases. Color saliency also interacts with direction, but to a lesser extent. In addition, we found that the effect of speed is significant and independent from that of direction. Considering the results above, it appears that position saliency in the first frame is consistently a major factor for selection. Motion outliers that are inside the point cloud might be overlooked if there is a confuser departing from a salient position."
    }, {
      "page": 7,
      "region": {
        "x1": 317.5870056152344,
        "x2": 558.4647827148438,
        "y1": 657.9601440429688,
        "y2": 711.4749755859375
      },
      "text": "Position saliency in the second frame degrades the effect of direction. Surprisingly, we found that position saliency in the second frame has a negative interaction with direction. This appears in Figure 6 as a decrease in the slope of the curve when saliency_xy2 increases. Participants are thus less likely"
    }, {
      "page": 7,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 8"
    }, {
      "page": 8,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.74078369140625,
        "y1": 452.0831604003906,
        "y2": 554.2310180664062
      },
      "text": "to erroneously select a point moving in a salient direction the more salient its final position. We hypothesize that this effect may be due to points moving out of the cloud clearly having direction perpendicular to the trend. As participants were instructed to select “the point that moves in the most deviant direction”, they may have been looking for points that were in the opposite direction of the mean. Points moving in the opposite direction would likely be inside the cloud, not moving out of it."
    }],
    "title": {
      "page": 7,
      "region": {
        "x1": 53.79800033569336,
        "x2": 165.58287048339844,
        "y1": 487.7208557128906,
        "y2": 494.1470031738281
      },
      "text": "Which features mislead?"
    }
  }, {
    "paragraphs": [{
      "page": 8,
      "region": {
        "x1": 53.45000076293945,
        "x2": 295.5872802734375,
        "y1": 593.34814453125,
        "y2": 707.4500122070312
      },
      "text": "In this section, we examine the number of times participants viewed the animation before selecting their answers. We analyze the distribution of correct and incorrect selections across the three possible values for number of views. Figure 7 shows this distribution split by task, condition, and whether the trial was completed correctly. Due to the study being deployed on Mechanical Turk, we are unable to separate divided attention from task difficulty as the cause for replays. A reproduction of this experiment in a controlled setting is necessary for establishing a causal relationship."
    }, {
      "page": 8,
      "region": {
        "x1": 317.70599365234375,
        "x2": 559.8947143554688,
        "y1": 645.5801391601562,
        "y2": 711.8619995117188
      },
      "text": "Overall, we observe the prevalence of a V-shaped distribution, suggesting that participants were more likely to watch the animation either the minimum or maximum allowed times. In the saliency charged group, speed task, we see a clear pattern of correct answers coming more often from 1- view judgments. This pattern is not present in the direction"
    }, {
      "page": 8,
      "region": {
        "x1": 54.0,
        "x2": 558.0009765625,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 9"
    }, {
      "page": 9,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.73895263671875,
        "y1": 96.1091537475586,
        "y2": 222.1669921875
      },
      "text": "task. In the saliency deficient group we observe the opposite pattern: correct judgments are more likely to come from 3- view judgments, with a few exceptions; namely, targets with salient position in the direction and speed tasks and with salient direction in the speed task seem to require less effort than targets in the other deficient conditions. These patterns are consistent with the coefficients found in the above analyses, suggesting task difficulty may be behind them. We also see the V-shaped symmetrical pattern in incorrect answers, especially in the direction tasks, suggesting confidence in wrong selections."
    }],
    "title": {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 89.2463607788086,
        "y1": 578.48388671875,
        "y2": 584.9100341796875
      },
      "text": "Replays"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 52.672000885009766,
        "x2": 295.741455078125,
        "y1": 251.52613830566406,
        "y2": 712.3289794921875
      },
      "text": "We found that motion outlier detection is unreliable in multivariate animated scatterplots. The accuracy of motion outlier detection is degraded in the absence of other salient cues. This suggests a level of interference of spatial (position) and form (color, size) encodings over motion, and between the individual components of motion (speed and direction). Furthermore, we found evidence that while people were selecting outliers based on the relevant features (speed, direction), irrelevant features may have acted as “boosters,” leading people to select the wrong target. We hypothesize that this may be due to people’s attention getting caught by near-outliers that have high global saliency; since the animation is short, they would not have enough time to revise a first impression. We found spatial saliency, which is closely tied to clutter, to have a large impact on accuracy in both speed and direction tasks. Here, we emphasize the distinction between occlusion and clutter. We inspected the stimuli for occlusion and adjusted the z-order of elements to prevent small points hiding under larger points. Instead of an effect due to inability to see the targets, we believe the effect is due to a difficulty of allocating attention, in the sense of feature congestion: as the feature space becomes crowded there is less chance for a single object to stand out [26]. The results suggest that it may be possible to predict scenes where outlier detection is difficult on the basis of saliency measurements. A linear model with a binary response variable and feature saliency coefficients such as the one we fit can output the odds of correct detection given a “scene.” A linear model of saliency (for clutter measurement) was used also by Rosenholtz et al. [26]. A threat to the generality of this approach is the fact that the statistical saliency model is invariant to scale (due to the use of Mahalonobis distance); for instance, points mapped to a very narrow color range yield the same saliency values as if they were mapped to a wide color range. At a more general level, the results expose a failure of mapping data outliers to visual outliers, which we refer to as a saliency deficit. A data point or a group of data points is saliency deficient when its importance in the data space"
    }, {
      "page": 9,
      "region": {
        "x1": 317.5867614746094,
        "x2": 559.8867797851562,
        "y1": 96.1091537475586,
        "y2": 413.45001220703125
      },
      "text": "is not reflected in the visualization due to a lack of saliency. Saliency deficit is thus a condition of imbalance between data and visual importance. In Kindlmann and Scheidegger’s algebraic model [18], such a failure is classified as a violation of the visual-data correspondence principle: important changes in the data should yield important visual changes. The notion of saliency deficit is task dependent: here we examined motion outlier detection, but it is possible that other tasks in other visualization types may suffer from the same problem. Interference between visual channels is not new in visualization research, which often points to the theory of separable and integral dimensions [11]. When a pair of visual dimensions is integral, information from an individual dimension cannot be accessed easily. However, these studies have been traditionally restricted to the task of class-separation and with static features. For instance, in a point cloud with varying hue and size, it’s not easy to separate points based on each dimension independently. It is plausible that the mechanism behind saliency deficit depends on the number of visual channels employed. That is, the more visual channels, the harder it becomes to perform tasks that rely on saliency along a single dimension. This sends us back to the feature congestion model of clutter, which predicts difficulty in creating salient targets within a crowded feature space. In order to assert this mechanism with confidence, further research needs to examine this effect with a variable number of visual channels."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 128.84927368164062,
        "y1": 236.66188049316406,
        "y2": 243.0880126953125
      },
      "text": "6 DISCUSSION"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 317.47698974609375,
        "x2": 559.8969116210938,
        "y1": 442.8091735839844,
        "y2": 640.5980224609375
      },
      "text": "We would like to see the present experiment extended in many ways. We controlled the outlierness of the targets, animation speed, and the distribution of the features and their correlation in order to isolate the effect of feature saliency. This imposes limitations on the scope of inference of the experiment. It is plausible that interactions exist between the controlled factors and the response variables; in particular, as the outlierness of the target increases, the effect of other features probably decreases. The effect of animation speed may be complex: fast transitions may make tasks more difficult, but studies in the topic of change blindness have found that large changes can also go undetected when introduced gradually [29]. We have investigated only positive outliers. Due to a known asymmetry in motion target detection—it is easier to find fast targets among slow distractors than the inverse—we cannot extend our conclusions to slow outliers."
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.89013671875,
        "y1": 646.047119140625,
        "y2": 712.3289794921875
      },
      "text": "As stated inDiscussion, wewould like tomeasure accuracy in an experiment where the number of irrelevant dimensions is manipulated. This could generate insights on the number of dimensions beyond which some tasks start to lose accuracy. Likewise, it would be interesting to measure the effect of motion on other encodings. Finally, it is possible that the"
    }, {
      "page": 9,
      "region": {
        "x1": 54.0,
        "x2": 558.001953125,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 10"
    }, {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5873107910156,
        "y1": 96.1091537475586,
        "y2": 138.48101806640625
      },
      "text": "estimates for size and color do not generalize to other ranges. In particular, the color saliency may vary depending on the direction of the colormap (bright to dark or inverse) and the background."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 317.95501708984375,
        "x2": 396.6128845214844,
        "y1": 427.9448547363281,
        "y2": 434.3710021972656
      },
      "text": "7 LIMITATIONS"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.7410583496094,
        "y1": 167.8401641845703,
        "y2": 317.8080139160156
      },
      "text": "We reported the results of a controlled experiment designed to test the effect of irrelevant visual dimensions on the accuracy of motion outlier detection in multivariate animated scatterplots. We found that color, size, position, speed, and direction influence the accuracy with which people detect the fastest or the most deviant data point. In particular, we found that spatial visual dimensions, such as position, speed, and direction have larger influence than form attributes, such as color and size. Mean accuracy in detection of speed outliers was higher than 75% only when targets had multiple salient features. When detecting direction outliers, mean accuracy was never higher than 30% when targets lacked salient features."
    }, {
      "page": 10,
      "region": {
        "x1": 53.43000030517578,
        "x2": 295.7355041503906,
        "y1": 323.2571716308594,
        "y2": 437.3599853515625
      },
      "text": "These results suggest a saliency deficit effect that prevents motion targets from being detected accurately when their overall saliency is low; as a consequence, animated scatterplots should be used with caution if outlier detection is a critical task. We believe saliency deficit may affect tasks in other multivariate visualizations. Models of task accuracy that rely on foundational variables, such as saliency, in conjunction with models of user intent may inform the introduction of automated interventions when the predicted accuracy of a task given a plot is low."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 135.48463439941406,
        "y1": 152.9759063720703,
        "y2": 159.40203857421875
      },
      "text": "8 CONCLUSION"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 53.31999969482422,
        "x2": 295.73773193359375,
        "y1": 466.71917724609375,
        "y2": 497.135986328125
      },
      "text": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) and Fundação CAPES (9078-13-4/Ciência sem Fronteiras)."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 167.46585083007812,
        "y1": 451.8548278808594,
        "y2": 458.2809753417969
      },
      "text": "ACKNOWLEDGEMENTS"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 57.50400161743164,
        "x2": 294.9170227050781,
        "y1": 525.8045654296875,
        "y2": 531.009033203125
      },
      "text": "[1] Lyn Bartram, Colin Ware, and Tom Calvert. 2003. Moticons:: detection,"
    }, {
      "page": 10,
      "region": {
        "x1": 57.503997802734375,
        "x2": 295.39263916015625,
        "y1": 535.7666015625,
        "y2": 700.3740234375
      },
      "text": "distraction and task. Int. Journal of Human-Computer Studies 58, 5 (2003), 515–545. [2] Douglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67, 1 (2015), 1–48. [3] Zoya Bylinskii, Tilke Judd, Ali Borji, Laurent Itti, Frédo Durand, Aude Oliva, and Antonio Torralba. [n. d.]. MIT Saliency Benchmark. http://saliency.mit.edu/. [4] Zoya Bylinskii, Nam Wook Kim, Peter O’Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, and Aaron Hertzmann. 2017. Learning visual importance for graphic designs and data visualizations. In Proc. 30th ACM Symp. on User Interface Software and Technology. ACM, 57–69. [5] Helen Chen, Sophie Engle, Alark Joshi, Eric D Ragan, Beste F Yuksel, and Lane Harrison. 2018. Using Animation to Alleviate Overdraw in Multiclass Scatterplot Matrices. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 417."
    }, {
      "page": 10,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.544677734375,
        "y1": 97.41058349609375,
        "y2": 710.3359985351562
      },
      "text": "[6] Lisa J Croner and Thomas D Albright. 1997. Image segmentation enhances discrimination of motion in visual noise. Vision Research 37, 11 (1997), 1415–1427. [7] M Dick, S Ullman, and D Sagi. 1987. Parallel and serial processes in motion detection. Science 237, 4813 (July 1987), 400–402. [8] J. Duncan and G. W. Humphreys. 1989. Visual search and stimulus similarity. Psychological Review 96, 3 (1989), 433–458. [9] Ronak Etemadpour and Angus Graeme Forbes. 2017. Density-based motion. Information Visualization 16, 1 (2017), 3–20. [10] Ronak Etemadpour, Paul Murray, and Angus Graeme Forbes. 2014. Evaluating density-based motion for big data visual analytics. In Proc. of the IEEE Int. Conf. on Big Data. IEEE, 451–460. [11] Wendell R Garner. 2014. The processing of Information and Structure. Psychology Press. [12] M. Gleicher, M. Correll, C. Nothelfer, and S. Franconeri. 2013. Perception of average value in multiclass scatterplots. IEEE Trans. on Visualization and Computer Graphics 19, 12 (Dec. 2013), 2316–2325. [13] Christopher Healey and James Enns. 2011. Attention and Visual Memory in Visualization and Computer Graphics. IEEE Trans. on Visualization and Computer Graphics (July 2011), 1–20. [14] Jeffrey Heer and Michael Bostock. 2010. Crowdsourcing graphical perception: using Mechanical Turk to assess visualization design. In Proc. SIGCHI Conf. on Human Factors in Computing Systems. 203–212. [15] Daniel E Huber and Christopher G Healey. 2005. Visualizing data with motion. In Visualization, 2005. VIS 05. IEEE. IEEE, 527–534. [16] Tilke Judd, Frédo Durand, and Antonio Torralba. 2012. A Benchmark of Computational Models of Saliency to Predict Human Fixations. Technical Report MIT-CSAIL-TR-2012-001. [17] Younghoon Kim and Jeffrey Heer. 2018. Assessing effects of task and data distribution on the effectiveness of visual encodings. Computer Graphics Forum (Proc. EuroVis) (2018). [18] Gordon Kindlmann and Carlos Scheidegger. 2014. An algebraic process for visualization design. IEEE Trans. on Visualization and Computer Graphics 20, 12 (2014), 2181–2190. [19] Yang Liu and Jeffrey Heer. 2018. Somewhere over the rainbow: An empirical assessment of quantitative colormaps. In Proc. SIGCHI Conf. on Human Factors in Computing Systems. [20] Laura E Matzen, Michael J Haass, Kristin M Divis, Zhiyuan Wang, and Andrew T Wilson. 2018. Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations. IEEE Transactions on Visualization & Computer Graphics 1 (2018), 563–573. [21] Peter McLeod, Jon Driver, and Jennie Crisp. 1988. Visual search for a conjunction of movement and form is parallel. Nature 332, 6160 (1988), 154. [22] D. Moritz, C. Wang, G. Nelson, H. Lin, A. M. Smith, B. Howe, and J. Heer. 2019. Formalizing visualization design knowledge as constraints: actionable and extensible models in Draco. IEEE Trans. on Visualization and Computer Graphics (2019). [23] Thomas V Papathomas, Andrei Gorea, and Bela Julesz. 1991. Two carriers for motion perception: color and luminance. Vision Research 31, 11 (1991), 1883–1892. [24] George Robertson, Roland Fernandez, Danyel Fisher, Bongshin Lee, and John Stasko. 2008. Effectiveness of animation in trend visualization. IEEE Trans. on Visualization and Computer Graphics 14, 6 (2008), 1325– 1332. [25] Ruth Rosenholtz. 1999. A simple saliency model predicts a number of motion popout phenomena. Vision Research 39 (1999), 3157–3163. [26] Ruth Rosenholtz, Yuanzhen Li, Jonathan Mansfield, and Zhenlan Jin. 2005. Feature Congestion: A Measure of Display Clutter. In Proc. of the SIGCHI Conf. on Human Factors in Computing Systems. 761–770. [27] Ruth Rosenholtz, Yuanzhen Li, and Lisa Nakano. 2007. Measuring visual clutter. Journal of Vision 7, 2 (2007), 17.1–22."
    }, {
      "page": 10,
      "region": {
        "x1": 54.0,
        "x2": 558.001953125,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 11"
    }, {
      "page": 11,
      "region": {
        "x1": 53.797996520996094,
        "x2": 295.3856506347656,
        "y1": 97.41058349609375,
        "y2": 192.27899169921875
      },
      "text": "[28] Ruth Rosenholtz, Allen L. Nagy, and Nicole R. Bell. 2004. The effect of background color on asymmetries in color search. Journal of Vision 4, 3 (March 2004), 224–240. [29] Daniel J. Simons, Steven L. Franconeri, and Rebecca L. Reimer. 2000. Change blindness in the absence of a visual disruption. Perception 29, 10 (2000), 1143–1154. [30] Nathaniel Smith and Stéfan van der Walt. 2015. A better default colormap for Matplotlib. https://youtu.be/xAoljeRJ3lU [31] Danielle Albers Szafir, Steve Haroz, Michael Gleicher, and Steven Franconeri. 2016. Four types of ensemble coding in data visualizations."
    }, {
      "page": 11,
      "region": {
        "x1": 317.95501708984375,
        "x2": 558.1953735351562,
        "y1": 97.41058349609375,
        "y2": 112.5780029296875
      },
      "text": "Journal of Vision 16, 5 (2016), 11. [32] Anne M Treisman and Garry Gelade. 1980. A feature-integration"
    }, {
      "page": 11,
      "region": {
        "x1": 317.95501708984375,
        "x2": 559.5426635742188,
        "y1": 117.3355712890625,
        "y2": 132.50299072265625
      },
      "text": "theory of attention. Cognitive Psychology 12, 1 (1980), 97–136. [33] Adrian Von Mühlenen and Hermann J Müller. 2000. Perceptual integra-"
    }, {
      "page": 11,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.199462890625,
        "y1": 137.2615966796875,
        "y2": 192.34600830078125
      },
      "text": "tion of motion and form information: evidence of parallel-continuous processing. Perception & Psychophysics 62, 3 (2000), 517–531. [34] Colin Ware. 2004. Information Visualization: Perception for Design (2nd ed.). Morgan Kaufmann Publishers Inc., San Francisco. [35] Jeremy MWolfe. 1998. What can 1 million trials tell us about visual search? Psychological Science 9, 1 (1998), 33–39."
    }, {
      "page": 11,
      "region": {
        "x1": 54.0,
        "x2": 558.0020141601562,
        "y1": 750.914794921875,
        "y2": 756.7197875976562
      },
      "text": "Paper 541 Page 12"
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 116.56490325927734,
        "y1": 511.630859375,
        "y2": 518.0570068359375
      },
      "text": "REFERENCES"
    }
  }]
}