{"authors": "Viviane Herdel; Anastasia Kuzminykh; Jessica R Cauchard; Andrea Hildebrandt", "pub_date": "", "title": "Drone in Love: Emotional Perception of Facial Expressions on Flying Robots", "abstract": "Figure 1: \"The drone looks like it is in love.\" Example of a participant's reasoning when deciding on the drone's emotional state based on its facial expression (left). Wheel of six emotions derived from Plutchik's theory of emotion [65] (right).", "sections": [{"heading": "INTRODUCTION", "text": "Over the last few years, small-size drones have become increasingly popular, being used in a wide range of applications from photo and videography to deliveries and search and rescue [78]. Early works on collocated Human-Drone Interaction (HDI) observed that people interacted with the drone as with a person or an animal [18,24]. Recently, researchers have highlighted novel opportunities created by social drones that operate in human spaces and can support people in their daily lives, such as when exercising [57], to get home safely [43], as a navigational aid [2,14,20], and even as a personal companion [41,45]. Yet, designing a social drone [7] is not trivial, and we are only at the beginning of understanding which factors infuence people's perception of drones [81]. However, the literature on interacting with ground robots is rich and teaches us that social robots can communicate with people using emotions and expressive behaviors designed around features such as: facial expression, posture, gesture, and voice [10]. This results in interactions that are informative, \"human-like\", and pleasant [32]. Unfortunately, fndings from ground robotics cannot be directly translated into drones [81]. For instance, prior work showed that robots with eyes and no mouth are perceived as unfriendly [40], while drones with equivalent facial features are perceived as likable and warm [70].\nWe address this gap in the literature by designing facial expressions to convey emotional states on a social drone. Our focus on facial expressions is motivated by their signifcance as a non-verbal communication channel in human-robot [40] and in human-human communication [61,64], as they trigger the tendency to read emotions and interpret intentions and personality traits [59,77]. Thus, the use of facial expressions in social drones might have a potential to, not only communicate the drone's state, but also to elicit particular reactions and behaviors from a user.\nWe designed six drone facial expressions and evaluated them in two online user studies (N = 98, N = 98) where we investigated how people recognize and interpret a drone's emotional state using both static and dynamic stimuli. Our results show that 5 diferent emotions (Joy, Sadness, Fear, Anger, Surprise) can be recognized with high accuracy in static stimuli and 4 emotions (Joy, Surprise, Sadness, Anger) in dynamic videos. Surprisingly, participants created narratives around the drone's emotional states, and either imagined that the drone's state is caused by external factors (environment or people) or that the drone afects its environment or people within it. Participants further envisioned themselves as involved in the scene, they described empathy towards the drone, which then triggered mentions of prosocial behaviors.\nOur work contributes in the following:\n\u2022 A set of fve rendered robotic faces representing Joy, Sadness, Fear, Anger, and Surprise. \u2022 Two user studies (N = 196) showing how people recognize, interpret, and are afected by emotions on drones. \u2022 Design recommendations for social drones using emotions and facial features. \u2022 Methodological insights on the use of static vs. dynamic stimuli in afective robotics.", "n_publication_ref": 22, "n_figure_ref": 0}, {"heading": "RELATED WORK", "text": "We present the state of the art on emotional robotics for both ground and aerial robots, and discuss the use of facial expressions in conveying emotions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Afective Robotics", "text": "The enriching efects of integrating convincing emotions into nonhuman agents has been extensively researched in the robotics domain [5,12,75]. Eyssel et al. [29] showed that users tend to perceive the interaction with an emotional robot as more pleasant, feel closer to it, and ascribe human attributes to it, such as intentionality. Attribution of intentions, in turn, can foster feelings of social connection, empathy, and prosociality [44,80]. Furthermore, there is evidence that robots' abilities to display emotions contribute, not only to their overall perceived relatability and sociability, but also to the efectiveness of their communication with a user. Leite et al. [50] found that when a robot acts as a game companion, their emotional behavior can help users better understand the game. In their recent work, Fischer et al. [31] showed that people expect robots to express emotions reactively, similarly to the conventionally required display of emotions between humans [73]. Indeed, we know from the research on human-human communication that displaying emotions has crucial communicative and social functions [33,46], such as forming guidelines for future behavior to avoid or elicit emotional outcomes [6]. Research showed that afective robots similarly induce people to make sense of their intentions to guide human behaviors [29,66]. The communicative and social functions of displaying emotions motivates the interest in the exploration of its application to the design of social drones [7,19]. In particular, it was recently argued that, in human spaces, drones need to present social features [7]. The researchers name the intuitive comprehension of drones' intentions -to what degree people are able to interpret intentions that the drone is trying to convey via the interaction -as one of the major human-centered concerns in the design of social drones. One major challenge is then to investigate how to appropriately embed the display of emotions into the interaction design of social drones, including how to display emotions and what emotions are reasonable to display.", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Conveying Emotion in Aerial Robotics", "text": "In both human-human and human-robot communication, the display of emotions occurs though external, i.e., visible and audible, behavioral manifestations. Such manifestations can take diverse forms [46], such as verbal and non-verbal elements of language, gesture, posture, and gaze. Correspondingly, in robotics, researchers have explored diverse ways to convey emotions. For instance, there is a large body of work on afective perceptions of robots' body movements, sound and color, and diverse combinations of these elements [37,38,52]. Furthermore, researchers found that people's ability to anthropomorphize objects [23,30,68] provides a supporting mechanism in the design of social robots, allowing to translate some of the social conventions, expectations, and perception mechanisms from human-human to human-robot contexts [22,47,48]. Drones, however, because of their fight capability tend to be nonanthropomorphic in nature, and as such, their design difers from their ground counterparts [81].\nIn drone design, the communication of emotions has been predominantly explored through fight path [19,74]. Sharma et al. [74] proposed expressive fights following the Laban Efort System [60] and showed that people can diferentiate between drone states along the valence and arousal dimensions. Cauchard et al. [19] later defned an emotional model space for drones and showed that humans can accurately associate a drone's movements and behavior to an emotional state corresponding to a personality model. Arroyo et al. [1] investigated other means to convey a drone's emotional state using head movement, eye color, and the propellers' noise. They demonstrated that diferent states of these elements are being selectively associated with positive, negative, and neutral emotions. Additional eforts have been conducted towards establishing design recommendations for social drones, suggesting the suitability of faces [41] that could convey friendly features [84], such as Kim et al. [45] who further proposed that an ideal companion drone should present \"adorability\" features. Wojciechowska et al. [81] quantifed how physical properties, such as facial features, infuence people's perception of drones, such as eyes, which increase perceived friendliness, likeability, and a person's willingness to interact with the drone. Although these works did not investigate emotional expression per se, they open the space to the use of facial features on drones.", "n_publication_ref": 21, "n_figure_ref": 0}, {"heading": "Facial Expressions and Emotions", "text": "In human-human communication, information conveyed through one's face plays a fundamental role in interactions [25], leading to the human's strong ability to use the facial information to infer emotional states, personality traits, and intentions [35,61]. In affective robotics, while facial expressions received much attention [4,11,15], the consensus on their recognition and interpretation is still lacking [76]. Prior works further highlighted that the accuracy of emotion recognition for artifcial emotions difer across robot faces' designs [11,17] and that it might depend on the displayed intensity of the emotion [4]. Moreover, it is unclear how fndings from afective ground robotics apply to drones given that there is evidence that robots with diferent shapes trigger diferent emotions in humans [39], and given that drones tend to be non-anthropomorphic in nature [81].\nFacial features are a key factor in creating afective robots, and it was shown that robots without a face are perceived as less sociable and amiable compared to robots with a face [13]. This is in line with fndings from drone literature showing that the presence of facial features infuence the perception of drones as more likable, trustworthy, and intelligent [70,81] compared to drones without facial features. While a frst attempt has been made at designing drone facial features (eyes) to enhance non-verbal communication with humans [79], the question of the recognition and interpretation of emotions displayed by drones remains open. This further highlights the challenge of identifying what emotions are relevant and appropriate to display in HDI. For instance, research in psychology shows that, inter-personally, some emotions are better recognized than others [27], the recognition speed difers for positive and negative emotions [51], and similar expressions are often confused [69,71].\nFor the purpose of this work, we focused on the six basic emotions [25]: Joy, Sadness, Fear, Anger, Surprise, and Disgust, along three levels of intensity: Low, Medium, and High. While some robotics researchers have used extended sets of emotions (e.g., [3,11,31]), our rationale for using basic emotions is two-fold. First, there is convincing evidence that the basic emotions are universally recognized by humans, independently of their culture [25]. Second, this set of six primary emotions is commonly applied in research on emotional perception of robots [4,11,17]. Building upon prior work, this paper explores the possibility to efectively convey drone's emotions through facial expressions. In the following section, we discuss our approach to the development of facial expressions and describe our design choices to display basic emotions on drones.", "n_publication_ref": 28, "n_figure_ref": 0}, {"heading": "DESIGNING EMOTIONS FOR DRONES THROUGH FACIAL FEATURES", "text": "While afective robotics ofers several developed sets of faces with emotion expressions (e.g., [4,11,17]), the emotion recognition rates between these existing faces are inconsistent [76]. Additionally, there is an open question of whether these sets of faces are appropriate for drones. These considerations motivated our decision to develop a novel set of drone faces that would allow us to explore the perception of emotional facial expressions on drones for six basic emotions each with three intensity levels. In this section, we describe the corresponding design process, including the chosen representation style, facial features, face canvas, and approach to the construction of emotional expressions.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Representation Style", "text": "The frst design choice we faced was concerned with the style of the faces, which refects on the degree of realism. Several approaches exist to the design of facial expressions to communicate emotions [40,66]. Some artifcial faces attempt to mimic the look of the human face, e.g., by using the Delsarte's code of facial expressions [15] or by establishing pseudo-facial expressions by \"cloning\" real human expressions onto an avatar [85]. Others choose to develop animal-like designs [11,76] or cartoon-like facial expressions [4] inspired by principles of animations [67], often minimizing the amount of facial elements [66]. Our design used a cartoon-like 2D format since such faces led to higher emotion recognition compared to photo-realistic faces in prior work [42,86], and to minimize the risk of falling into the uncanny valley [54], which can trigger undesired emotional reaction [53,56].", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "Constructing the Face", "text": "The chosen cartoon-like format allowed us to minimize the number of included facial features which contribute to reducing the cognitive eforts needed for a person to process the resulting facial expressions [42]. Our next step was to identify the minimal set of features required to convey the chosen set of emotions. We used the well-established Facial Action Coding System (FACS) [26], which documents single muscle units required to create universally recognizable basic emotions (Table 1). This provided us with the necessary systematical approach needed for creating emotional facial expressions for drones. Furthermore, we chose to include pupils, as rendered robot faces without pupils are perceived negatively [40]. Our fnal resulting set included: eyes, eyebrows, pupils, and mouth. Finally, to assemble the set of chosen facial features into a face, we needed to decide on the face canvas. We turned to the existing body of work on rendered robot faces to fnd a suitable base for our work [40]. We opted for the face of Omate Yumi 1 , chosen as best-suited for a domestic robot, as it presented high likeability and ft our chosen set of facial features. ", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Stimuli", "text": "Image and video stimuli are commonly used in perception studies in the robotics literature [31,40,53,81]. We presented the developed set of faces (Figure 2) on a screen display embedded on the DJI Phantom 3 body 2 . Since design elements have been demonstrated to afect drone perception, and to avoid potential biases in emotion perceptions [62], the body was chosen for its average scores (e.g., on friendliness) in a prior perception study [81]. In Study I, we presented 18 stimuli images (6 emotions \u00d7 3 intensities) each displaying one of the developed facial expressions on the drone's body (e.g., Figure 1 left). In Study II, the drone was presented in 16.6 seconds video clips (e.g., Figure 3). The drone was shown approaching in a straight line for 10 seconds. While the drone moved, its face displayed a neutral expression. Once stopped, its face changed from neutral to low, medium, and high intensity of emotion (in 600 ms). We used the developed set of faces (Figure 2) as key-frames and blended them in the Unity 3 game-engine to create the animated facial expressions. After reaching high intensity, the drone remained on display for an additional 6 seconds. The chosen speed, distance, and movement of the drone are based on the literature on comfortable approach [82]. In total, Study II included fve video stimuli, one per emotion of: Joy, Sadness, Fear, Anger, and Surprise. Disgust was omitted based on low accuracy results from Study I.", "n_publication_ref": 7, "n_figure_ref": 4}, {"heading": "Tasks Description", "text": "This section describes the main tasks of Study I and II. Notes. AU corresponding to dark colored cells were used to design the drone faces, while AU corresponding to light colored cells were not manipulated.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Designing Emotions on the Face", "text": "For each of the basic emotions, we designed representations with three levels of intensity by intensifying the corresponding Action Units. We put special attention in the design of each feature to represent the emotions by extensively surveying robot faces in the literature and on the market. The resulting core set of rendered faces (Figure 2) includes 18 images of cartoon-like facial expressions (6 basic emotions \u00d7 3 intensity levels). We additionally designed a neutral face.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "METHODOLOGY", "text": "To explore the recognition, interpretation, and reactions to the emotional facial expressions on drones, we conducted two empirical studies, both employing a mixed-methods approach. Study I explored the perception of emotional facial expressions of diferent intensities presented statically (image-based). Study II was conducted four months after Study I and addressed the perception of animated emotional facial expressions on drones presented dynamically (video-based). We investigated both static and dynamic stimuli, as prior work had discussed [40] and proved [11,70] that these stimuli can elicit diferent responses in humans. In this section, we describe the participants, stimuli, tasks, study procedures, and the process of data preparation, and analysis.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Participants", "text": "Participants were recruited using the Amazon mechanical Turk platform. The recruitment selection was based on HIT rate (\u2265 97) and approved number of HITs (\u2265 100) with all participants located in the US. The resulting samples included N 1 = 98 (Study I) and  1). All emotion categories performed well, only Disgust did not perform as well as the other emotion categories. In Task 1, participants frst chose an emotion label to best describe the expression on the drone's face (see Figure A.1). We used a forced choice paradigm [11,17,19] to facilitate homogeneity and comparability of answers, given the wide variations in language that people use to defne emotions. The set of emotion labels was provided using a modifed version of Plutchik's wheel of emotions [65], presented side-by-side with each stimuli to avoid memory biases (Figure 1). Our wheel shows the six basic emotion categories [25] with labels describing high (inner circle), medium (middle circle), and low intensities (outer circle) of emotions corresponding to the 18 stimuli images. With respect to their best-choice answer, participants were then asked to justify their choice (free-form answer), specify how confdent they were with their answer (7-point Likert scale), and rate the intensity of the facial expression (7-point Likert scale). Finally, we asked participants to select any additional emotion labels that could also apply the drone's facial expression.\nIn Task 2, the 18 image stimuli were presented all at once, alongside with a table of 18 cells labeled with six emotions and three intensities each (e.g., \"Sadness + Low Intensity\"). Participants were asked to drag and drop each image into the cell that best ft the drone's facial expression. Note that participants did not receive any feedback regarding the validity of their answers.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "Study II \u25a1.", "text": "There were two main tasks for each video stimulus. Task 1 aimed to assess the participants' emotional response towards the drone and Task 2 measured how well dynamic facial expressions of emotions could be recognized, and how they would be interpreted by participants.\nIn Task 1, participants were frst presented with a video stimulus, which they could watch multiple times (see Figure A.2). They were then asked to rate how they felt towards the drone using the Self-Assessment Manikin (SAM) [9], a 9-point Likert scale using sketches of a manikin to measure emotions along three dimensions: Valence (from negative to positive emotions), Arousal (from low to high intensity), and Dominance (from submissive to in control).\nIn Task 2, participants were asked to watch the same video again and to select the emotion category that best matches the drone's facial expression on the original Plutchik's wheel of eight emotions [65]. As per Study I, participants then had to justify their best-choice answer (free-form answer) and specify their level of confdence (7-point scale). Finally, participants had the opportunity to select any other additional emotion categories that could also apply.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Procedure", "text": "Both studies followed a within-subject design and were conducted as online surveys. In both studies, participants were frst presented with the study description and asked to sign a consent form.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study I \u25b3.", "text": "After reading the instructions, participants performed a training session using the neutral face to get familiarized with Task 1. Participants then flled in a demographic questionnaire. The study proceeded to the frst block of tasks where they performed Task 1 for each of the 18 (static) stimuli images presented to them one-by-one in a random order. This frst block of tasks was followed by a control, attention-check question, and by the second task thereafter. The study concluded with additional questions including participants' prior experience with drones.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study II \u25a1.", "text": "As per Study I, the instructions were followed by a training session using the drone's neutral facial expression which provided a baseline for Tasks 1 and 2. The study then proceeded to the core phase where participants were presented with the fve video stimuli one-by-one in a random order. For each video, participants completed frst Task 1 and then Task 2. Half way through participants answered a control question. The study concluded with a demographic questionnaire and a series of additional questions such as what participants thought about the drone in the videos and their prior experience with drones.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dependent Variables & Analysis", "text": "The two studies generated a consequent amount of data. The dependent variables collected from both studies that are further described in the result section are summarized in Table 3. The measures of recognition rate and intensities (I a, I b, II a) are standard in the literature [4,11,17,76]. In addition, we measured how participants felt towards the drone (II b). Furthermore, the qualitative data gathers participants' reasoning on their choice of emotion label (I c, II c) and opinion of the drone (II d).\n4.5.1 Qalitative Analysis. The qualitative data was analyzed using a thematic analysis for all free-form answers. This exploratory method strives to identify patterns of themes that depend on the related data (as in [45,83]). First, incremental open coding [21] was performed by the primary author, and codes were discussed and refactored in consultation with additional members of the research team. Next, we performed axial coding, seeking relationships between the lower-level coding. This resulted in two coding schemes: one used for both studies, justifcation for the best-choice of emotion label (see Table 4), and one for the participants' opinion of the drone provided in Study II (see Table 5). After the primary researcher coded the entire dataset, a second member of the team independently reviewed the coded dataset. Disagreements were resolved in discussions amongst team members. Quotes were separated into elements respective to the categories. For each element that applied to a specifc category, we incremented the occurrences by 1 in the respective category.\nThe within-subject study design of both studies led to paired samples, as all participants evaluated all stimuli. Thus, we used a linear mixed-efects model which is appropriate for paired data with the advantage that a Poisson link function can be used. This was here necessary for count dependent variables (non-negative integers, heteroscedastic, skewed). With the described model, we analyzed whether categories difered signifcantly within emotions (e.g., whether external factors were signifcantly named more often than internal factors within each emotion category) or across emotions (e.g., whether external factors were signifcantly named more often in Surprise compared to Sadness). We did not perform a statistical analysis across studies as they collected uneven data and contained a diferent number of stimuli (18 images with diferent emotion intensities vs. 5 video clips created upon the images), thus generating a diferent amount of count-data. 4.5.2 Qantitative Analysis. For Study I, we calculated the proportion of how often images belonging to an emotion category were associated with that category (e.g., images: serenity, joy, and ecstasy; participant's choice: serenity or joy or ecstasy) (see Section 5.2.1). In addition, we calculated the percentages of how often participants selected both the correct emotion category and intensity (e.g., image: serenity; participant's choice: serenity) and how often they selected the wrong emotion category or the correct emotion category but wrong intensity (e.g., image: serenity; participant's choice: apprehension) for each of the 18 images of drones (see Section 5.3.1). Two resulting confusion matrices were computed, one for emotion category recognition, and one for emotion intensity label. For each confusion matrix, we analyzed whether the emotion/intensity were selected above random choice. To test whether the participants' best-choice answers were signifcantly above random choice, we used a binomial test for choosing both the correct emotion intensity label (random choice = 1/18) and the correct emotion category (random choice = 1/6). We used the Benjamini-Hochberg procedure to control false discovery rates (FDR) and thus the proportion of Type I errors. When more than one emotion intensity, or emotion category, was signifcantly above random choice for a stimulus, we additionally tested whether the number of best-choice answers difered signifcantly from each other (\u03c7 2 -goodness of ft test). We used the same statistical procedure to analyze emotion classifcation accuracy in Study II.\nTo analyze the SAM data in Study II, we applied a diference score path model (DSM) [55] for each of the three scales: Valence, Arousal, and Dominance. The aim was to quantify diferences in participants' emotional responses after seeing a video of a drone with one of the fve emotions (emotional condition) as compared with a drone displaying a neutral expression (baseline). This study design led to paired samples, as all participants were exposed to the baseline, as well as to all emotional stimuli. In DSMs, the variance of the comparison condition (i.e., emotional condition) is decomposed onto baseline (i.e., neutral condition) variance and diference to the baseline. Variance decomposition is achieved by model constraints, specifcally on the regression weights of the baseline and the diference score predicting the comparison condition (fxed to 1). Additionally, the residual variance of the comparison condition is being constrained to 0. All fve emotions were included in DSMs ftted separately for Valence, Arousal, and Dominance. Thus, each DSM contained fve diference scores (emotional vs. neutral condition), which allowed estimating sample average diferences in ratings of emotional as compared to neutral drones, as well as individual diferences therein.   considering the role and interpretation of facial features, emotion and intensity recognition, and the understanding of the drone's emotional state. We then report how participants were emotionally afected by the perceived drone emotions.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "RESULTS", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "We", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Role & Interpretation of Facial Features \u25b3 \u25a1", "text": "This section describes the role of individual facial features in the recognition of drone emotions. When asked to explain their reasoning in choosing emotions, participants most often referred to specifc facial features. Our results reveal the role of these features in making sense of emotions. Figure 4 illustrates absolute frequencies of facial feature naming for static \u25b3 (left) and dynamic \u25a1 (right) stimuli. Interestingly, participants did not only name existing facial features. They also invented new ones, such as teeth, tears, and lips. Invented features were thus included in the analysis for static stimuli \u25b3. This was not the case for the dynamic stimuli \u25a1 where invented features were only named 7 times overall. We describe below the role of each facial feature and their nature of mentioning: descriptive vs. interpretive (see We found for all emotions that some features were named more often than others (\u25b3, \u25a1). For example, with the static stimulus \u25b3 of Joy, the mouth was named most frequently compared to all other facial features. We found that the sequence pattern of facial feature naming frequency for static stimuli \u25b3 was: mouth > eyes > eyebrows > face > invented. Interestingly, the sequence was diferent when the stimuli were dynamic -with the face signifcantly more often named than all other facial features with some exceptions when compared to the mouth. The general sequence pattern for the dynamic stimuli \u25a1 was: face = mouth > eyes = eyebrows > invented.\n\u2022 \u2022 Invented Features were the least named in both static and dynamic stimuli, with the exception of Anger in static stimuli \u25b3, in which no signifcant diferences were found between face and invented features naming.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Descriptions and Interpretations of Facial Features Within", "text": "Emotions \u25b3 \u25a1. We further coded whether facial features were mentioned with additional information of either descriptive or interpretive nature (see Table 4). In static stimuli \u25b3, interpretations of facial features were more prevalent than descriptions of facial features within the emotion Joy (Figure 5), possibly due to the coding scheme, where \"smile\" was coded as interpretive. Other emotions had signifcantly more descriptive than interpretive facial feature naming -except Sadness, for which we did not fnd signifcant diferences.\nIn dynamic stimuli \u25a1, the above found diferences in descriptive and interpretive facial feature naming disappeared. Descriptive facial features were more frequently mentioned over interpretive facial features for Surprise only (Figure 5). We found more interpretive facial features over descriptive ones for Sadness -which was the only emotion for static stimuli \u25b3 where descriptive facial features were not mentioned predominantly. These fndings suggest that facial features become proportionally more interpretive for dynamic compared to static stimuli. For dynamic stimuli \u25a1, we found no statistical diferences for the frequency of eyebrows naming across emotions.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Role of Facial Features Across", "text": "\u2022 Face naming, in static stimuli \u25b3, showed one signifcant difference. Face was more frequently named for Joy compared to Anger (b = 0.539). We did not fnd any signifcant diference in face naming across emotion categories for dynamic stimuli \u25a1. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Emotion Recognition \u25b3 \u25a1", "text": "This section describes results on emotion recognition rates and on the frequency of emotion naming in both studies.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Emotion Recognition", "text": "Rates. Table 6 shows two confusion matrices illustrating the absolute frequencies of emotion category selections and the proportions of correct selections for each study. It shows, for example, that for static stimuli, Joy images (i.e., serenity, joy, and ecstasy for low, medium, and high intensities) were correctly recognized 95% of the time. In both studies, Joy, Surprise, Sadness, and Anger stimuli \u25b3 \u25a1 were recognized with high accuracy (above 70% and up to 99%). Interestingly, for Joy, Fear, and Surprise, the recognition accuracy was higher for static \u25b3 than for dynamic \u25a1 stimuli, while the opposite was true for Sadness and Anger. While Fear was recognized above average accuracy (62%) in static stimuli \u25b3, its recognition rate dropped for dynamic stimuli \u25a1, where we observed signifcant confusions with Sadness and Disgust.\nWe further found that Disgust did not perform as well as the other emotion categories, with only 29% accuracy in static stimuli \u25b3. The confusion matrix shows that participants selected Sadness signifcantly more often than Disgust. As such, Disgust was removed from further analysis and was not used as emotion category in Study II. Notes. Values are rounded to the nearest integer with entries < 0.5 rounded to 0. p-values resulting from a binomial test were adjusted using Benjamini-Hochberg (BH) correction. Proportions correct with p-value < .05 are highlighted in green (correct choice). Grey indicates confusion frequencies above random choice. Rows correspond to emotion category of the stimuli and columns to the best-choice emotion. Top. In Study I, the emotion category is an aggregate across low, medium, and high intensity labels within the same emotion category. Bottom. In Study II, stimuli and labels directly correspond to the applied emotion categories.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Emotion", "text": "Naming \u25b3 \u25a1. Our results demonstrate that in both studies, participants used signifcantly more provided emotional words over invented ones (see Figure 5). This was the case for all emotions, except for Joy for which the drone was mostly described as \"happy\" (invented emotion). In the case of Fear, we did not fnd signifcant diferences between provided and invented emotional words in static stimuli \u25b3.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Recognition of Intensities \u25b3", "text": "This section describes the recognition rate for emotion intensities within the correct emotion group and the validation that participants could discriminate between the diferent emotion intensities.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Intensities", "text": "Recognition Rate \u25b3. Table 7 presents the confusion matrix of recognition rates of emotion intensities from Study I (static stimuli). We found that for most emotions, participants tended to use the medium intensity label within the correct emotion category regardless of the intensity of the stimuli. For example, participants signifcantly chose the label sadness (medium intensity) for images of pensiveness and grief (resp. low and high intensity). We found that for the majority of stimuli, the correct intensity label was chosen signifcantly above random choice (green cells on the table). As mentioned in the emotion recognition results section, intensities belonging to the Disgust emotion category were significantly mistaken for emotions of Sadness. Pensiveness, fear, and rage were the only emotion intensities (out of 18) to be signifcantly recognized above random choice in a wrong emotion category. To summarize, we found a tendency towards labeling images with medium intensity labels within emotion categories, and we only observed few confusions across emotion categories.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Validity of Intensity Recognition", "text": "\u25b3. Task 2 of Study I was intended to validate discriminations between intensities within emotion categories. We found that each of the 18 intensity images were labeled with the correct intensity signifcantly more often than with a wrong intensity label. This result holds for both within and across emotion categories. Furthermore, confusions only happened between emotion categories for pensiveness, annoyance, and loathing; yet their intensity levels were still chosen correctly. For instance, the loathing (Disgust high intensity) image was incorrectly labeled as grief (Sadness high intensity) and as rage (Anger high intensity) in a signifcant amount of cases (14% and 15% respectively).\nOur results indicated that the recognition rates of all low and high intensities were signifcantly higher when participants could perceive all images at once (Task 2 compared to Task 1), except for the Anger emotion category. This is evidence that individuals can recognize and distinguish between emotion intensities based on facial expressions on a drone.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Drone's State \u25b3 \u25a1", "text": "This section reports on the free-form answers related to how participants described the drone's emotional state (Tables 4 and 5). We conclude with the described drone's capabilities, behaviors, and expectations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "5.4.1", "text": "Internal and External \u25b3 \u25a1. Internal states were mentioned signifcantly more often within all displayed emotions (Figure 5). Yet, participants attributed emotional states to external factors 23%  ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Direction of External Drone States (Cause & Efect", "text": ") \u25b3 \u25a1. We found that there was a direction of the drone's external state either directed towards the drone (Cause) or away from it (Efect).\n\u2022 Cause and Efect Within Emotions found that Fear and Surprise evoked signifcantly more Cause over Efect naming for static stimuli \u25b3 (Figure 5); while Anger provoked the opposite. Participants mentioned signifcantly less Cause than Efect naming. We did not fnd statistically signifcant diferences between Cause and Efect for the emotions Joy and Sadness in static stimuli \u25b3. Interestingly, we found the same pattern for dynamic stimuli \u25a1 for Sadness, Fear, and Surprise. However, there was signifcantly more Cause than Efect naming in Joy for dynamic stimuli \u25a1. We did not fnd signifcant diferences between them in Anger.   4) for each of the emotion categories: Joy, Sadness, Fear, Anger, and Surprise for static (left) stimuli and dynamic (right) stimuli. Note that the count data for static stimuli are aggregated over the three intensities (low, medium, and high). * indicates p-values <.01 for within emotion comparisons, which were provided by linear mixed-efect models with Poisson link function. \u2022 Physical Reactions \u25b3 \u25a1 For both static and dynamic stimuli, participants mentioned on several occasions that the drone had physical reactions based on its emotional state. These included trembling, gasping, cowering in fear, feeing when it was scared, gritting its teeth in anger, striking out in frustration, and starting a fght to beat someone up. Participants also acknowledged the fying capability of the drone: \"the drone is feeling fear about something it has to do, and it doesn't want to do it. Maybe doesn't want to fy high\" (S98) and even imagined that the drone could be afraid because \"the fight must have gone wrong\" (D97). \u2022 Perceived Flying Speed \u25a1 Participants mentioned the fying speed of the drone in dynamic stimuli as related to its emotional state. This was a surprising fnding since the displayed speed was constant for all emotions. We found, for instance, that some participants perceived the speed being slow when the drone looked sad \"He seems to be slow and down hearted\" (D2), \"The expression was negative. It moved very slowly as well\" (D94). In contrast, the fying speed was occasionally perceived as fast, such as when the drone was angry \"Its rotors even seemed to spin faster the madder it got\" (D29), \"It seemed to moving slowly and then pick up speed before landing, as if something just made it [angry]\" (D87). \u2022 Expectations \u25a1 Participants expressed various expectations around the drone's emotional state \"It seems kind of strange that a drone would be sad\" (D58), \"What would be the point? To feel bad emotions at the hand of an AI system? I think not. I am the one in control in this situation\" (D10). For some, facial expressions on drones were almost surprising and rather positive \"I think the drone's reaction was cute. I wasn't expecting that. I'm curious [about] what the drone fnd[s] surprising about my presence\" (D78). We also found preferences and concerns around the emotional state of the drone: \"I am not sure I would like a drone that feels fear\" (D45), \"It should always make a happy face by default\" (D13). Interestingly, some people actively recalled that it is a drone they are facing when describing their thoughts: \"He brought out the instinct in me to comfort him. Which is silly cause he is a drone, but his face was so sad\" (D89), \"I loved that it was excited to see me, but yet, I still don't trust it fully because it's a drone\" (D98). \u2022 Capabilities \u25b3 \u25a1 We found a trend towards the drones being described as agents with autonomy, consciousness; as well as cognitive, afective, and behavioral abilities: \"He looks as though someone told him something he didn't want to hear\" (S89). For some, the drone can develop sympathy and antipathy for humans (\"I don't trust it at all. I'm a good person, and he just thinks I'm terrible\" (D98). The drone was described as capable of creating deep bonds with others, feeling complex emotions of love and hate (e.g., \"they may grieving a loss of a loved one\" (S32). However, participants did not uniformly ascribed the same level of capabilities to the drone, leaving some participants wondering what the drone's abilities actually are: \"I felt concerned with what abilities it would have to be able to harm me\" (D74). Participants reported, both for static and dynamic stimuli \u25b3 \u25a1, that the drone reacted to or afected them in some way: \"It's clear happiness. In fact, just looking at its happy face made me feel happy for a moment and uplifted\" (S98), \"I feel the drone was curious about me, but then when it reached me, he was suddenly sad about my presence...almost like he/she has met me before and is unhappy with me for some reason\" (D98). In the next section, we analyze how people were emotionally afected by the diferent drone's emotions.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "\u2022 Cause and Efect Across Emotions", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Afect on Participants' Emotional State \u25a1", "text": "In Study II, participants were surveyed on their emotional reaction to the drone's video stimuli. We here describe the results of the SAM questionnaire and how participants described being afected by the drone in the free-form answers (see Table 5).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Emotional Assessment (SAM) \u25a1.", "text": "The results of the SAM questionnaire expand across three dimensions. Baseline scores (assessed on a 9-point scale) averaged across all participants as follows: Valence 5.55 (SD = 1.26), Arousal 3.67 (SD = 1.81), and Dominance 5.29 (SD = 2.05). We estimated the average diference scores between the baseline and each emotional stimulus category by using DSM (see Section 4.5.2) to measure the afect change elicited for each emotion. Figure 7 illustrates the overall diferences in each SAM dimension for each emotion compared to the baseline. Results can be summarized as follows:\n\u2022 Valence was afected according to the emotion, such that it was signifcantly higher when individuals were exposed to a positive emotion: Joy; signifcantly lower in case of negative emotions: Sadness, Fear, and Anger; and did not appear to change signifcantly as compared to the baseline with a neutral emotion, such as Surprise. \u2022 Arousal was signifcantly higher for all emotions displayed on the drone. \u2022 Dominance was signifcantly increased for Joy and Surprise; and signifcantly decreased for Fear as compared to the neutral baseline.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Participants Emotional", "text": "Response \u25a1. We found that participants discussed how the drone afected them emotionally in the free-form answer (see Table 5) and report example quotes in Table 8.\nThe emotions Joy and Surprise signifcantly triggered participants It's depressing and I would want to avoid it. Makes me sad looking at it D81 I feel protective towards it, like I want to assist it to fx the problem D36 I wanted to tell it that everything will be okay D98 Fear I didn't like it. I would never want to use something that made an expression like that D73 Anger I didn't like that it challenged me and seemed to threaten my status in the space D63 Surprise\nThe drone was very cute in it's almost childlike expression of excitement It created a feeling of cooperation and openness with me D48 to mention positive responses, while Fear and Anger signifcantly triggered negative emotion mentioning in the free-form answers (see Figure 8). Interestingly, Sadness did not appear to lead to signifcant diferences. However, we found much discussion around empathy when participants were exposed to expressions of Sadness.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Empathy and Prosocial", "text": "Behavior \u25a1. Some emotions evoked empathy, such as Sadness, which led to signifcantly higher empathy towards the drone compared to all other emotions (b = 1.299b = 3.5), gathering 64% of all empathy quotes. Fear and Joy also triggered empathy, with 17% and 12% of empathy quotes respectively. We found that empathy was linked to participants' motivation to prosocially interact with the drone. For example, when the drone displayed a sad facial expression, more than a third of participants suggested prosocial interactions (see Table 8). ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION", "text": "Our results demonstrate the potential ofered by using facial expressions of emotions on drones. Here, we further and highlight particularly interesting aspects of people's perception, recognition, and interpretation of emotional expressions on drones; as well as the use of facial versus bodily expression in HDI.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Perception and Interpretation of Emotions", "text": "This section dives into the role of facial features in the emotion recognition process and further contextualizes the ambiguity encountered in this process. We then discuss diferences in participants' sensitivity to emotion intensities. Finally, we develop on how the experience of visualizing a drone with emotional features prompts narrative development and storytelling as well as empathetic responses.\nRole of Facial Features. Our results show that four facial features: eyes, eyebrows, pupils, and mouth, are sufcient to generate fve recognizable emotions on drones. We observed that the facial features were not equally referred to when describing the choice of emotion, and our results point to the fact that some facial features are more powerful than others, with diferences both within and across emotions. For example, we observed a tendency towards facial feature naming when the said facial feature was manipulated, so that the mouth, which was the most manipulated feature, was the most frequently named facial feature in both studies. Similarly, we observed that the eyebrows naming occurred signifcantly more often for emotions in which they were manipulated (e.g., Anger and Surprise compared to Joy) for static stimuli. Prior human-human communication literature demonstrated that some emotions are better recognized using the bottom half of the face (Joy and Disgust), and others using the top half of the face (Sadness, Fear, and Anger) [16]. Our results suggest that, similarly to human-human communication, specifc facial features may have diferent contributions to the recognition of specifc emotions in human-drone emotional face recognition. Further investigation is required to establish particular patterns of recognition in human-drone and human-robot communication.\nAmbiguity in Emotion Recognition. Our results show high to near perfect recognition rates for four emotions: Joy (best in static), Sadness (best in dynamic), Anger, and Surprise. However, Disgust showed poor recognition rates in static and was not further investigated in dynamic stimuli. Finally, while Fear could be well recognized in static stimuli, its recognition rate dropped by 19% in dynamic stimuli. We found that Disgust was more often associated with expressions of Sadness; and Fear was occasionally associated with Disgust or Sadness (in dynamic stimuli). We suggest two main factors that could have contributed to this ambiguity. The frst one is the perceived legitimacy of the emotion in human-drone interaction, where the participants may not have envisioned this emotion as applicable to a drone. For example, participants made comments such as \"I think it looks a little weird, seeing a drone with a scared expression\", \"It would have to be a fake fear as robot's do not feel emotion\". Another potential contributing factor is the design of the facial expressions. Our choice of facial features did not include a nose into the drone's face, while it is included for Disgust in FACS [26] (see Table 1). Similarly, the recognition of Disgust as Sadness may result from the squeezed eye design that one participant referred to as if the drone had \"been crying for a while\". This suggests that future face designs should reconsider the shape of the eyes and potentially add a nose in the facial expression of Disgust. However, we believe that the constructed facial expression of Fear can be used for future designs.\nSensitivity to Emotion Intensities. The accuracy of recognition of emotional intensities seems to be mediated by how it is presented to the participant (individually vs. in a series). In Study I, when stimuli were presented one by one, participants tended to identify each emotion as of a medium intensity. However, when presented in a series (here, all 18 stimuli at once), the intensities were correctly recognized more often than not. These results indicate that the presence of comparison points guides the recognition of intensity, and that participants tend to interpret facial expression on drones with a focus on the emotion category rather than on the intensity.\nNarratives and Storytelling. When trying to make sense of the drone's emotional facial expression, participants developed narratives in which they integrated external factors that we divided into two categories: people and environment. We found that the drone's state was either explained by a prior external event (e.g., \"The drone received some good news\") or be preceding a future action of the drone (e.g., \"The drone looks angry at me and looks like it will do something to me\"). We refer to this notion as the direction of the external drone's state, which we found was signifcantly afected by the displayed emotion. For example, Anger (in both static and dynamic stimuli) was often seen as a targeted action towards people -including towards participants. Prior human-human communication works showed that emotions can convey information about the social situation of others [34,36]. Interestingly, this tendency of making sense of emotions of others and to infer a story behind that emotion seems to hold in HDI.\nIn addition, we found that participants often included themselves in their stories. For example, they wondered why their presence made the drone feel sad, or felt that the drone recognized them and wanted to greet it back. Our fndings further suggest that participants perceived the drone as an autonomous agent with a range of capabilities. Put together, these fndings show that participants perceived the drone as having its own state, which can be afected by, or afecting, its environment and people within it.\nTaking it Personal. The narratives revealed that participants expressed diferent emotional reactions to the drone's emotions; such as treating the drone's behavior as a reaction to their own actions or presence, or even experiencing empathetic emotions similar to the ones of the drone. The perception of the drone's state as an emotionally charged feedback to participants' actions suggests that some interpersonal mechanisms in human-human communication persist for HDI (e.g., seeking for harmonic reciprocal relationships with drones). These efects can become a powerful mechanism to shape human-drone interactions, where for example, the drone's expressions of emotions can serve to trigger behavior change, mediate human-human relationships, and be used in the development of novel emotional support systems [6,19,63]. For example, the drone's emotional state could be used to prompt feelings of Joy during difcult times or feelings of Fear to notify of immediate threats and danger.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Facial versus Bodily Expression", "text": "Prior to this work on facial expressions of emotions, the HDI community had focused on bodily expressions of drones in collocated interactions. In the following section, we discuss how our work intersects with this prior literature.\nEmotional States and Perceived Agency. Prior works have shown that people can identify a drone's emotional state and personality through its fight path [19,74]. Our fndings extend these works by showing that people can make sense of a drone's emotional state using facial expressions of emotion, at a higher emotional resolution. Both in this work and prior works, participants associated drone's intentions to, respectively, facial expressions and fying behavior. This opens an exciting opportunity to facilitate better emotion recognition and prompt the attribution of intentions and personality to a social drone, by combining bodily and facial expressions of emotions. Interestingly, the notion of participants developing narratives to make sense of the drone's state was also found in bodily expressions [74], such as participants mentioning that the drone was happy to see them. This exemplifes the opportunity to use both facial and bodily expressions to create the feeling that people are part of a story that is conveyed through the drone.\nEmpathy and Synchrony. Our work showed that a drone displaying facial expressions of emotions can evoke empathy and synchrony (i.e., participant reporting feeling a similar emotion). Prior work showed that empathy can be fostered, not only by the facial display of emotion, but also through kinesthesia in humanhuman interactions [8]. Given the similarities highlighted earlier in this discussion between human-human and human-drone interaction, we ponder on the role of the drone's movement in fostering empathy. Interestingly, recent works investigated collocated HDI using bodily expressions of both people and drones acting in synchrony [28,49]. In terms of interaction design, this means that empathy and synchrony in HDI can be achieved both with and without elements of anthropomorphism (e.g., facial features). This raises the question of how much interpretation we want to keep open when we interact with social drones, and how our interpretation of the drone is shaped through its bodily and facial expressions. Prior work suggested that a tempered anthropomorphism [72] can be an exciting element for art [28] and can enrich freedom for the interpretation of a drone's bodily expressions. This opens interesting avenues for future research in understanding the role of anthropomorphism in HDI.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "RECOMMENDATIONS", "text": "To further synthesize and operationalize our fndings, we provide a set of design recommendations divided into design and methodological recommendations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Design Recommendations", "text": "Considering the synthesis of our fndings from two studies on the recognition and perception of emotional facial expressions in drones, we discuss how they can be applied to the design of social drones and HDI through a set of fve design recommendations.\nDR1. Use facial features to convey emotions on drones. Conveying drone emotions is a challenging task given their traditional non-anthropomorphic form. While prior research had suggested the use of the drone's fight path and behavior to encode personality [19], we propose to add expressions onto a drone's body to enable its social perception by humans. We approached it by using minimal facial features. We show that 4 core facial featuresnamely: eyes, eyebrows, pupils, and mouth -were sufcient to generate 5 recognizable emotions. We further describe how individual facial features are interpreted by participants and show that adding basic anthropomorphic features onto a non-anthropomorphic body dramatically changes how it is being perceived.\nDR2. Design with fve basic emotions. We fnd that fve basic emotions -namely: Joy, Sadness, Fear, Anger, and Surprise -are applicable to drones. Emotions can be identifed with recognition rates ranging from 62%-95% using static and 43%-99% using dynamic stimuli. Joy is the best recognized emotion in static, and Sadness in dynamic stimuli. Disgust was poorly recognized and as such should be carefully considered in future designs. People correctly interpreted emotions using diverse and nuanced emotional vocabulary in free-form answers, including additional invented facial features, to describe these basic emotions. We found that participants could also discriminate between emotion intensities, although they tended to rate all intensities as medium. More work is still needed to fully appreciate which emotions are appropriate for drones, and in which context they might enrich and enhance human-drone communication.\nDR3. Consider reciprocity in emotional reactions of humans. Extending evidence from afective robotics [44], our results suggest that drones' emotional expressions tend to provoke an emotional response from humans. This emotional response often mirrors the interpreted emotional state of a drone (e.g., feeling uplifted when the drone is happy, or feeling low when the drone is sad). At times, this empathetic reaction brings in an infantilizing response, such as comparing the drone to a child who needs its mother's comfort. Interestingly, socially undesirable emotional states of a drone, such as Anger, can lead to people feeling combative or frightened. Finally, the reciprocal response from humans opens an exciting potential for designing drones for emotional support and for embedding emotional behaviors into the HDI process.\nDR4. Shape prosocial intentions through empathy. Our results demonstrate that empathetic response to the drone's emotional state can evoke prosocial intentions in observers. For instance, participants who expressed empathy to the sad drone would often express willingness to interact with the drone to make it feel better, comfort it, or give it a hug. Alongside communicative intentions, empathetic responses to the drone seem to also have a potential to evoke broader social behavioral intentions, such as wanting to help the drone fx a problem. These results contribute to a number of direction for applications of emotional displays on drones, e.g., design of drones as pets [19], or for behavior change, such as a physical exercise companion [58].\nDR5. Design to ft the narrative. The qualitative analysis of the responses revealed the participants' tendency to develop narratives to make sense of the drone's emotional state. In particular, participants would often create scenarios, which would explain why the drone was in the respective emotional state. These narratives appear both when a drone is perceived as a social entity (e.g., \"like it just witnessed a tragedy or received horrible news\") and as a mechanical entity (e.g., the drone's state is interpreted as a request to change its battery). Additionally, we found that people would often include themselves into the developed narrative, such as interpreting the drone's emotions as a reaction to their actions. This trend is particularly prevalent when the drone is presented dynamically. These results inform the directions for narrative-based design for HDI and open new research directions to identify the factors which afect and direct such sense-making narrative.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Methodological Recommendations", "text": "The comparative analysis of data from the static and dynamic stimuli conditions suggest a number of diferences in the participants' perception of emotional drones in respective conditions. This, for instance, includes the variations in recognition rates for diferent emotions, the richness of interpretations of facial features, and the level of participants' inclusion into the sense-making narrative. Correspondingly, we suggest that the comparison of the participants' reactions to static and dynamic stimuli allows to elicit richer and more contextualized data in investigating the perceived emotional expressions in drones. Furthermore, in dynamic stimuli condition, participants tend to base their reasoning on the overall drone face more often, compared to the static stimuli. Thus, we suggest that studies focusing on particular facial features might beneft from the use of static stimuli, whereas studies focusing on the holistic perception of drone emotional states might prefer using dynamic stimuli.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "LIMITATIONS AND FUTURE WORK", "text": "This work investigated the emotional perception of facial expression on drones using static and dynamic stimuli. These methodologies are well-established in the literature [11,40,81], highly scalable, reproducible, and safe [82]. Yet, they present limitations as there will be additional factors afecting people's perception when exposed to a real drone (e.g., noise and wind generated). As prior work has shown that a drone's movements and behaviors can be used to convey emotions [19,74], future work is needed to investigate the perception of facial expressions of drones with diferent fight paths and behaviors, and to fully understand whether some modalities are more persuasive in communicating emotions than others. Finally, future work should further investigate the use of drone facial expressions in context, to fully assess the appropriateness of emotional expressions in diferent scenarios of use (e.g., delivery vs. law enforcement vs. sports).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "CONCLUSION", "text": "This work presented the frst systematic exploration of emotional perception using facial expression on drones. We designed a set of rendered robotic faces using a minimal number of facial features to represent basic emotions. In two user studies, we showed that people can recognize fve basic emotions: Joy, Sadness, Fear, Anger, and Surprise on drones, as well as discriminate between diferent emotion intensities. We found that beyond recognition, people interpret the drone's emotions and create narratives around the drone's state. In our work, participants were further afected by the drone and displayed diferent responses, including empathy, depending on the valence of the drone's emotion. We conclude with design and methodological recommendations for future research into social drones. This work contributes to the growing body of work on factors contributing to the acceptability of drones in human spaces.  Top: Dynamic drone stimulus (here Joy), followed by Questions 1 to 4 and their possible answers. Note that the Plutchik's wheel of emotions [65] was clickable.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A APPENDIX", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Gilad Ostrin for developing the drone video stimuli, Anna Wojciechowska and Chloe Benmussa for their help with the video material, and Dr. Jeremy Frey for his helpful feedback on this work. Many thanks to the other members of the Magic Lab for their support and in particular to Carmel Shavitt.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Daedalus: A sUAV for Human-Robot Interaction", "journal": "", "year": "2014", "authors": "Dante Arroyo; Cesar Lucho; Silvia Julissa Roncal; Francisco Cuellar"}, {"title": "Germany) (HRI '14)", "journal": "Association for Computing Machinery", "year": "", "authors": ""}, {"title": "Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Markus Mauro Avila Soto; Matthias Funk; Robin Hoppe; Katrin Boldt; Niels Wolf;  Henze"}, {"title": "Synthesizing Expressions Using Facial Feature Point Tracking: How Emotion is Conveyed", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Tadas Baltru\u0161aitis; Laurel D Riek; Peter Robinson"}, {"title": "In your face, robot! The infuence of a character's embodiment on how users perceive its emotional expressions", "journal": "", "year": "2004", "authors": "Christoph Bartneck; Juliane Reichenbach; Albert Van Breemen"}, {"title": "The Role of Emotion in Believable Agents", "journal": "Commun. ACM", "year": "1994-07", "authors": "Joseph Bates"}, {"title": "How Emotion Shapes Behavior: Feedback, Anticipation, and Refection, Rather Than Direct Causation", "journal": "Personality and Social Psychology Review", "year": "2007-06", "authors": "Roy Baumeister; Kathleen Vohs; C Dewall; Liqing Zhang"}, {"title": "The Design of Social Drones: A Review of Studies on Autonomous Flyers in Inhabited Environments", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Damla Mehmet Aydin Baytas; Yuchong \u00c7ay; Mohammad Zhang;  Obaid"}, {"title": "Moving in and out of synchrony: A concept for a new intervention fostering empathy through interactional movement and dance", "journal": "The Arts in Psychotherapy", "year": "2012", "authors": "Andrea Behrends; Sybille M\u00fcller; Isabel Dziobek"}, {"title": "Measuring emotion: the selfassessment manikin and the semantic diferential", "journal": "Journal of Behavior Therapy and Experimental Psychiatry", "year": "1994", "authors": "Margaret M Bradley; Peter J Lang"}, {"title": "Afective Interaction between Humans and Robots", "journal": "Springer-Verlag", "year": "2001", "authors": "Cynthia Breazeal"}, {"title": "Emotion and Sociable Humanoid Robots", "journal": "International Journal of Human-Computer Studies", "year": "2003-07", "authors": "Cynthia Breazeal"}, {"title": "Toward sociable robots", "journal": "Robotics and Autonomous Systems", "year": "2003", "authors": "Cynthia Breazeal"}, {"title": "Robots with display screens: a robot with a more humanlike face display is perceived to have more mind and a better personality", "journal": "PloS one", "year": "2013", "authors": "Elizabeth Broadbent; Vinayak Kumar; Xingyan Li; John Sollers 3rd; Rebecca Q Staford; Bruce A Macdonald; Daniel M Wegner"}, {"title": "FlyMap: Interacting with Maps Projected from a Drone", "journal": "Association for Computing Machinery", "year": "2018", "authors": "M Anke; Julia Brock; Michelle Chatain; Tommy Park; Martin Fang; James A Hachet; Jessica R Landay;  Cauchard"}, {"title": "The role of expressiveness and attention in human-robot interaction", "journal": "IEEE", "year": "2002", "authors": "A Bruce; I Nourbakhsh; R Simmons"}, {"title": "Confgural information in facial expression perception", "journal": "Journal of Experimental Psychology: Human Perception and Performance", "year": "2000", "authors": "Andrew J Calder; Andrew W Young; Jill Keane; Michael Dean"}, {"title": "2001. I show you how I like you -can you read it in my face?", "journal": "", "year": "", "authors": "Lola Ca\u00f1amero; Jakob Fredslund"}, {"title": "", "journal": "IEEE Transactions on Systems, Man, and Cybernetics -Part A: Systems and Humans", "year": "2001", "authors": ""}, {"title": "Drone & me: An Exploration into Natural Human-Drone Interaction", "journal": "Association for Computing Machinery", "year": "2015", "authors": "Jessica R Cauchard; Jane L E ; Kevin Y Zhai; James A Landay"}, {"title": "Emotion Encoding in Human-Drone Interaction", "journal": "IEEE", "year": "2016", "authors": "Jessica R Cauchard; Kevin Y Zhai; Marco Spadafora; James A Landay"}, {"title": "Investigating Drone Motion as Pedestrian Guidance", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Ashley Colley; Lasse Virtanen; Pascal Knierim; Jonna H\u00e4kkil\u00e4"}, {"title": "Grounded theory research: Procedures, canons, and evaluative criteria", "journal": "Qualitative Sociology", "year": "1990", "authors": "Juliet M Corbin; Anselm Strauss"}, {"title": "Drone in Love: Emotional Perception of Facial Expressions on Flying Robots CHI '21", "journal": "", "year": "2021", "authors": ""}, {"title": "Physiological and subjective evaluation of a human-robot object hand-over task", "journal": "Applied Ergonomics", "year": "2011", "authors": "Fr\u00e9d\u00e9ric Dehais;  Emrah Akin; Rachid Sisbot; Micka\u00ebl Alami;  Causse"}, {"title": "Anthropomorphism and the social robot", "journal": "Robotics and Autonomous Systems", "year": "2003", "authors": "R Brian;  Dufy"}, {"title": "Drone & Wo: Cultural Infuences on Human-Drone Interaction Techniques", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Jane L E ; Ilene L E ; James A Landay; Jessica R Cauchard"}, {"title": "Constants across cultures in the face and emotion", "journal": "Journal of Personality and Social Psychology", "year": "1971", "authors": "Paul Ekman; Wallace V Friesen"}, {"title": "Facial Action Coding System: The Manual on CD ROM", "journal": "", "year": "2002", "authors": "Paul Ekman; Wallace V Friesen; Joseph C Hager"}, {"title": "When familiarity breeds accuracy: Cultural exposure and facial emotion recognition", "journal": "Journal of Personality and Social Psychology", "year": "2003", "authors": "Hillary Anger Elfenbein; Nalini Ambady"}, {"title": "Dancing With Drones: Crafting Novel Artistic Expressions Through Intercorporeality", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Sara Eriksson; \u00c5sa Unander-Scharin; Vincent Trichon; Carl Unander-Scharin; Hedvig Kjellstr\u00f6m; Kristina H\u00f6\u00f6k"}, {"title": "Anthropomorphic inferences from emotional nonverbal cues: A case study", "journal": "IEEE", "year": "2010", "authors": "Friederike Eyssel; Frank Hegel; Gernot Horstmann; Claudia Wagner"}, {"title": "Anthropomorphism and Human Likeness in the Design of Robots and Human-Robot Interaction", "journal": "Springer", "year": "2012", "authors": "Julia Fink"}, {"title": "Lars Christian Jensen, and Maria Vanessa aus der Wieschen", "journal": "", "year": "2019", "authors": "Kerstin Fischer; Malte Jung"}, {"title": "ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE", "journal": "", "year": "", "authors": ""}, {"title": "A survey of socially interactive robots", "journal": "Robotics and Autonomous Systems", "year": "2003", "authors": "Terrence Fong; Illah Nourbakhsh; Kerstin Dautenhahn"}, {"title": "Evolution and facial action in refex, social motive, and paralanguage", "journal": "Biological Psychology", "year": "1991", "authors": "Alan J Fridlund"}, {"title": "The social roles and functions of emotions. In Emotion and Culture: Empirical Studies of Mutual Infuence", "journal": "American Psychological Association", "year": "1994", "authors": "H Nico; Batja Frijda;  Mesquita"}, {"title": "How we predict what other people are going to do", "journal": "Brain Research", "year": "2006", "authors": "Chris D Frith; Uta Frith"}, {"title": "Emotion cycles: On the social infuence of emotion in organizations", "journal": "Research in Organizational Behavior", "year": "2008", "authors": "Shlomo Hareli; Anat Rafaeli"}, {"title": "Creation and Evaluation of emotion expression with body movement, sound and eye color for humanoid robots", "journal": "RO-MAN", "year": "2011", "authors": "Markus H\u00e4ring; Nikolaus Bee; Elisabeth Andr\u00e9"}, {"title": "Designing Robots with Movement in Mind", "journal": "Journal of Human-Robot Interaction", "year": "2014-02", "authors": "Guy Hofman; Wendy Ju"}, {"title": "The efects of overall robot shape on the emotions invoked in users and the perceived personalities of robot", "journal": "Applied Ergonomics", "year": "2013", "authors": "Jihong Hwang; Taezoon Park; Wonil Hwang"}, {"title": "Characterizing the Design Space of Rendered Robot Faces", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Alisa Kalegina; Grace Schroeder; Aidan Allchin; Keara Berlin; Maya Cakmak"}, {"title": "Social Drone Companion for the Home Environment: A User-Centric Exploration", "journal": "Association for Computing Machinery", "year": "2017", "authors": ""}, {"title": "Iconic faces are not real faces: enhanced emotion detection and altered neural processing as faces become more iconic", "journal": "Cognitive Research: Principles and Implications 1, 1, Article", "year": "2016", "authors": "L N Kendall; Quentin Rafaelli; Alan Kingstone; Rebecca M Todd"}, {"title": "Getting Home Safely with Drone", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Bomyeong Kim; Hyun Young Kim; Jinwoo Kim"}, {"title": "Can robotic emotional expressions induce a human to empathize with a robot", "journal": "", "year": "2009", "authors": "Eun Ho Kim; Sonya S Kwak; Yoon Keun Kwak"}, {"title": "The Naughty Drone: A Qualitative Research on Drone as Companion Device", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Hyun Young Kim; Bomyeong Kim; Jinwoo Kim"}, {"title": "Emotions in Mediated Interpersonal Communication: Toward modeling emotion in virtual humans", "journal": "", "year": "2008", "authors": "Elly A Konijn; Henriette C Van Vugt"}, {"title": "Afective State Estimation for Human-Robot Interaction", "journal": "IEEE Transactions on Robotics", "year": "2007", "authors": "Dana Kulic; Elizabeth A Croft"}, {"title": "Biological movement increases acceptance of humanoid robots as human partners in motor interaction", "journal": "AI & Society", "year": "2011", "authors": "Aleksandra Kupferberg; Stefan Glasauer; Markus Huber; Markus Rickert; Alois Knoll; Thomas Brandt"}, {"title": "Drone Chi: Somaesthetic Human-Drone Interaction", "journal": "", "year": "2020", "authors": "Mehmet Aydin Joseph La Delfa; Rakesh Baytas; Hazel Patibanda;  Ngari; Ashok Rohit; Florian 'floyd' Khot;  Mueller"}, {"title": "Are emotional robots more fun to play with", "journal": "IEEE", "year": "2008", "authors": "Andr\u00e9 Iolanda Leite; Carlos Pereira; Ana Martinho;  Paiva"}, {"title": "Positive facial expressions are recognized faster than negative facial expressions, but why?", "journal": "Psychological Research", "year": "2004", "authors": "M Jukka; Jari K Lepp\u00e4nen;  Hietanen"}, {"title": "Multimodal Expression of Artifcial Emotion in Social Robots Using Color, Motion and Sound", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Diana L\u00f6fer; Nina Schmidt; Robert Tscharn"}, {"title": "Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley", "journal": "Cognition", "year": "2016", "authors": "B Maya; David B Mathur;  Reichling"}, {"title": "Uncanny but not confusing: Multisite study of perceptual category confusion in the Uncanny Valley", "journal": "", "year": "2020", "authors": "Maya B Mathur; David B Reichling; Francesca Lunardini; Alice Geminiani; Alberto Antonietti; A M Peter; Carmel Ruijten; Gideon Levitan; Dylan Nave;  Manfredi"}, {"title": "Latent Variable Modeling of Diferences and Changes with Longitudinal Data", "journal": "Annual Review of Psychology", "year": "2009", "authors": "John J Mcardle"}, {"title": "The Uncanny Valley", "journal": "", "year": "2012", "authors": "Masahiro Mori; Karl F Macdorman; Norri Kageki"}, {"title": "", "journal": "IEEE Robotics Automation Magazine", "year": "2012", "authors": ""}, {"title": "Understanding the Design of a Flying Jogging Companion", "journal": "", "year": "2014", "authors": "Florian Mueller; Matthew Muirhead"}, {"title": "UIST'14 Adjunct)", "journal": "Association for Computing Machinery", "year": "", "authors": ""}, {"title": "Understanding the Design of a Flying Jogging Companion", "journal": "", "year": "2014", "authors": "Florian Mueller; Matthew Muirhead"}, {"title": "UIST'14 Adjunct)", "journal": "Association for Computing Machinery", "year": "", "authors": ""}, {"title": "Nonverbal Leakage in Robots: Communication of Intentions through Seemingly Unintentional Behavior", "journal": "Association for Computing Machinery", "year": "2009", "authors": "Bilge Mutlu; Fumitaka Yamaoka; Takayuki Kanda; Hiroshi Ishiguro; Norihiro Hagita"}, {"title": "Laban for all", "journal": "Taylor & Francis US", "year": "2004", "authors": "Jean Newlove; John Dalby"}, {"title": "The functional basis of face evaluation", "journal": "Proceedings of the National Academy of Sciences", "year": "2008", "authors": "N Nikolaas; Alexander Oosterhof;  Todorov"}, {"title": "Shared perceptual basis of emotional expressions and trustworthiness impressions from faces", "journal": "Emotion", "year": "2009", "authors": "N Nikolaas; Alexander Oosterhof;  Todorov"}, {"title": "Are You Sad, Cozmo?\": How Humans Make Sense of a Home Robot's Emotion Displays", "journal": "", "year": "2020", "authors": "R M Hannah; Mathias Pelikan; Leelo Broth;  Keevallik"}, {"title": "Facial Expression and Interactional Regulation of Emotion", "journal": "Oxford University Press", "year": "2012", "authors": "Anssi Per\u00e4kyl\u00e4; Johanna Elisabeth Ruusuvuori"}, {"title": "A general psychoevolutionary theory of emotion", "journal": "Elsevier", "year": "1980", "authors": "Robert Plutchik"}, {"title": "Robotics facial expression of anger in collaborative human-robot interaction", "journal": "International Journal of Advanced Robotic Systems", "year": "2019", "authors": "Mauricio E Reyes; Ivan V Meza; Luis A Pineda"}, {"title": "The Illusion of Robotic Life: Principles and Practices of Animation for Robots", "journal": "Association for Computing Machinery", "year": "2012", "authors": "Tiago Ribeiro; Ana Paiva"}, {"title": "How Anthropomorphism Afects Empathy toward Robots", "journal": "Association for Computing Machinery", "year": "2009", "authors": "Laurel D Riek; Bhismadev Tal-Chen Rabinowitch; Peter Chakrabarti;  Robinson"}, {"title": "Confusion of fear and surprise: A test of the perceptual-attentional limitation hypothesis with eye movement monitoring", "journal": "Cognition and Emotion", "year": "2014", "authors": "Annie Roy-Charland; Melanie Perron; Olivia Beaudry; Kaylee Eady"}, {"title": "If Drones Could See: Investigating Evaluations of a Drone with Eyes", "journal": "Springer", "year": "2018", "authors": "A M Peter; Raymond H Ruijten;  Cuijpers"}, {"title": "Multidimensional scaling of emotional facial expressions: Similarity from preschoolers to adults", "journal": "Journal of Personality and Social Psychology", "year": "1985", "authors": "James A Russell; Merry Bullock"}, {"title": "Re-evaluating the form and communication of social robots", "journal": "International Journal of Social Robotics", "year": "2015", "authors": "Eleanor Sandry"}, {"title": "Afectivity in conversational storytelling: An analysis of displays of anger or indignation in complaint stories", "journal": "Pragmatics", "year": "2010", "authors": "Margret Selting"}, {"title": "Communicating Afect via Flight Path: Exploring Use of the Laban Efort System for Designing Afective Locomotion Paths", "journal": "IEEE", "year": "2013", "authors": "Megha Sharma; Dale Hildebrandt; Gem Newman; James E Young; Rasit Eskicioglu"}, {"title": "Human interactive robot for psychological enrichment and therapy", "journal": "", "year": "2005", "authors": "Takanori Shibata; Kazuyoshi Wada; Tomoko Saito; Kazuo Tanie"}, {"title": "Design and Evaluation of Emotion-Display EDDIE", "journal": "IEEE", "year": "2006", "authors": "Stefan Sosnowski; Ansgar Bittermann; Kolja Kuhnlenz; Martin Buss"}, {"title": "Unconscious evaluation of faces on social dimensions", "journal": "Journal of Experimental Psychology: General", "year": "2012", "authors": "Lorna H Stewart; Sara Ajina; Spas Getov; Bahador Bahrami; Alexander Todorov; Geraint Rees"}, {"title": "The State-of-the-Art of Human-Drone Interaction: A Survey", "journal": "IEEE Access", "year": "2019", "authors": "Dante Tezza; Marvin Andujar"}, {"title": "Drones with eyes: expressive Human-Drone Interaction", "journal": "", "year": "2019-06", "authors": "Tim Treurniet; Lang Bai; Xintong Simon \u00c0 Campo;  Wang"}, {"title": "Robots As Intentional Agents: Using Neuroscientifc Methods to Make Robots Appear More Social", "journal": "Frontiers in Psychology", "year": "2017", "authors": "Eva Wiese; Giorgio Metta; Agnieszka Wykowska"}, {"title": "Designing Drones: Factors and Characteristics Infuencing the Perception of Flying Robots", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2019-09", "authors": "Anna Wojciechowska; Jeremy Frey; Esther Mandelblum; Yair Amichai-Hamburger; Jessica R Cauchard"}, {"title": "Collocated Human-Drone Interaction: Methodology and Approach Strategy", "journal": "", "year": "2019", "authors": "Anna Wojciechowska; Jeremy Frey; Sarit Sass; Roy Shafr; Jessica R Cauchard"}, {"title": "", "journal": "IEEE", "year": "", "authors": ""}, {"title": "Chasing Lions: Co-Designing Human-Drone Interaction in Sub-Saharan Africa", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Anna Wojciechowska; Foad Hamidi; Andr\u00e9s Lucero; Jessica R Cauchard"}, {"title": "Exploring Proxemics for Human-Drone Interaction", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Alexander Yeh; Photchara Ratsamee; Kiyoshi Kiyokawa; Yuki Uranishi; Tomohiro Mashita; Haruo Takemura; Morten Fjeld; Mohammad Obaid"}, {"title": "Facial Expression Synthesis Using PAD Emotional Parameters for a Chinese Expressive Avatar", "journal": "Springer-Verlag", "year": "2007", "authors": "Shen Zhang; Zhiyong Wu; Helen M Meng; Lianhong Cai"}, {"title": "An event-related potential comparison of facial expression processing between cartoon and real faces", "journal": "PLoS ONE", "year": "2019", "authors": "Jiayin Zhao; Qi Meng; Licong An; Yifang Wang"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Set of rendered faces representing six basic emotions in three diferent intensity levels \u00a9Viviane Herdel. The faces use four core facial features: eyes, eyebrows, pupils, and mouth based upon FACS (Table1). All emotion categories performed well, only Disgust did not perform as well as the other emotion categories.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Study II. Screenshots from video stimuli of drone displaying Surprise. The emotion intensity increases from a) to b).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "report the results of Study I \u25b3 and Study II \u25a1. All statistical tests discussed as demonstrating statistically signifcant results have a p-value < .05. Example quotes provide additional information about the participant number and whether the participant belong to Study I: S (Static \u25b3) or to Study II: D (Dynamic \u25a1) stimuli (e.g., S44 corresponds to participant 44 from Study I). We describe the results indicating how individuals perceive facial expressions in drones,", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Absolute frequency of facial feature naming for static (Left) and dynamic (Right) stimuli.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Mouth was the most frequently named facial feature for all emotions in static stimuli \u25b3 (b = 0.281 -b = 3.272) except for Anger in which we found no signifcant diferences between mouth and eyes naming. In dynamic stimuli \u25a1, the mouth was the most frequently named facial feature in Sadness only (b = 0.503 -b = 1.363), but it did not difer from face naming for Joy, Anger, and Surprise. Furthermore, face naming was more frequent than mouth naming for Fear (b = 0.693).\u2022 Eyes were the second most frequently named facial feature in Joy, Sadness and Fear (b = 0.534 -b = 2.683) for static stimuli \u25b3. In the case of Surprise, we found no signifcant diferences between eyes and eyebrows naming. For dynamic stimuli \u25a1, we did not fnd any signifcant diferences in eyes and eyebrows naming.\u2022 Eyebrows were the third most frequently named facial feature in static stimuli \u25b3 for all emotions (b = 0.496 -b = 3.94), except for Joy, in which the face was named more often. For dynamic stimuli \u25a1, eyebrows were never named signifcantly more often than any other facial feature. \u2022 Face naming was more frequent for static stimuli \u25b3 than eyebrows naming for the emotion Joy (b = 0.613) and than invented features for the emotions Sadness (b = 0.56), Fear (b = 1.539), and Surprise (b = 2.862). In contrast, face naming for dynamic stimuli \u25a1 became more prevalent and was more often named than eyes and eyebrows for all emotions (b = 0.693 -b = 1.846), and than mouth for Fear (b = 0.693).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Emotions \u25b3 \u25a1. Overall, we found less diferences in facial feature naming across emotions in dynamic \u25a1 compared to static stimuli \u25b3. We describe below the role of each facial feature across emotions.\u2022 Mouth naming for Joy, Sadness and Surprise was more prevalent as compared to both Anger and Fear (Joy: b = 0.41, b = 0.308, Sadness: b = 0.35, b = 0.25, Surprise: b = 0.386, b = 0.284) for static stimuli \u25b3. This was also the case for dynamic stimuli \u25a1 for Fear only (Joy: b = 0.817, Sadness: b = 0.817, Surprise: b = 0.744). \u2022 Eyes naming frequency was found to be similar (no statistical diferences) across all emotions for both static \u25b3 and dynamic stimuli \u25a1. \u2022 Eyebrows were divided in three subgroups for static stimuli \u25b3: Surprise = Anger > Sadness = Fear > Joy. They were more frequently named in Surprise (b = 0.476 -b = 1.377) and Anger (b = 0.351 -b = 1.327) compared to all emotions, followed by Fear (b = 0.976) and Sadness (b = 0.901).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_6", "figure_caption": "\u2022Invented Features were signifcantly more frequently named in Sadness (b = 0.916, b = 0.799, b = 2.303) and Anger (b = 1.012, b = 0.894, b = 2.4) compared to Joy, Fear, and Surprise for static stimuli \u25b3.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Notes. p-values resulting from a binomial test were adjusted using Benjamini-Hochberg (BH) correction. Color-coded cells represent proportions with p-value <.05. Signifcantly correct choices of intensity and corresponding emotion category are highlighted in green (yellow if only the corresponding emotion category is correct). Confusion rates above random choice are indicated in grey. Rows correspond to the emotion label of the stimuli and columns to the best-choice emotion label chosen by participants.Abbreviations: S = Serenity, J = Joy, E = Ecstasy, P = Pensiveness, Sa = Sadness, G = Grief, A = Apprehension, F = Fear, T = Terror, An = Annoyance, Ang = Anger, R = Rage, D = Distraction, Su = Surprise, Am = Amazement, B = Boredom, Di = Disgust, L = Loathing. of the time for static \u25b3 and 30% of the time for dynamic \u25a1 stimuli. The number of mentioned external states difered across emotion categories for static stimuli \u25b3 where Fear, Anger, and Surprise were interpreted by using signifcantly more external factors than for Joy and Sadness (Fear: b = 0.956, b = 0.74, Anger: b = 1.03, b = 0.815, Surprise: b = 1.125, b = 0.91). This fnding did not replicate for dynamic stimuli \u25a1. Results on external factors are surprising given that the drone was the only actor in all visual stimuli. They indicate that participants interpreted the drone's emotions in a context involving the outside world, triggered and dependent upon the emotion displayed on the drone.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_8", "figure_caption": "\u2022We found that Anger evoked signifcantly more Efect naming compared to all other emotions for static stimuli \u25b3 (b = 1.156 -b = 2.043) (Figure 6). This fnding was partly aligned with fndings for dynamic stimuli \u25a1, in which Efect naming in Anger difered signifcantly from Joy (b = 1.041) and Surprise (b = 1.447), but not from Sadness and Fear. The dominant emotions for Cause naming were Surprise (b = 1.253 -b = 1.54) and Fear (b = 0.916 -b = 1.204), which difered signifcantly from all other emotions for static stimuli \u25b3. However, this efect almost disappeared for dynamic stimuli \u25a1, as the Cause naming in Surprise and Fear difered signifcantly from Cause naming only in Sadness (Surprise: b = 0.728, Fear: b = 0.827). 5.4.3 External Factors Across Emotions \u25b3 \u25a1. For both static and dynamic stimuli participants mentioned two diferent types of external factors: environmental or human. Environment to Drone With respect to both the direction and type of external factor, participants mentioned signifcantly more often that the drone was afected by the environment for Fear (b = 1.119 -b = 1.149) and Surprise (b = 1.402 -b = 1.777) compared to Joy, Sadness, and Anger for static stimuli \u25b3. This was partly replicated in dynamic stimuli \u25a1 for Surprise compared to Sadness (b = 0.832) and Anger (b = 0.938) as well as Fear compared to Anger (b = 0.799).", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_9", "figure_caption": "Figure 5 :5Figure 5: Barplots illustrating the frequency of coded categories (Table4) for each of the emotion categories: Joy, Sadness, Fear, Anger, and Surprise for static (left) stimuli and dynamic (right) stimuli. Note that the count data for static stimuli are aggregated over the three intensities (low, medium, and high). * indicates p-values <.01 for within emotion comparisons, which were provided by linear mixed-efect models with Poisson link function.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_10", "figure_caption": "Figure 6 :\u20226Figure 6: Representation of comparisons across emotion for Cause and Efect (Table4) in static (left) and dynamic (right) stimuli. Thick lines indicate that count data for Cause and Efect in the respective reference emotion category (e.g., Surprise) was signifcantly (p-values <.05) higher compared to the emotion categories in which the thick line ends (e.g., Joy, Sadness, Anger). The reverse is shown by thin lines originating from the respective reference emotion category..", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_11", "figure_caption": "Figure 7 :7Figure 7: Results of the SAM questionnaire showing the participants' overall emotional assessment of the drone for each emotion across: Arousal, Dominance, and Valence. The bars represent standard error of mean. Positive values indicate values greater than the corresponding baseline, and negative values indicate values smaller than the corresponding baseline. * indicates p-values <.01 and ** p-values <.001.", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_12", "figure_caption": "Figure 8 :8Figure 8: Compared absolute frequencies of participants reporting on being negatively or positively afected by the drone for each emotion. * indicates p-values <.01.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_13", "figure_caption": "Figure A. 1 :1Figure A.1: Study I. Screenshot of the study interface used in Task I. Top: Static drone stimulus (here Joy) with wheel of six emotions based on Plutchik's theory of emotion [65], followed by Questions 1 to 5 and their possible answers.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_14", "figure_caption": "Figure A. 2 :2Figure A.2: Study II. Screenshot of the study interface used in Task II.Top: Dynamic drone stimulus (here Joy), followed by Questions 1 to 4 and their possible answers. Note that the Plutchik's wheel of emotions[65] was clickable.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "Viviane Herdel, Anastasia Kuzminykh, Andrea Hildebrandt, and Jessica R. CauchardN 2 = 98 (Study II). We found that 31 individuals participated inboth studies; yet, their recognition rate did not signifcantly diferfrom other participants. Additional demographic information areprovided in Table 2. Participants were remunerated US$4.90 (StudyI) and US$3.70 (Study II) with a bonus of respectively $0.5-$1 and$0.5-$2 for elaborate answers on the open-ended questions. The: Colored cells represent Action Units (AU) from thesurveys took in average respectively 30 and 25 minutes to complete.Facial Action Coding System (FACS) [26] required to createNote that we initially recruited 100 participants and that 2 werespecifc basic emotions.discarded in each study due to technical issues.Table 2: Demographic information from participants inStudy I and II.Study I Study IINumber of Participants9898GenderMale4949Female4848Prefer not to say11AgeM4042SD8.79.9Range23-6925-68Education (%) College degree or higher6976"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Dependent variables collected in both studies that are further described in the results section.Ia) Recognition rate of six emotion categories: Joy, Sadness, Fear, Anger, Surprise, Disgust. Ib) Recognition rate of three intensities: low, medium, and high, for each emotion category. Ic) Free-form answers justifying the best-choice of emotion. IIa) Recognition rate of fve emotion categories: Joy, Sadness, Fear, Anger, Surprise.", "figure_data": "Study I\u25b3Study II\u25a1IIb) Self-Assessment Manikin (SAM) answers in Valence, Arousal, and Dominance.IIc) Free-form answers justifying the best-choice of emotion.IId) Free-form answers regarding participants' opinion of the drone."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Description of the coding scheme identifed in the thematic analysis of the qualitative data from the free-form answers. Mentions of specifc elements are referred to as \"naming\". For example, when a participant reported \"the eyes are shining\", we counted one occurrence of naming the facial feature 'eye' and of an interpretive descriptor 'shining'. FF: e.g., mouth, eyes, eyebrows, face.Naming of imaginary FF: e.g., teeth, tongue, nose.Purely informative description about the FF appearance (e.g., big, raised). Interpretations of FF beyond pure form factor (e.g., frowned, teary).", "figure_data": "ThemeCategoryDescriptionFacial Features (FF) Naming of displayed Emotion Facial feature name Invented Descriptive Interpretive Provided Emotions provided in the corresponding wheel of emotions.NamingInventedEmotions not provided in the corresponding wheel of emotions.Drone's StateInternalExternal-Efect -Drone to EnvironmentDrone to Human-Cause -Environment to DroneHuman to Drone"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Description of the coding scheme identifed in the thematic analysis of the qualitative data from the free-form answers regarding participants' opinion of the drone", "figure_data": "ThemeCategoryDescriptionDrone & Participants EmpathyMentions of either emotion contagion or empathy.Motivation to InteractMentions of wanting to (prosocially) interact with the drone.ExpectationsMentions of expectations or preferences around the drone.EmotionsPositiveMentions of positive emotions caused by the drone.NegativeMentions of negative emotions caused by the drone."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ". Efect estimates stem from linear mixed-efect models with Poisson link function. They indicate fxed efects, thus non-standardized regression weights b quantifying count diferences of naming a given facial feature as compared to other features within an emotion category or count diferences of naming a given facial feature across emotion categories. All reported bs are statistically signifcant at an \u03b1 level .05. Positive values indicate more counts for the respective facial feature.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Confusion matrices illustrating correct emotion recognition rates for static and dynamic stimuli.", "figure_data": "Static StimuliJoy Sadness Fear Anger Surprise DisgustJoy9505Sadness839025Fear113629511Anger43271119Surprise00792Disgust5141529Dynamic StimuliJoy Sadness Fear Anger Surprise Disgust Trust AnticipationJoy811342Sadness991Fear24431022Anger377812Surprise2151874"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Confusion matrix illustrating emotion recognition rates for each of the 18 presented emotion labels and their intensities in Study I Task 1.", "figure_data": "JoySadnessFearAngerSurpriseDisgustEmotion CategoryImagesSJE P S G A F T An Ang R D Su Am B Di L % CorrectJoySerenity40 56 21140Joy9 78 12178Ecstasy1 53 3321133SadnessPensiveness12 43 3 20 232112112Sadness1 83 9321183Grief74 24124FearApprehension1253 27 31 48612215227Fear14 11 4 44 1212511 1344Terror494 20 39315319139AngerAnnoyance7315154312121172843Anger111159145859Rage1117204422 344SurpriseDistraction11411 67241Surprise13321 603060Amazement44444848DisgustBoredom23 22 4271022 16 102Disgust26 32 2252321 721Loathing10 41 125191 21 88"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Example quotes of how participants were positively or negatively afected after being presented with the drone's emotional state.", "figure_data": "Drone's Emotion Participants Emotional ResponseJoyI like him. He seems to have a very upbeat cheerful personalityD89Sadness"}], "doi": "10.1145/3411764.3445495"}