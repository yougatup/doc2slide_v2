
 Welcome to the presentation of our paper: Drone in Love: Emotional Perception of Facial Expressions on Flying Robots published at CHI 2021. My name is Viviane Herdel and I am a first year Ph.D. student from the Ben Gurion University of the Negev under the supervision of Professor Jessica Cauchard. I worked together on the paper with Professor Anastasia Kuzminykh from the University of Toronto, Professor Andrea Hildebrandt from the University of Oldenburg, and with Professor Jessica Cauchard from the Ben Gurion University of the Negev. We studied how people percept emotional facial expression on flying robots.
 But why do we need drones with emotions? Our motivation to display emotional facial expressions on drones was that drones are populating human spaces and as such we need to understand how we can best interact with this technology. Also, previous research has shown that users tend to perceive the interaction with an emotional robot as more pleasant, they even feel closer to it, and ascribe human attributes to it, such as intentionality. However, to convey emotions in drones is quite new and only few prior works have been done before. Due to the fact that the basic emotions:
 Joy, Sadness, Fear, Anger, Surprise, and Disgust are universally recognizable and are well documented in the so-called Facial Action Coding System, we created facial expressions for these emotions. We also decided to use different intensities inspired by prior research corresponding to the basic emotions in their: low, medium, and high intensity forms. This enabled us to also investigate the intensities of basic emotions on drones. Thus, in total we created 18 images of cartoon-like facial expressions.
 Upon that we used the created facial expressions in two user studies showing how people recognize emotions on drones
 including recognition rates of intensities and emotion categories.
 We also show how people interpret emotions on drones including for instance interpretations of drone capabilities and narratives around the drone.
 We also found that drone emotions evoked a reciprocal emotional response in participants.
 Finally, we conclude with design recommendations
 and methodological insights for future research into social drones.
 We found that five basic emotions - namely: Joy, Sadness, Fear, Anger, and Surprise - can be recognized on drones. Light green indicates that the recognition rate was above random choice and red indicates that the confusion rate was above random choice. As we see for static stimuli, only the correct emotion categories were selected above random choice with recognition rates ranging from 62 to 95% except for the emotion Disgust, which was significantly confused with Sadness and was therefore not implemented in Study II.
 For the dynamic stimuli in Study II, we found recognition rates ranging from 43 to 99%. Note that in Study II, participants had the full Wheel of Emotions to select from and thus Trust and Anticipation are also inside the table.
 Participants created narratives around the drone's state that would often include external factors that were not present in the stimuli. The mentioning of external factors included either
 the environment or people. An example is: 'I feel it looks like it recognizes me and wants to say “hi”!'
 Lastly, we show how people were affected by the emotions on the drone.
 For instance, we found that some emotions evoked empathy, such as Sadness, gathering 64% of all empathy quotes For example: 'I feel bad for it. It seems like something hurt it!'
 We also found that empathy was linked
 to participants’ motivation to pro-socially interact with the drone. For example: 'It seems very sad and it makes me want to help it!'
 While these are the highlights of our findings, for the detailed results we invite you to read the full paper. We also would be happy to hear your thoughts on the paper or answer any questions you might have! Please do not hesitate to contact us! Thank you very much for your attention!
