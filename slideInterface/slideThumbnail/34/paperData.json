{
  "abstractText": {
    "page": 0,
    "region": {
      "x1": 53.46699905395508,
      "x2": 295.5600891113281,
      "y1": 398.7930908203125,
      "y2": 583.7260131835938
    },
    "text": "ABSTRACT The recently advanced robotics technology enables robots to assist users in their daily lives. Haptic guidance (HG) improves users’ task performance through physical interaction between robots and users. It can be classifed into optimal action-based HG (OAHG), which assists users with an optimal action, and user prediction-based HG (UPHG), which assists users with their next predicted action. This study aims to understand the diference between OAHG and UPHG and propose a combined HG (CombHG) that achieves optimal performance by complementing each HG type, which has important implications for HG design. We propose implementation methods for each HG type using deep learning-based approaches. A user study (n=20) in a haptic task environment indicated that UPHG induces better subjective evaluations, such as naturalness and comfort, than OAHG. In addition, the CombHG that we proposed further decreases the disagreement between the user intention and HG, without reducing the objective and subjective scores."
  },
  "figures": [{
    "caption": "Figure 4: The process of implementing (a) optimal actionbased haptic guidance (OAHG) and user prediction-based haptic guidance (UPHG), and (b) combined haptic guidance (CombHG). The three proposed methods (i.e., UT, UA, and SC), whose efectiveness is investigated through a subsequent user experiment, are highlighted in gray shades.",
    "captionBoundary": {
      "x1": 53.52003479003906,
      "x2": 295.648681640625,
      "y1": 643.6340942382812,
      "y2": 704.0496826171875
    },
    "figType": "Figure",
    "imageText": [],
    "name": "4",
    "page": 5,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 295.0,
      "y1": 399.0,
      "y2": 630.0
    }
  }, {
    "caption": "Table 1: Proposed methods applied to our HG implementation.",
    "captionBoundary": {
      "x1": 178.73599243164062,
      "x2": 432.9693908691406,
      "y1": 85.7260971069336,
      "y2": 91.35699462890625
    },
    "figType": "Table",
    "imageText": ["✓A", "checkmark", "indicates", "whether", "it", "was", "used", "for", "the", "corresponding", "HG", "type.", "UT", "Uncertainty-based", "thresholding", "✓", "✓", "✓", "UA", "User", "adaptation", "✓", "✓", "SC", "Similarity-based", "combination", "✓", "Abbrev", "Full", "Form", "OAHG", "UPHG", "CombHG"],
    "name": "1",
    "page": 5,
    "regionBoundary": {
      "x1": 163.0,
      "x2": 449.0,
      "y1": 106.0,
      "y2": 168.14898681640625
    }
  }, {
    "caption": "Figure 5: Experimental setup of our user study. A participant performs a virtual air hockey task with a haptic device.",
    "captionBoundary": {
      "x1": 53.79798889160156,
      "x2": 294.0436706542969,
      "y1": 260.4311218261719,
      "y2": 277.01898193359375
    },
    "figType": "Figure",
    "imageText": [],
    "name": "5",
    "page": 6,
    "regionBoundary": {
      "x1": 89.0,
      "x2": 258.0,
      "y1": 83.0,
      "y2": 247.0
    }
  }, {
    "caption": "Table 3: Comparisons showing statistically signifcant diferences between HG types.",
    "captionBoundary": {
      "x1": 134.0070037841797,
      "x2": 477.69830322265625,
      "y1": 85.7260971069336,
      "y2": 91.35699462890625
    },
    "figType": "Table",
    "imageText": ["CombHG", ">", "NHG", ".001", "Win", "rate", "OAHG", ">", "NHG", ".014", "Naturalness", "NHG", ">", "OAHG", ".000", "UPHG", ">", "NHG", ".002", "NHG", ">", "UPHG", ".025", "CombHG", ">", "NHG", ".001", "UPHG", ">", "OAHG", ".001", "Mean", "smash", "speed", "CombHG", ">", "NHG", ".023", "CombHG", ">", "OAHG", ".000", "CombHG", ">", "OAHG", ".045", "Controllability", "NHG", ">", "OAHG", ".000", "Defense", "rate", "OAHG", ">", "NHG", ".038", "UPHG", ">", "OAHG", ".000", "Mean", "disagreement", "OAHG", ">", "CombHG", ".000", "CombHG", ">", "OAHG", ".000", "UPHG", ">", "CombHG", ".002", "Comfort", "UPHG", ">", "OAHG", ".003", "Helpfulness", "UPHG", ">", "NHG", ".009", "CombHG", ">", "OAHG", ".001", "Metrics", "Comparison", "p", "Metrics", "Comparison", "p"],
    "name": "3",
    "page": 9,
    "regionBoundary": {
      "x1": 128.0,
      "x2": 484.0,
      "y1": 106.0,
      "y2": 237.0
    }
  }, {
    "caption": "Figure 6: Experimental results of the objective (top) and subjective (bottom) metrics for the nine HG conditions (a)–(i). The HG conditions of the same HG type are indicated by bars of the same color. Error bars represent 95% confdence intervals.",
    "captionBoundary": {
      "x1": 53.79800033569336,
      "x2": 558.202880859375,
      "y1": 307.860107421875,
      "y2": 324.44793701171875
    },
    "figType": "Figure",
    "imageText": [],
    "name": "6",
    "page": 7,
    "regionBoundary": {
      "x1": 66.0,
      "x2": 546.0,
      "y1": 83.0,
      "y2": 294.0
    }
  }, {
    "caption": "Figure 2: The air hockey game environment we developed. (a) Example of control based on a haptic device. (b) In-game screen and composition of our air hockey game.",
    "captionBoundary": {
      "x1": 53.520050048828125,
      "x2": 295.5770568847656,
      "y1": 213.63807678222656,
      "y2": 241.18292236328125
    },
    "figType": "Figure",
    "imageText": [],
    "name": "2",
    "page": 3,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 295.0,
      "y1": 83.0,
      "y2": 200.0
    }
  }, {
    "caption": "Figure 3: Overview of the model structure applied to both the optimal action model and the user prediction model.",
    "captionBoundary": {
      "x1": 317.9549560546875,
      "x2": 558.2007446289062,
      "y1": 257.6341247558594,
      "y2": 274.22198486328125
    },
    "figType": "Figure",
    "imageText": [],
    "name": "3",
    "page": 3,
    "regionBoundary": {
      "x1": 353.0,
      "x2": 523.0,
      "y1": 83.0,
      "y2": 244.0
    }
  }, {
    "caption": "Table 2: Comparisons showing statistically signifcant diferences between implementation methods within HG type.",
    "captionBoundary": {
      "x1": 68.61100006103516,
      "x2": 543.0951538085938,
      "y1": 85.7260971069336,
      "y2": 91.35699462890625
    },
    "figType": "Table",
    "imageText": ["Comfort", "(i)", ">", "(g)", ".023", "Mean", "disagreement", "(d)", ">", "(f)", ".000", "Mean", "disagreement", "(g)", ">", "(h)", ".000", "(e)", ">", "(f)", ".000", "(g)", ">", "(i)", ".000", "Helpfulness", "(e)", ">", "(d)", ".041", "(h)", ">", "(i)", ".000", "Naturalness", "(f)", ">", "(d)", ".032", "Helpfulness", "(i)", ">", "(g)", ".023", "Controllability", "(e)", ">", "(d)", ".028", "Naturalness", "(i)", ">", "(g)", ".001", "(f)", ">", "(d)", ".004", "(i)", ">", "(h)", ".043", "Comfort", "(f)", ">", "(d)", ".001", "Controllability", "(i)", ">", "(g)", ".009", "OAHG", "Mean", "disagreement", "(b)", ">", "(c)", ".000", "CombHG", "Mean", "smash", "speed", "(i)", ">", "(g)", ".003", "UPHG", "Win", "rate", "(f)", ">", "(d)", ".009", "(i)", ">", "(h)", ".028", "Type", "Metrics", "Comparison", "p", "Type", "Metrics", "Comparison", "p"],
    "name": "2",
    "page": 8,
    "regionBoundary": {
      "x1": 97.0,
      "x2": 515.0,
      "y1": 106.0,
      "y2": 237.0
    }
  }],
  "sections": [{
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 53.79800033569336,
        "x2": 119.50102996826172,
        "y1": 598.5338745117188,
        "y2": 604.7520141601562
      },
      "text": "∗Corresponding author"
    }, {
      "page": 0,
      "region": {
        "x1": 53.31680679321289,
        "x2": 294.8095703125,
        "y1": 621.4981079101562,
        "y2": 707.3472290039062
      },
      "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. CHI ’21, May 8–13, 2021, Yokohama, Japan © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445115"
    }, {
      "page": 0,
      "region": {
        "x1": 317.9537658691406,
        "x2": 559.8134155273438,
        "y1": 398.7930908203125,
        "y2": 441.2579345703125
      },
      "text": "CCS CONCEPTS • Human-centered computing → Human computer interaction (HCI); Haptic devices; User models; • Computing methodologies → Machine learning."
    }, {
      "page": 0,
      "region": {
        "x1": 317.6600036621094,
        "x2": 559.5477294921875,
        "y1": 455.8420715332031,
        "y2": 551.468017578125
      },
      "text": "KEYWORDS Haptic guidance, Physical human–robot interaction (pHRI), User adaptation, Deep learning ACM Reference Format: Hee-Seung Moon and Jiwon Seo. 2021. Optimal Action-based or User Prediction-based Haptic Guidance: Can You Do Even Better?. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3411764. 3445115"
    }]
  }, {
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 317.5249938964844,
        "x2": 559.7167358398438,
        "y1": 580.9449462890625,
        "y2": 707.345947265625
      },
      "text": "With the recent advancements in artifcial intelligence and robotics technology, it is becoming increasingly common for users to be assisted by robots or computing machineries. In the context of physical human–robot interaction (pHRI), haptic modality has a high use potential; haptic feedback can directly pass through the neuromuscular system without going through a high-level recognition process [1]; therefore, a user can respond faster than when other modalities (e.g., visual feedback) are used. Recently, the haptic guidance (HG) system, also called the haptic shared control system, has been accepted as a promising approach for human–machine interfaces or pHRI situations [2]. The HG system is defned as a method in which the control input determined through physical"
    }, {
      "page": 1,
      "region": {
        "x1": 53.53277587890625,
        "x2": 295.5615539550781,
        "y1": 87.79393005371094,
        "y2": 181.32000732421875
      },
      "text": "interaction between the force exerted by a human operator and the guiding force of a robot is applied to the target system [1]. Compared to the previous support systems using haptic cueing [12, 29] or input-mixing shared control [13], an advantage of the HG system is that the user can not only recognize the intention of the robot (e.g., direction and strength), but also choose to what extent this intention to be refected. For example, a user usually follows the robot’s guidance, but when his/her choice is necessary, he/she can apply a force that exceeds the HG to perform the desired control."
    }, {
      "page": 1,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5636291503906,
        "y1": 186.42393493652344,
        "y2": 345.7040100097656
      },
      "text": "HG technologies presented in earlier studies can be classifed into two categories according to their design method: optimal actionbased haptic guidance (OAHG) and user prediction-based haptic guidance (UPHG). OAHG is designed to convey the optimal movement for performing a task in the current state (e.g., guiding the user toward the center of the road in a steering task [15, 35, 53] or through the movement of a skilled expert in a peg-in-hole task [38]). Meanwhile, UPHG is designed to provide proactive guidance in the direction the user intends to move based on their behavior prediction. For example, in a steering task, it guides the users to their individually preferred courses rather than the center of the road [10]. Both types of HG systems have been proven to have several positive efects in terms of task performance improvement [5, 10, 15, 19, 27, 35, 55], user workload reduction [19, 27, 35], and user subjective satisfaction [15, 19, 27] in various recent studies."
    }, {
      "page": 1,
      "region": {
        "x1": 52.78499984741211,
        "x2": 295.5594787597656,
        "y1": 350.80792236328125,
        "y2": 532.0050048828125
      },
      "text": "While most of the previous studies deal with the design methods and efects of OAHG and UPHG, very few studies have clearly compared OAHG and UPHG. The two HGs are expected to have diferent characteristics because each goal behavior is diferent. OAHG informs the user of the most optimal action for the current task. However, if the guidance is in confict with the user’s intention (i.e., a disagreement occurs), it will lead to an undesired physical interaction between the user and the robot, which can induce discomfort and frustration for the user [13, 19]. On the other hand, UPHG supports comfortable movements of the users by reducing trajectory mismatch with the robot [10], but has a limitation in that it cannot present more optimal movements, although they may exist. In this context, this study aims to answer the following questions, which have important implications for pHRI design, “What is the diference between OAHG and UPHG in terms of user acceptance?” and “Is it possible to design a better HG by combining OAHG and UPHG?”"
    }, {
      "page": 1,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.5554504394531,
        "y1": 537.1089477539062,
        "y2": 707.3469848632812
      },
      "text": "To impartially compare OAHG and UPHG, we need to implement each type of HG to achieve its best performance for a given application. Consequently, we present the following implementation methods to improve the performance of each HG based on deep learning approaches, which are attracting attention in recent HG studies [7, 46, 52]. First, a deep reinforcement learning algorithm is used to train an optimal action model (for OAHG) to learn the optimal policy (i.e., the optimal way of behaving from a specifc state during a task) through self-play between AI agents [3] (the upper fow in Figure 1(b)). Second, we train a user prediction model (for UPHG) in a supervised manner with multiple user’s behavior data. To deal with individual diferences between human operators, we apply a meta-learning approach to enable to adapt the model parameters according to the current user (the lower fow in Figure 1(b)). Third, both the optimal action model and the user prediction model are designed to infer the model uncertainty from"
    }, {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.203125,
        "y1": 87.79393005371094,
        "y2": 115.5670166015625
      },
      "text": "their outputs. By making HGs consider their model uncertainty at every timestep, we aim to prevent the decrease in HG performance because of an inaccurate model."
    }, {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7184448242188,
        "y1": 120.67094421386719,
        "y2": 258.031982421875
      },
      "text": "Another essential purpose of our study is to explore the possibility of complementing each HG type. If OAHG and UPHG are in a trade-of relationship with each other, it can be expected to produce HG that achieves optimal performance in terms of objective and subjective metrics by properly harmonizing the two HG types. In particular, we focus on the fact that the disagreement between the guiding force and the user intention reduces the OAHG performance [13, 19]. Therefore, we devised a method to implement the combined haptic guidance (CombHG) utilizing the similarity of guiding forces generated by the two HG types every timestep to minimize the disagreement. With this similarity-based method, the combined guiding force is adjusted according to the diference between the two guiding force directions."
    }, {
      "page": 1,
      "region": {
        "x1": 317.6409912109375,
        "x2": 559.7147827148438,
        "y1": 263.1359558105469,
        "y2": 455.2929992675781
      },
      "text": "We conducted a user experiment with 20 participants to verify how each type of HG has diferent efects on objective and subjective evaluations. We built a task environment for confronting an AI agent in an air hockey game that can be operated and assisted through a haptic device, as shown in Figure 1(a). Each participant played the air hockey game with the HGs we implemented, and subjective evaluations including user interviews were conducted after each task. Our user experiment results indicate the following important points. First, all types of HG—OAHG, UPHG, and CombHG—led to a signifcant improvement in objective user performance compared to the case where the user did not receive any HG, but there was no signifcant diference in objective performance between the three HG types. Second, UPHG and CombHG elicited a signifcantly higher score in subjective metrics, such as perceived naturalness and comfort, than OAHG. Finally, CombHG signifcantly lowered the disagreement between the user intention and HG compared to the cases of OAHG and UPHG, without reducing the objective and subjective scores."
    }, {
      "page": 1,
      "region": {
        "x1": 327.9166564941406,
        "x2": 559.7207641601562,
        "y1": 460.3948669433594,
        "y2": 546.0180053710938
      },
      "text": "Overall, this paper has following three key contributions: • We present deep learning-based approach to implement OAHG and UPHG to achieve their best performance, applying a self-play-based reinforcement learning framework for OAHG and a meta-learning framework for UPHG. In particular, we propose and verify two novel implementation methods—uncertainty-based thresholding and user adaptation—to improve the HG performance."
    }, {
      "page": 1,
      "region": {
        "x1": 333.91766357421875,
        "x2": 559.7216796875,
        "y1": 550.3936157226562,
        "y2": 633.68896484375
      },
      "text": "• We propose and verify a combined approach (CombHG) of OAHG and UPHG that can complement each HG type, utilizing our similarity-based combination method. To the authors’ knowledge, this is the frst attempt to combine OAHG and UPHG to achieve better performance. • We experimentally compare OAHG, UPHG, and CombHG in terms of objective and subjective metrics through a user study and interview (n=20)."
    }],
    "title": {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 420.68597412109375,
        "y1": 566.2501220703125,
        "y2": 573.1010131835938
      },
      "text": "1 INTRODUCTION"
    }
  }, {
    "paragraphs": [],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 421.7441711425781,
        "y1": 649.1890869140625,
        "y2": 656.0399780273438
      },
      "text": "2 RELATED WORK"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 317.7309875488281,
        "x2": 558.203369140625,
        "y1": 679.574951171875,
        "y2": 707.3469848632812
      },
      "text": "Early HG studies were based on control assistance methods in virtual or teleoperation environments, such as a virtual fxture [42] and an artifcial force feld [54], which help users accurately move"
    }, {
      "page": 2,
      "region": {
        "x1": 53.59199905395508,
        "x2": 295.0312805175781,
        "y1": 87.79393005371094,
        "y2": 203.23797607421875
      },
      "text": "toward goals and prevent access to dangerous areas. Robotic devices enabled the delivery of haptic feedback generated by the virtual fxture or artifcial force feld techniques to users, allowing them to perform the tasks with improved stability [4, 28]. Over the past few decades, the implementation of HG has made signifcant progress, and it has been embedded in a variety of forms, including haptic devices [5, 10, 38, 55], sleeve devices [8, 16], pen devices [25], and steering wheels [9, 21, 35, 50, 53]. Accordingly, the range of HG applications has also expanded, such as in surgical assistance [20, 41, 55], driving assistance [21, 35, 50, 53], teleoperation of robots [38, 47] and UAVs [28, 49], and desktop computer interfaces [11, 26]."
    }, {
      "page": 2,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5624084472656,
        "y1": 208.3419647216797,
        "y2": 389.53900146484375
      },
      "text": "A majority of the previous studies have reported positive efects of HG, such as improvement in task performance and user comfort, and reduction of user workload. Nevertheless, some elements of HG that hamper usability remain. As the most representative example, a confict between the user and the HG can lead to a temporary increase in the force exerted by the user [6, 36], or even a decrease in performance [37]. In addition, if the interference of HG is excessive for a user, then the user requires more physical efort, which leads to a deteriorated user evaluation (e.g., low comfort and controllability) [30]. A detailed analysis of the factors afecting HG will bring useful implications in designing a user-friendly HG. In this study, we aim to determine what users expect (and not expect) from HG through a comparison between OAHG and UPHG, which was not sufciently covered in previous studies. Furthermore, we attempt, for the frst time, to optimally combine OAHG and UPHG to achieve better performance based on the understanding of the factors that infuence HG performance."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 425.32232666015625,
        "y1": 664.8764038085938,
        "y2": 671.727294921875
      },
      "text": "2.1 Haptic Guidance"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.5591125488281,
        "y1": 427.5199279785156,
        "y2": 564.8820190429688
      },
      "text": "The most straightforward way of implementing OAHG is to set a reference trajectory for performing a task and deliver a continuous guiding force so that the user does not leave the trajectory. In the context of a steering task such as when driving, a number of studies have implemented OAHG in the form of a guiding force directed toward the center of the path [9, 15, 21, 35, 53]. As another example, in a backward parking situation, Tada et al. [50] implemented OAHG by utilizing a Bezier curve between a start point and a target parking point as a reference trajectory. Meanwhile, when the reference trajectory could not be clearly defned in advance, a demonstration by skilled experts also served as the reference trajectory, for example, in the case of a handwriting task [5, 51] and a peg-in-hole task [38]."
    }, {
      "page": 2,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5646667480469,
        "y1": 569.9859619140625,
        "y2": 707.3469848632812
      },
      "text": "Recently, attempts have been made to apply reinforcement learning techniques in HG implementation to train an optimal policy for a task. Scobee et al. [46] attempted to determine the underlying value function of each observation state of a steering task from the movement of an expert operator through the inverse reinforcement learning method, and developed OAHG based on the trained value function. Meanwhile, the deep Q-network (DQN) was applied in [52] to train HG that minimizes the magnitude of the steering wheel angle during a steering task. These previous reinforcement learning-based HGs showed promising results, but had a limitation in that they cannot provide a perfectly optimal action because they were trained by data from very few (one or two) human operators. In this study, we trained an optimal action model for implementing"
    }, {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.1990356445312,
        "y1": 87.79393005371094,
        "y2": 104.60797119140625
      },
      "text": "OAHG by using a latest reinforcement learning framework based on self-play between AI agents only [3]."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 53.79800033569336,
        "x2": 279.7581481933594,
        "y1": 412.8250732421875,
        "y2": 419.6759948730469
      },
      "text": "2.2 Implementing Optimal Action-based HG"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 317.7309875488281,
        "x2": 559.7178344726562,
        "y1": 134.7429656982422,
        "y2": 283.0639953613281
      },
      "text": "Research on UPHG emerged later than research on OAHG and has received less attention. De Jonge et al. [10] presented UPHG according to a reference trajectory adapted to each user based on his/her previous trial in a steering task, whereas OAHG utilized a fxed reference trajectory (i.e., center of the path). Their personalized UPHG demonstrated the positive efect of reducing the confict between the human operator and HG. Meanwhile, data-driven approaches have also been used to model a user’s movement. Hernández et al. [19] stochastically modeled each user’s individual movement based on a hidden Markov model (HMM) in a task of moving a virtual object to a target position avoiding obstacles, and presented UPHG to assist the user’s movement based on the user model. Previous studies such as [41] and [55] also presented HG based on a user movement model using HMMs in a surgical task environment."
    }, {
      "page": 2,
      "region": {
        "x1": 317.9525451660156,
        "x2": 559.1871948242188,
        "y1": 288.1658630371094,
        "y2": 403.6109924316406
      },
      "text": "In this study, we implement a user model for UPHG based on deep neural networks. Such an approach has recently been highlighted for user modeling in HCI and HRI felds [32, 34, 39, 43]. To apply UPHG tailored for an individual user, it is necessary to learn on each individual’s data. However, collecting enough data to train the neural network-based model from scratch for every new user is very inefcient, particularly, when targeting numerous users. To solve this challenging problem, we apply a meta-learning approach, enabling the trained model to adapt the model parameters to a new user based on his/her short trial data, which is the frst attempt in UPHG implementation."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 544.2860717773438,
        "y1": 120.0480728149414,
        "y2": 126.89898681640625
      },
      "text": "2.3 Implementing User Prediction-based HG"
    }
  }, {
    "paragraphs": [],
    "title": {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 480.7950744628906,
        "y1": 419.0520935058594,
        "y2": 425.90301513671875
      },
      "text": "3 HAPTIC GUIDANCE DESIGN"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 317.6409912109375,
        "x2": 559.7124633789062,
        "y1": 449.43792724609375,
        "y2": 652.552978515625
      },
      "text": "As a target task for the experimental investigation of various types of HG, we developed a virtual air hockey game environment that can be controlled using a haptic device (Figure 2(a)). In our air hockey game environment, a player competes with an opponent AI on a slippery surface by moving his/her paddle, which is used to hit a puck (Figure 2(b)). The player’s successful task execution is judged by how many times he/she wins against the opponent AI. The player wins a round if he/she succeeds in putting a puck into the opposing goalpost, whereas he/she loses the round if the opponent puts a puck in the player’s goalpost. Each paddle is set to move only within its own side (i.e., above or below the halfline). The player’s paddle is controlled by a two-dimensional action vector, which corresponds to the desired paddle location on the xy coordinates with the midpoint of the player’s side as the origin (Figure 2(b)). The action vector has one-to-one correspondence with the position of the end efector of the haptic device. For intuitive control of the player, the end efector is also set to move only on the 2D plane, and the moving directions of the end efector and the paddle are matched (Figure 2(a))."
    }, {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2042236328125,
        "y1": 657.6569213867188,
        "y2": 707.3469848632812
      },
      "text": "A video game environment is suitable for comparing OAHG and UPHG in that each player can adopt various strategies according to his/her preference [22, 40] while having a common winning formula. A player’s basic strategy to win the air hockey game is to accurately smash the puck toward the opponent’s goal and defend"
    }, {
      "page": 3,
      "region": {
        "x1": 53.487030029296875,
        "x2": 295.42742919921875,
        "y1": 264.2619323730469,
        "y2": 379.70599365234375
      },
      "text": "against the puck from the opponent heading inside the player’s goal. However, in a detailed process, players can have several choices. A player can smash along a path that goes directly toward the goal, or he/she can choose a path through one or two refections using the wall, avoiding the opponent’s paddle. In addition, in a defensive situation, that is, when the opponent has the puck, a player can press the opponent near the half-line to narrow the angle of attack or wait for the opponent’s attack right in front of the goal. We expect UPHG to predict the player’s choices in advance and assist him/her with the corresponding action, while OAHG informs him/her of the most optimal action he/she can choose."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 469.1223449707031,
        "y1": 434.7393798828125,
        "y2": 441.5903015136719
      },
      "text": "3.1 Target Task Environment"
    }
  }, {
    "paragraphs": [{
      "page": 3,
      "region": {
        "x1": 53.36800003051758,
        "x2": 295.4273986816406,
        "y1": 421.8559265136719,
        "y2": 537.2999877929688
      },
      "text": "We implemented the following two models based on deep neural networks: 1) an optimal action model that outputs the most optimal action that a user can take and 2) a user prediction model that outputs the expected action to be selected by the current user. Because both models use the same type of input (i.e., current game state) and output (i.e., target action vector), we designed them to have the same structure but with diferent hyperparameters (i.e., depth and size of the hidden layer) that are fne-tuned for each model’s training. Each model was trained through diferent learning approaches based on reinforcement learning and meta-learning, respectively."
    }, {
      "page": 3,
      "region": {
        "x1": 53.79798889160156,
        "x2": 295.5636291503906,
        "y1": 548.0679321289062,
        "y2": 707.3469848632812
      },
      "text": "3.2.1 Model Architecture. Figure 3 shows the structure of our model. The air hockey game state—the two-dimensional position and velocity vectors of two paddles and a puck (total, size of 12)—is mapped to the two-dimensional distribution data, which are the mean and standard deviation (STD) of the distribution of each xy coordinate of the action vector (total, size of 4). The output distribution represents the distribution of the target action vector used to generate the guiding force (i.e., optimal action for OAHG or predicted action for UPHG). Notably, we designed the model to output the mean and STD of the distribution rather than a single action value. In our HG implementation, we utilize the mean of the distribution as a reference action to guide the users, and also consider the model uncertainty which can be inferred from the STD of the distribution. Fully connected (FC) hidden layers with rectifed linear unit (ReLU) activation functions are present between the"
    }, {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7144775390625,
        "y1": 284.9049377441406,
        "y2": 345.55499267578125
      },
      "text": "input and output nodes. We used grid search to determine the optimal hyperparameters that exhibit the best learning performance in each training method; therefore, the depth and size of the hidden layers are diferent for the optimal action model and user prediction model: 2 × 64 and 4 × 80, respectively. The output STD goes through an additional sigmoid activation function."
    }, {
      "page": 3,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7232666015625,
        "y1": 356.2129211425781,
        "y2": 548.3690185546875
      },
      "text": "3.2.2 Optimal Action Model Training. Reinforcement learning techniques have succeeded in acquiring an optimal policy that surpasses a human player in simulated video game environments [31, 48]. To train the optimal action model for our air hockey task, which is based on the competition between two players, we applied a latest learning framework based on self-play between two AI agents [3]. In this framework, a training agent of the frst generation initially confronts an opponent with random movements and updates the action model for increasing the expected reward. As the learning progresses, the training agent of the next generation competes with the agents of the previous generations, which also grow gradually, therefore the optimal action model can be progressively developed without human operator intervention. Meanwhile, the training agent updates the model using the sampled action from the action distribution output of the model. Therefore, when the specifc action is confdently more optimal than other actions, the model will train to output a lower STD value, that is, the STD refects the model uncertainty at current state."
    }, {
      "page": 3,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.7208862304688,
        "y1": 553.4729614257812,
        "y2": 636.0399780273438
      },
      "text": "For implementation details, trust region policy optimization (TRPO) [44] was used as the learning algorithm that updates the policy of the training agent, because it internally had the best learning performance for our air hockey task among the latest algorithms such as proximal policy optimization (PPO) [45] and soft actor-critic (SAC) [18]. Self-play-based procedural learning was performed over 100 generations. Each generation was trained with a simulation of 200K timesteps, and batch updates were applied every 5K timesteps."
    }, {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7163696289062,
        "y1": 646.6979370117188,
        "y2": 707.345947265625
      },
      "text": "3.2.3 User Prediction Model Training. In utilizing the user prediction model, we mainly focus on how to train the model to adapt to diferent users. Inspired by the fact that the existing meta-learning frameworks train models to quickly learn to perform new unseen tasks using only a few datapoints of each new task, we applied a meta-learning approach to train the user prediction model for"
    }, {
      "page": 4,
      "region": {
        "x1": 380.6180114746094,
        "x2": 385.5674743652344,
        "y1": 560.1439819335938,
        "y2": 606.966552734375
      },
      "text": ""
    }, {
      "page": 4,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.56378173828125,
        "y1": 87.79393005371094,
        "y2": 246.664794921875
      },
      "text": "HG to quickly adapt to diferent users. We demonstrated in our prior study [33] that model-agnostic meta-learning (MAML) [14], a recent mainstream meta-learning algorithm, can be used to train a model that mimics user behavior for dynamic difculty adjustment and efectively adapts to a new user with his/her minimal demonstration data. In the current study, we present a user adaptation (UA) method that trains the user prediction model based on the MAML algorithm and updates the model parameters for each user. The training process of our user prediction model is as follows. The training data consists of a set of task execution data DU of diferent users U , and DU is divided into DU for model adaptation, anddemo DU for meta-update. First, the algorithm updates the model valid parameter θ by a single gradient descent that reduces L(θ , DU ),demo which is the model loss function for DU demo . Therefore,"
    }, {
      "page": 4,
      "region": {
        "x1": 117.17200469970703,
        "x2": 294.0453796386719,
        "y1": 252.81417846679688,
        "y2": 264.41497802734375
      },
      "text": "θU = θ − α∇θ L(θ , DU (1)adapt demo ),"
    }, {
      "page": 4,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5624694824219,
        "y1": 272.1408996582031,
        "y2": 313.9290466308594
      },
      "text": "where α denotes the inner learning rate. Subsequently, a metaupdate is performed to reduce model loss for DU using the valid updated parameters. Overall, the objective of our meta-learning is as follows: Õ"
    }, {
      "page": 4,
      "region": {
        "x1": 53.46699905395508,
        "x2": 294.6788024902344,
        "y1": 455.3081970214844,
        "y2": 542.56103515625
      },
      "text": "where N denotes the size of the training data D (i.e., DU ordemo DU ), subscript i indicates the i-th sample from D, yi is an actual valid action taken by a player at a game state xi , and ŷi and σ̂i are the predicted mean and STD output by feeding xi into the model with parameter θ . With this loss function, the model can be trained to output the STD that implies the uncertainty of the predicted action; that is, the lower the STD values, the more confdent the model is about the prediction."
    }, {
      "page": 4,
      "region": {
        "x1": 53.46875,
        "x2": 295.4273986816406,
        "y1": 547.6649780273438,
        "y2": 586.39697265625
      },
      "text": "For the user prediction model training, we collected data from nine participants aged between 22–29 (mean=25.33, STD=2.16) while performing the air hockey task without haptic guidance. Each participant was provided sufcient practice time to be familiar"
    }, {
      "page": 4,
      "region": {
        "x1": 317.6440734863281,
        "x2": 559.7208251953125,
        "y1": 87.79393005371094,
        "y2": 170.36102294921875
      },
      "text": "another user prediction model in a typical supervised manner, excluding the meta-learning approach, employing the same dataset used in the training with UA. In this case, the model trained without UA will predict generalized user movement because it is trained to use fxed network parameters for various user behaviors. An Adam optimizer with a learning rate of 0.001 was used and the batch size was 1K timesteps. The entire training was conducted for 100 epochs."
    }],
    "title": {
      "page": 3,
      "region": {
        "x1": 53.79800033569336,
        "x2": 289.2927551269531,
        "y1": 394.2100830078125,
        "y2": 414.0101013183594
      },
      "text": "3.2 Optimal Action Model and User Prediction Model"
    }
  }, {
    "paragraphs": [{
      "page": 4,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.19140625,
        "y1": 201.17393493652344,
        "y2": 318.5820007324219
      },
      "text": "The trained models output the target action for assisting the user’s task. A straightforward and reliable approach to generate HG is to implement a spring force toward the target action so that users can match their actions accordingly [10, 15, 35, 46]. Meanwhile, one of the important implications from the previous studies on HG implementation is that a human operator cannot respond to HG immediately. Accordingly, a lookahead method [15] where HG should be proactively applied based on a slight future game state, considering the reaction time of a human, has been applied as a common technique. Following the method described in [15], we used the anticipated game state xlookahead , which is obtained by"
    }, {
      "page": 4,
      "region": {
        "x1": 125.03459930419922,
        "x2": 558.3744506835938,
        "y1": 315.015380859375,
        "y2": 327.5769958496094
      },
      "text": "L(θU (2)adapt , D v U alid ).min virtually moving the puck and paddles with the current velocity for"
    }, {
      "page": 4,
      "region": {
        "x1": 53.52899932861328,
        "x2": 558.2048950195312,
        "y1": 329.5341796875,
        "y2": 399.63299560546875
      },
      "text": "U a short time Tlookahead , instead of the current game state x as theThrough this two-fold backpropagation, the user prediction model eventually learns to rapidly adapt to unseen users. Meanwhile, we aim to train the model to output the STD of the predicted action distribution, which is impossible with the mean-squared error loss function commonly used in general regression learning. Inspired by a previous work [24] that modeled uncertainty in deep learning model input. Therefore, the guiding force of our OAHG and UPHG is generated by the following equation: FHG = clip(−K(u − ŷ)), (4)"
    }, {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 559.1927490234375,
        "y1": 391.1129150390625,
        "y2": 421.5509948730469
      },
      "text": "where K is the stifness gain, u is the user’s current action vector, and ŷ is the mean of the target action vector distribution obtained by feeding xlookahead into the model. A clipping function was models for computer vision tasks, we use the following loss function to train our user prediction model:"
    }, {
      "page": 4,
      "region": {
        "x1": 62.475677490234375,
        "x2": 558.7411499023438,
        "y1": 412.0427551269531,
        "y2": 451.7619934082031
      },
      "text": "1 1 1 σi ∥2 ∥yi − ŷi ∥2 + log ∥σ̂i ∥2 2∥ ˆ 2 Õ applied to make the guiding force bounded, because momentary , (3) excessive force may induce safety problems to the user.L(θ , D) = N (xi ,yi )∈D Additionally, we propose an uncertainty-based thresholding (UT)"
    }, {
      "page": 4,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.720703125,
        "y1": 456.8659362792969,
        "y2": 561.3510131835938
      },
      "text": "method that adjusts the magnitude of HG by considering model uncertainty. The importance of model uncertainty has been recognized in several previous pHRI studies [17, 19, 23], as the results from inaccurate models of robot or human movements can lead to human discomfort or safety issues during human–robot collaborations. We infer the model uncertainty at a specifc state from the STD of the target action; that is, if the output STD is large, the model at current state is judged to be highly uncertain. Using this, we defned the uncertainty weight according to the squared magnitude of the STD as follows:"
    }, {
      "page": 4,
      "region": {
        "x1": 380.6180114746094,
        "x2": 558.2039794921875,
        "y1": 557.4539794921875,
        "y2": 615.2155151367188
      },
      "text": "1, if ∥σ̂ ∥2 < Tlow    Thiдh −∥σ̂ ∥2 , (5)Thiдh −Tlow if Tlow ≤ ∥σ̂ ∥ 2 < Thiдh"
    }, {
      "page": 4,
      "region": {
        "x1": 53.46699905395508,
        "x2": 559.5834350585938,
        "y1": 591.4383544921875,
        "y2": 663.511962890625
      },
      "text": "0, otherwise, with the task prior to data collection. A total of 360K timesteps of Wunc = recorded behavioral data were employed to train the user prediction model. During our meta-learning process, the adaptation was conducted with an inner learning rate of 0.1, and the meta-update where σ̂ is the output STD, and Tlow and Thiдh are threshold values. was conducted using an Adam optimizer with a learning rate of 0.001. The batch size of DU and DU was 1K timesteps, anddemo valid the entire training was conducted for 200 epochs."
    }, {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5646667480469,
        "y1": 668.615966796875,
        "y2": 707.3469848632812
      },
      "text": "To verify the practical efectiveness of the UA method, in a subsequent user study, we experimentally compared the performance of two UPHGs based on models trained with and without UA. For the UPHG implementation without using the UA method, we trained"
    }, {
      "page": 4,
      "region": {
        "x1": 317.5249938964844,
        "x2": 559.1959228515625,
        "y1": 635.6919555664062,
        "y2": 707.3469848632812
      },
      "text": "With the UT method, calculated FOAHG or FU PHG from (4) is multiplied by the uncertainty weight, which ranges from 0 to 1, and then provided to users. In other words, only when the STD is less than a certain level (i.e., Thiдh ), we determine the HG to be confdent in its purpose and can assist the users. Figure 4(a) summarizes the process of generating OAHG and UPHG, including the UT method."
    }, {
      "page": 5,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5574951171875,
        "y1": 190.4889373779297,
        "y2": 262.09698486328125
      },
      "text": "A straightforward approach to implement CombHG is to use the average of two guiding force vectors from OAHG and UPHG, therefore make the user to be assisted by both HG types simultaneously. This approach is intuitive in that the combined HG guides the user to the midpoint between the two target actions to be guided by OAHG and UPHG. Accordingly, we implement the simplest CombHG as follows:"
    }, {
      "page": 5,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.0323791503906,
        "y1": 271.1186828613281,
        "y2": 380.9739990234375
      },
      "text": "FCombHG = (FOAHG + FU PHG )/2. (6) However, simply taking the average of two guiding forces may worsen the results, guiding the user to an unintended third direction if the two HGs have diferent directions. To solve this problem, we propose a conservative combination method, namely similaritybased combination (SC), that considers the similarity of two HGs based on the angle between the two guiding forces and reduces the magnitude of HG when the similarity is low. The main purpose of the SC method is to provide an appropriate guiding force only when the target action is optimal and meets the user’s intention,"
    }, {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2008056640625,
        "y1": 190.4889373779297,
        "y2": 207.302001953125
      },
      "text": "that is, when the directions of the two HGs match. To do this, we defne the similarity weight as follows:"
    }, {
      "page": 5,
      "region": {
        "x1": 403.30615234375,
        "x2": 558.1976928710938,
        "y1": 216.85194396972656,
        "y2": 226.64898681640625
      },
      "text": "Wsim = cos2(ϕ/2), (7)"
    }, {
      "page": 5,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.5842895507812,
        "y1": 237.1179656982422,
        "y2": 308.72698974609375
      },
      "text": "where ϕ is the angle between the two guiding forces according to OAHG and UPHG. The similarity weight ranges from 0 to 1, which corresponds to the angle ϕ from π (opposite direction) to 0 (matched direction). With the SC method, FCombHG from (6) is provided to the user after multiplying by the similarity weight. Figure 4(b) shows the implementation of CombHG using OAHG and UPHG, including the SC method."
    }],
    "title": {
      "page": 4,
      "region": {
        "x1": 317.9549865722656,
        "x2": 549.2932739257812,
        "y1": 186.47909545898438,
        "y2": 193.33001708984375
      },
      "text": "3.3 Generating HGs Based on Trained Models"
    }
  }, {
    "paragraphs": [],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 443.518798828125,
        "y1": 324.5400695800781,
        "y2": 331.3909912109375
      },
      "text": "4 USER STUDY DESIGN"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.7147216796875,
        "y1": 354.9259338378906,
        "y2": 536.1229858398438
      },
      "text": "The objective of this research is divided into two main parts: a presentation of the implementation methods for three types of HG, that is, OAHG, UPHG, and CombHG, and an experimental investigation into how each type of HG difers in the context of user acceptance. We implemented the HGs by generating a guiding force based on the real-time output of the optimal action model and the user prediction model we trained. To achieve better performance of HG, we applied the three proposed methods (i.e., UT, UA, and SC), according to the HG type specifed in Table 1. To aid future HG studies, this user study frst aims to experimentally confrm whether the methods actually lead to a HG performance improvement. Subsequently, by using HGs to which all the applicable methods are employed, we determine whether each HG assists users when compared to a no HG (NHG) condition. Further, we investigate how the three types of HG assist users diferently in terms of objective and subjective evaluations, which is our primary research objective. Therefore, we formulate the following four research questions:"
    }, {
      "page": 5,
      "region": {
        "x1": 333.91766357421875,
        "x2": 559.7205810546875,
        "y1": 543.8519897460938,
        "y2": 649.0670166015625
      },
      "text": "• RQ1: Do the UT, UA, and SC methods that we propose and apply to the HG implementation contribute to an improvement in HG performance? • RQ2: Do OAHG, UPHG, and CombHG improve users’ task performance when compared to NHG? • RQ3: What diferences do OAHG and UPHG have in users’ objective and subjective evaluations? • RQ4: Can CombHG, which integrates OAHG and UPHG, complement each HG or provide better efects in users’ objective and subjective evaluations?"
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 439.7223815917969,
        "y1": 340.22735595703125,
        "y2": 347.0782775878906
      },
      "text": "4.1 Research Questions"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.5249938964844,
        "x2": 558.2032470703125,
        "y1": 679.574951171875,
        "y2": 707.345947265625
      },
      "text": "We conducted an indoor laboratory experiment to measure the actual assisting performance of the implemented HGs. We recruited 20 participants (4 females and 16 males) aged between 21–30 (mean="
    }, {
      "page": 6,
      "region": {
        "x1": 53.57400131225586,
        "x2": 295.5554504394531,
        "y1": 301.9099426269531,
        "y2": 329.6820068359375
      },
      "text": "25.58, STD=2.09) for this user study. All participants were righthanded, and none of them reported perception defects in their vision and touch."
    }, {
      "page": 6,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.0291442871094,
        "y1": 334.7859191894531,
        "y2": 395.4360046386719
      },
      "text": "Figure 5 presents the experimental setup. The air hockey task environment was composed of a haptic interface device (Omega.7, Force Dimension) and a 24-inch monitor, which were connected to a PC. Each participant was instructed to sit down and perform the air hockey task by holding the end efector of the haptic device with his/her dominant hand."
    }, {
      "page": 6,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.55877685546875,
        "y1": 400.5399169921875,
        "y2": 439.27099609375
      },
      "text": "We selected HGs under the following nine conditions for the user experiment for verifying the efectiveness of the HG implementation methods (RQ1) and comparing the efect of each HG type (RQ2–RQ4):"
    }, {
      "page": 6,
      "region": {
        "x1": 64.35144805908203,
        "x2": 295.55706787109375,
        "y1": 446.9103698730469,
        "y2": 463.7223815917969
      },
      "text": "(a) NHG: no haptic guidance. (b) OAHG (vanilla): OAHG from Equation (4) with no addi-"
    }, {
      "page": 6,
      "region": {
        "x1": 65.1967544555664,
        "x2": 295.56134033203125,
        "y1": 468.82391357421875,
        "y2": 485.63800048828125
      },
      "text": "tional method. (c) OAHG (UT): OAHG applying the uncertainty-based thresh-"
    }, {
      "page": 6,
      "region": {
        "x1": 64.49737548828125,
        "x2": 294.0489807128906,
        "y1": 490.7419128417969,
        "y2": 507.5559997558594
      },
      "text": "olding in addition to OAHG (vanilla). (d) UPHG (vanilla): UPHG from Equation (4) using a model"
    }, {
      "page": 6,
      "region": {
        "x1": 63.95042419433594,
        "x2": 295.56378173828125,
        "y1": 512.6599731445312,
        "y2": 650.02197265625
      },
      "text": "not trained with the meta-learning algorithm, and therefore without the user adaptation. (e) UPHG (UA): UPHG from Equation (4) using a model trained with the meta-learning algorithm (i.e., UA applied). (f) UPHG (UT+UA): UPHG applying the uncertainty-based thresholding in addition to UPHG (UA). (g) CombHG (UA): CombHG from Equation (6) based on OAHG (vanilla) and UPHG (UA). Note that because the basic concept of UPHG is assisting a user according to personalized prediction, we use UPHG (UA) instead of UPHG (vanilla) to implement the underlying CombHG. (h) CombHG (UA+SC): CombHG applying the similaritybased combination on OAHG (vanilla) and UPHG (UA)."
    }, {
      "page": 6,
      "region": {
        "x1": 66.4520492553711,
        "x2": 295.5570068359375,
        "y1": 655.1239013671875,
        "y2": 671.9400024414062
      },
      "text": "(i) CombHG (UT+UA+SC): CombHG applying the similaritybased combination on OAHG (UT) and UPHG (UT+UA)."
    }, {
      "page": 6,
      "region": {
        "x1": 53.46699905395508,
        "x2": 294.04791259765625,
        "y1": 679.5704345703125,
        "y2": 707.3469848632812
      },
      "text": "Our experimental procedure is as follows. First, all participants were informed that they would experience nine diferent HGs, and interviews would be conducted about their impressions received"
    }, {
      "page": 6,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.189453125,
        "y1": 87.79563903808594,
        "y2": 323.7860107421875
      },
      "text": "from each HG, immediately after experiencing each HG. Second, each participant was provided with as much practice time as he/she wanted to be familiar with the task environment. Third, for the purpose of updating the user prediction model parameters (i.e., UA), each participant performed one game in the NHG condition, which was not refected in the evaluation. One game basically consists of seven rounds (one round ends when either a player or an AI scores a goal), but if it ends earlier than the minimum play time that we set (two minutes per task), up to three additional rounds proceed to secure more data from the player. After completing UA with the recorded data, that is, updating the parameters of the user prediction model through Equation (1), the participant sequentially performed all nine HG conditions in a random order with counterbalancing. All participants performed one game for each HG condition, and before the start of each game, they were allowed 30 seconds to adapt to the given HG, which was not refected in the evaluation. Following the end of each game, participants assessed the subjective scores for the HG they had just been assisted with, and a short interview with a supervising researcher was conducted. All participants were able to rest as much as they wanted between each game. The total duration of the experiment was between 1–1.5 hours, depending on the participants."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 451.0678405761719,
        "y1": 664.880126953125,
        "y2": 671.7310180664062
      },
      "text": "4.2 Experimental Method"
    }
  }, {
    "paragraphs": [{
      "page": 6,
      "region": {
        "x1": 317.5249938964844,
        "x2": 558.8361206054688,
        "y1": 354.8329162597656,
        "y2": 426.4419860839844
      },
      "text": "We measured the assisting performance of each HG condition in terms of objective metrics automatically measured by the system and subjective metrics based on participants’ evaluations. Four objective metrics were used: win rate, mean smash speed, and defense rate, which indicate the degree of high task performance; and mean disagreement, which indicates the degree of high confict between a user and HG. The metrics are defned as follows:"
    }, {
      "page": 6,
      "region": {
        "x1": 333.91766357421875,
        "x2": 559.7215576171875,
        "y1": 434.6910705566406,
        "y2": 623.1900634765625
      },
      "text": "• Win rate: the ratio of the participant winning the opponent in one game (7–10 rounds). • Mean smash speed: the average speed of the puck hit by the participant over the opponent’s side (i.e., smashed). Note that, the speed was measured in relative fgures because the air hockey task was built in a virtual environment without specifc units. • Defense rate: the proportion of the pucks blocked by the participant among the pucks headed into the participant’s goal. • Mean disagreement: the average of the disagreement between the participant and HG, which is proposed in [19] and defned as follows (∆u denotes the user’s action change during a timestep after receiving HG).  HG ·∆u− FT , if FT · ∆u < 0∥∆u ∥ HG disaдreement = 0, otherwise."
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.3713989257812,
        "y1": 631.8639526367188,
        "y2": 659.636962890625
      },
      "text": "For the subjective evaluation, we selected the following four items and asked the participants to rate each HG condition in terms of the items on a 7-point Likert score:"
    }, {
      "page": 6,
      "region": {
        "x1": 333.91766357421875,
        "x2": 559.7199096679688,
        "y1": 667.8860473632812,
        "y2": 707.3469848632812
      },
      "text": "• Helpfulness: how much the participants felt the HG helped them perform the task. • Naturalness: how natural the participants felt the assistance of the HG."
    }, {
      "page": 7,
      "region": {
        "x1": 69.76719665527344,
        "x2": 294.04730224609375,
        "y1": 346.0569763183594,
        "y2": 385.52099609375
      },
      "text": "• Controllability: how well the participants felt they were able to control the paddle under the HG. • Comfort: how comfortable the participants were with the assistance of the HG."
    }],
    "title": {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 499.7550964355469,
        "y1": 340.1380920410156,
        "y2": 346.989013671875
      },
      "text": "4.3 Measured Variables and Metrics"
    }
  }, {
    "paragraphs": [{
      "page": 7,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5633850097656,
        "y1": 419.7099304199219,
        "y2": 502.2770080566406
      },
      "text": "Figure 6 shows the objective and subjective evaluation results for each HG condition. To answer RQ1, we frst investigated how assisting performance varies according to implementation methods within each HG type. Next, we examined how each HG type provides diferent assistance for the user (RQ2–RQ4), using the results of the HG conditions to which all applicable implementation methods were employed, that is, OAHG (UT), UPHG (UT+UA), and CombHG (UT+UA+SC)."
    }],
    "title": {
      "page": 7,
      "region": {
        "x1": 53.79800033569336,
        "x2": 117.15803527832031,
        "y1": 405.01507568359375,
        "y2": 411.8659973144531
      },
      "text": "5 RESULTS"
    }
  }, {
    "paragraphs": [{
      "page": 7,
      "region": {
        "x1": 53.36800003051758,
        "x2": 295.0341796875,
        "y1": 536.4659423828125,
        "y2": 608.073974609375
      },
      "text": "We frst compared the evaluated results of the two conditions within the OAHG type according to the implementation methods (i.e., vanilla vs. UT). A paired t-test revealed that the use of the UT method in the OAHG type signifcantly reduces the participants’ mean disagreement (t = 12.143, p < .001). On the other hand, no signifcant diference was found in the remaining seven metrics between the OAHG (vanilla) and OAHG (UT) conditions."
    }, {
      "page": 7,
      "region": {
        "x1": 53.349998474121094,
        "x2": 295.56402587890625,
        "y1": 613.177978515625,
        "y2": 708.6920166015625
      },
      "text": "To compare the three conditions within the UPHG type (i.e., vanilla, UA, UT+UA), a repeated measures ANOVA with a Greenhouse-Geisser correction was used. The analysis revealed that there were signifcant efects of the UPHG implementation methods on the following metrics: win rate (F2,38 = 6.430, p = .004, η2 = .253), mean disagreement (F2,38 = 19.856, p < .001, η2 = .511), helpfulness (F2,38 = 3.422, p = .044, η2 = .153), naturalness (F2,38 = 5.156, p = .013, η2 = .213), controllability (F2,38 = 10.188, p = .001, η2 = .349), and comfort (F2,38 = 7.012, p = .006, η2 = .270)."
    }, {
      "page": 7,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7144165039062,
        "y1": 346.7889404296875,
        "y2": 462.2330017089844
      },
      "text": "However, there was no signifcant efect on the mean smash speed and defense rate. For the metrics with signifcant efects, we conducted post hoc tests with a Bonferroni correction to investigate a signifcant mean diference between the conditions. Table 2 summarizes the comparisons of the conditions in which signifcant mean diferences exist (i.e., p < .05). According to the analysis, the use of methods such as UA and UT contributed individually or together to improve objective evaluations (e.g., an increase in win rate and a decrease in mean disagreement) and subjective evaluations (e.g., an increase in helpfulness, naturalness, controllability and comfort) from participants."
    }, {
      "page": 7,
      "region": {
        "x1": 317.5069885253906,
        "x2": 559.7203979492188,
        "y1": 467.3369140625,
        "y2": 649.177001953125
      },
      "text": "We also conducted a repeated measures ANOVA with a Greenhouse-Geisser correction to compare the results of the conditions representing the three implementation methods within the CombHG type (i.e., UA, UA+SC, and UT+UA+SC). There were signifcant efects of the implementation methods on the following metrics: mean smash speed (F2,38 = 5.733, p = .011, η2 = .232), mean disagreement (F2,38 = 79.038, p < .001, η2 = .806), helpfulness (F2,38 = 3.295, p = .049, η2 = .148), naturalness (F2,38 = 7.883, p = .002, η2 = .293), controllability (F2,38 = 5.607, p = .008, η2 = .228), and comfort (F2,38 = 4.057, p = .027, η2 = .176). However, there was no signifcant efect on win rate and defense rate. Post hoc tests with a Bonferroni correction were also conducted, and comparisons between conditions with signifcant mean differences are summarized in Table 2. Similar to the analysis of the UPHG type, the post hoc analysis indicated that the methods we present for CombHG (i.e., UT and SC) contribute to improving the objective and subjective aspects of assisting performance for users."
    }],
    "title": {
      "page": 7,
      "region": {
        "x1": 53.79800033569336,
        "x2": 229.36904907226562,
        "y1": 521.7711181640625,
        "y2": 528.6220092773438
      },
      "text": "5.1 Comparison Within HG Types"
    }
  }, {
    "paragraphs": [{
      "page": 7,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.74462890625,
        "y1": 679.574951171875,
        "y2": 707.3469848632812
      },
      "text": "To compare the assisting performance between HG types, we used the OAHG (UT), UPHG (UT+UA), and CombHG (UT+UA+SC) conditions, which showed the best assisting performance for each"
    }, {
      "page": 8,
      "region": {
        "x1": 53.349998474121094,
        "x2": 295.5634460449219,
        "y1": 256.2419128417969,
        "y2": 460.3219909667969
      },
      "text": "HG type in Section 5.1. Hereafter, the above three conditions will represent each HG type. A repeated measures ANOVA with a Greenhouse-Geisser correction was conducted to determine the efect of HG types (i.e., NHG, OAHG, UPHG, and CombHG) on each metric. As an exception, since mean disagreement cannot be calculated under the NHG condition, the analysis for mean disagreement was conducted only between the other three HG types. The ANOVA analysis revealed that the HG type has a signifcant efect on all metrics and statistical values of each metric are as follows: win rate (F3,57 = 8.112, p < .001, η2 = .299), mean smash speed (F3,57 = 5.740, p = .002, η2 = .232), defense rate (F3,57 = 3.014, p = .041, η2 = .137), mean disagreement (F2,38 = 13.586, p < .001, η2 = .417), helpfulness (F3,57 = 8.614, p < .001, η2 = .312), naturalness (F3,57 = 30.107, p < .001, η2 = .613), controllability (F3,57 = 30.083, p < .001, η2 = .613), and comfort (F3,57 = 7.108, p = .001, η2 = .272). Post hoc tests with a Bonferroni correction were conducted to fgure out whether HG type pairs had a statistically signifcant diference, and the results are summarized in Table 3."
    }, {
      "page": 8,
      "region": {
        "x1": 53.48400115966797,
        "x2": 295.5619201660156,
        "y1": 465.4249267578125,
        "y2": 657.58203125
      },
      "text": "Based on the post hoc analysis, we can conclude the following: All three HG types, that is, OAHG, UPHG, and CombHG, led to signifcantly higher win rates of participants than NHG, which demonstrates the objective efectiveness of the HGs presented in this study. Specifcally, CombHG induced a signifcantly higher mean smash speed than NHG, while OAHG induced a signifcantly higher defense rate than NHG. UPHG also induced a marginally higher smash speed than NHG, but it was not statistically significant (p = .073). Meanwhile, CombHG showed a lower mean disagreement than both UPHG and OAHG. In terms of the subjective metrics, UPHG and CombHG scored signifcantly higher in helpfulness than NHG, whereas OAHG did not. UPHG and CombHG received similar levels of subjective evaluation from participants, scoring signifcantly better than OAHG for the remaining three metrics, that is, naturalness, controllability, and comfort. NHG received the highest scores in naturalness and controllability, but this can be interpreted as an inevitable result of not exerting any artifcial force on the users."
    }],
    "title": {
      "page": 7,
      "region": {
        "x1": 317.9549865722656,
        "x2": 468.1842041015625,
        "y1": 664.880126953125,
        "y2": 671.7310180664062
      },
      "text": "5.2 Comparison of HG Types"
    }
  }, {
    "paragraphs": [{
      "page": 8,
      "region": {
        "x1": 53.36800003051758,
        "x2": 294.046630859375,
        "y1": 690.533935546875,
        "y2": 707.3469848632812
      },
      "text": "We summarize the fndings of this paper as answers to our research questions based on the analysis results and comments from the"
    }, {
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.203369140625,
        "y1": 256.2419128417969,
        "y2": 284.0150146484375
      },
      "text": "user interviews. In addition, we discuss the implications of our HG design regarding its generalization and utilization outside of the air hockey environment."
    }],
    "title": {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 134.54714965820312,
        "y1": 675.839111328125,
        "y2": 682.6900024414062
      },
      "text": "6 DISCUSSION"
    }
  }, {
    "paragraphs": [{
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.0971069335938,
        "y1": 318.1551208496094,
        "y2": 345.7040100097656
      },
      "text": "RQ1: Do the UT, UA, and SC methods that we propose and apply to the HG implementation contribute to an improvement in HG performance?"
    }, {
      "page": 8,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.7176513671875,
        "y1": 350.80792236328125,
        "y2": 510.0870056152344
      },
      "text": "The experimental results showed that the three methods proposed in this study contributed to the enhancement of the assisting performance in several metrics. First, the application of UT led to a reduction in mean disagreement for all HG types. This can be explained by the fact that the confict between HG and a user because of an inaccurate model output, that is, with a high uncertainty, was efectively prevented by the uncertainty weight. The user interviews provided more details. Five participants (P6, P7, P11, P12, and P17) commented on OAHG (vanilla) that “The interference of the guidance was excessive,” but only two participants (P8, P11) made such comments on OAHG (UT). A similar tendency was observed for UPHG. When comparing cases of non-applied and applied UT (i.e., UPHG (UA) vs. UPHG (UT+UA)), the number of participants commenting that “The interference frequency of the guidance was appropriate” increased from zero to fve (P7, P10, P16, P17, and P20)."
    }, {
      "page": 8,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.71240234375,
        "y1": 515.1909790039062,
        "y2": 608.7169799804688
      },
      "text": "On the other hand, the application of UA improved the users’ evaluation of helpfulness and controllability. Moreover, when UA worked with UT on UPHG, there were enhancements in win rate, naturalness, and comfort when compared to UPHG (vanilla). This enhancement can be explained by UA providing the efect of adjusting HG to suit an individual user’s playstyle, as we intended. In the interviews, three participants (P7, P8, and P13) reported on UPHG (UA) that, “The guidance understood my ofensive and defensive intentions,” whereas no one reported this on UPHG (vanilla)."
    }, {
      "page": 8,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.1912841796875,
        "y1": 613.8209838867188,
        "y2": 707.3469848632812
      },
      "text": "The application of SC reduced the mean disagreement, which is similar to the efect of UT. This can be explained by the fact that the HG was fully delivered to the user only when OAHG and UPHG have a matched direction, thereby reducing the degree of HG interference. Note that there was no deterioration in other metrics, even though the HG interference was controlled this way. Rather, when SC worked with UT on CombHG (i.e., CombHG (UT+UA+SC)), the mean smash speed and all subjective evaluations were enhanced when compared to CombHG (UA). This suggests that users may"
    }, {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.0461120605469,
        "y1": 256.2419128417969,
        "y2": 284.0150146484375
      },
      "text": "prefer to reduce such unnecessary HG interference. A remark from P3 to CombHG (SC+UA), “It felt positive that the guidance assisted me only when I needed it,” can supplement this."
    }, {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.56488037109375,
        "y1": 289.1189270019531,
        "y2": 415.5220031738281
      },
      "text": "Furthermore, we analyzed the distribution of Wunc and Wsim used in the UT and SC methods to determine the degree to which each method adjusted the guiding force to improve the HG performance. First, we calculated the mean and STD of the distribution of each weight within the same game. After averaging the mean and STD values over all games executed, the distribution (mean ± STD) of Wunc for OAHG and UPHG were 0.600 ± 0.390 and 0.751 ± 0.306, respectively. Because a lower weight implies a stronger decrease in guiding force, the result indicated that the UT method more aggressively controlled the guiding force for OAHG than for UPHG. Meanwhile, the distribution of Wsim was measured as 0.515 ± 0.343."
    }, {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.9417419433594,
        "y1": 420.8480529785156,
        "y2": 437.4389953613281
      },
      "text": "RQ2: Do OAHG, UPHG, and CombHG improve users’ task performance when compared to NHG?"
    }, {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.2817687988281,
        "y1": 442.5429382324219,
        "y2": 492.2340087890625
      },
      "text": "In the user experiment, all three types of HG led to a signifcantly higher win rate for users than NHG, which clearly indicates that the user’s haptic task performance was improved by the guiding force. The specifc diferences in assisting performance between the HG types are discussed in more detail in RQ3 and RQ4."
    }, {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.9410705566406,
        "y1": 497.56005859375,
        "y2": 514.1519775390625
      },
      "text": "RQ3: What diferences do OAHG and UPHG have in users’ objective and subjective evaluations?"
    }, {
      "page": 9,
      "region": {
        "x1": 52.78499984741211,
        "x2": 295.5574645996094,
        "y1": 519.2559204101562,
        "y2": 700.450927734375
      },
      "text": "There was no signifcant diference in participants’ win rates between OAHG and UPHG. However, diferences between the two HG types were revealed in other objective metrics. OAHG induced a signifcant increase in defense rate, whereas UPHG induced a marginal increase in mean smash speed when compared to NHG. This tendency was also observed in the user interviews. Six participants (P1, P2, P3, P5, P6, and P19) commented on OAHG that “The guidance helped to defend the puck toward our goal,” which is the highest number among the other HG types. P6 additionally mentioned that “It pinpointed the spot to be blocked for important defense,” which indicates that OAHG reinforced the user’s insufcient defensive ability. On the other hand, the most frequent comment participants (P1, P3, P4, P7, P10, P13, P15, and P20) mentioned about UPHG was that “The guidance assisted me in the direction I am moving,” which may induce an increase in the smash speed. This can be explained as the trained user prediction model could easily expect the user to continue to move in a direction when he/she starts moving, thereby"
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.4371337890625,
        "y1": 256.2419128417969,
        "y2": 305.9330139160156
      },
      "text": "providing an assisting guidance in that direction. Furthermore, P3 remarked that, “It informed me where to stop smashing during the smashing process, so I was able to smash stable,” which indicates that UPHG assisted the user’s smashing motion rather than simply accelerating in the moving direction."
    }, {
      "page": 9,
      "region": {
        "x1": 316.9419860839844,
        "x2": 559.7186889648438,
        "y1": 311.03692626953125,
        "y2": 579.905029296875
      },
      "text": "In subjective metrics, user evaluation of OAHG and UPHG exhibited a clear diference. UPHG was evaluated to be signifcantly higher in most metrics, that is, in naturalness, controllability, and comfort, and marginally higher in helpfulness, when compared to OAHG. We looked for the cause of this one-sided subjective evaluation in the user interviews. Participants mentioned several negative comments on OAHG as follows: “In an ofensive situation, the guidance over-asserted its intention” (P5, P7, P9, P11, and P17); “It followed the puck too hard” (P3, P10, P11, P13, and P15); “It did not ft my intention sometimes” (P13, P14, P19, and P20). P3 remarked in detail, “When the puck was on the opponent’s side, I wanted to wait, but the guidance preferred to move from side to side along the puck.” These comments indicate that, even if the behavior suggested by OAHG is optimal, it could inconvenience the users when it does not match their intentions. On the other hand, it can be seen that UPHG received higher subjective evaluations in that it did not harm the intention of the participants. P7 remarked on UPHG, “When I wanted to stay still, I could stay still, and when I tried to move toward the puck, I was assisted,” and fve participants (P7, P10, P16, P17, and P20) also positively commented that “The interference frequency of the guidance was appropriate.” Meanwhile, there was also a skeptical view on UPHG. P13 mentioned that “I tried to stop in front of the puck, but as I was assisted in the direction of movement, it moved further and touched the puck,” indicating that an occasional inaccurate guidance could induce user’s inconvenience."
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.0986328125,
        "y1": 585.2310180664062,
        "y2": 612.781982421875
      },
      "text": "RQ4: Can CombHG, which integrates OAHG and UPHG, complement each HG or provide better efects in users’ objective and subjective evaluations?"
    }, {
      "page": 9,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.71240234375,
        "y1": 617.8859252929688,
        "y2": 700.4530029296875
      },
      "text": "CombHG signifcantly lowered the mean disagreement than OAHG and UPHG, without reducing other objective and subjective metrics. This implies that CombHG succeeded in assisting users with less interference by efectively combining OAHG and UPHG. In detail, CombHG received a positive comment from six participants (P1, P3, P6, P13, P16, and P20), the highest number along with OAHG, that “The guidance helped to defend the puck toward our goal.” Additionally, it also scored high subjective evaluations along"
    }, {
      "page": 10,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5554504394531,
        "y1": 87.79393005371094,
        "y2": 192.27899169921875
      },
      "text": "with UPHG. Several user comments can summarize the advantages of CombHG. P3 remarked on CombHG that “The guidance was actively helping me in the defensive situation and allowing me attack freely in the ofensive situation, which felt ideal for me to play.” P19 also mentioned, “I felt like the guidance was chasing the puck but giving me a choice to attack.” Through these interviews, we judged that CombHG satisfed participants by providing adequate guidance in situations where it is necessary (e.g., defense against the fast-approaching puck) and ensuring participants’ autonomy in situations where various strategies are possible."
    }],
    "title": {
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 497.5951232910156,
        "y1": 303.236083984375,
        "y2": 310.0870056152344
      },
      "text": "6.1 Answers to Research Questions"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 52.99100112915039,
        "x2": 295.5574645996094,
        "y1": 224.7799530029297,
        "y2": 449.81298828125
      },
      "text": "The proposed OAHG and UPHG implementation methods (i.e., UT and UA) are applicable to other pHRI tasks if the optimal action and user behavior for the given task can be modeled as any neural network structure that receives the task state as input and outputs the action distribution (e.g., Figure 3). Because the UT method utilizes Wunc calculated using Equation (5) based on the output STD from the model, it is applicable to any neural network structure that outputs the STD of the action distribution. Because the UA method is based on the parameter update of the MAML with the user demonstration data (Equation (1)), it is applicable to any neural network structure that can be trained by the MAML algorithm. Therefore, our framework is not limited to the video game, but it can be easily extended to other pHRI tasks because data-driven modeling of the optimal action or user behavior has been successfully demonstrated in various HG scenarios (e.g., robot-assisted surgery [41, 55] and steering task [46]). To model the optimal action, we can either apply reinforcement learning in simulated environments, which is not limited to the self-play-based learning, or use the movement of skillful experts as in [5, 38, 51]. To model the user behavior, we can apply the MAML algorithm utilizing sufcient motion data from multiple users."
    }, {
      "page": 10,
      "region": {
        "x1": 53.79644775390625,
        "x2": 295.564697265625,
        "y1": 454.91693115234375,
        "y2": 515.5670166015625
      },
      "text": "In addition, our CombHG implementation method (i.e., SC) is further general because it is not limited to the specifc implementation methods of OAHG and UPHG. Any OAHG and UPHG implementations (e.g., the traditional OAHG method using a fxed reference path [15]) can be combined by the SC method because it requires only the similarity of guiding force vectors from OAHG and UPHG."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 151.8053436279297,
        "y1": 210.08609008789062,
        "y2": 216.93701171875
      },
      "text": "6.2 Generalization"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.56390380859375,
        "y1": 548.0679321289062,
        "y2": 707.3469848632812
      },
      "text": "In this paper, we proposed deep learning-based novel implementation methods for OAHG and UPHG, applying a self-play-based reinforcement learning framework for OAHG and a meta-learning framework for UPHG to achieve their best performance. Further, we proposed CombHG that aimed to complement each HG type and provide better performance than OAHG and UPHG. In detail, the three proposed implementation methods (i.e., UT, UA, and SC) were applied to the given problem and demonstrated clear performance enhancement. Through the user study, we validated the assisting performance of each HG for users conducting a haptic task and investigated the diference in the user’s subjective evaluation for each HG. The user study results indicated that UPHG and CombHG elicited signifcantly better subjective scores than OAHG. In addition, CombHG exhibited a further decrease in user disagreement compared to OAHG and UPHG, without reducing any objective and"
    }, {
      "page": 10,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.3715209960938,
        "y1": 87.79393005371094,
        "y2": 170.36102294921875
      },
      "text": "subjective scores. The comparison of each HG type based on our experimental analyses and user interviews can suggest the criteria for general HG design based on the aspects of HG that positively or negatively afect users. Considering that the generalization of the proposed HG implementation methods for other HG applications is straightforward, our fndings are expected to contribute to the design of other HG-based pHRI applications beyond the video game environment considered in this study."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 141.57261657714844,
        "y1": 533.3731079101562,
        "y2": 540.2239990234375
      },
      "text": "7 CONCLUSION"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.204345703125,
        "y1": 200.37193298339844,
        "y2": 228.14398193359375
      },
      "text": "This work was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2018R1D1A1B07043580)."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 317.9549865722656,
        "x2": 435.5114440917969,
        "y1": 185.67709350585938,
        "y2": 192.52801513671875
      },
      "text": "ACKNOWLEDGMENTS"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 321.197998046875,
        "x2": 558.2019653320312,
        "y1": 256.46710205078125,
        "y2": 261.02099609375
      },
      "text": "[1] David A Abbink and Mark Mulder. 2010. Neuromuscular analysis as a guideline"
    }, {
      "page": 10,
      "region": {
        "x1": 321.2008056640625,
        "x2": 559.026611328125,
        "y1": 264.4371337890625,
        "y2": 276.96209716796875
      },
      "text": "in designing shared control. Advances in Haptics (2010), 499–516. [2] David A Abbink, Mark Mulder, and Erwin R Boer. 2012. Haptic shared control:"
    }, {
      "page": 10,
      "region": {
        "x1": 317.95794677734375,
        "x2": 559.3827514648438,
        "y1": 280.37811279296875,
        "y2": 707.3469848632812
      },
      "text": "smoothly shifting control authority? Cognition, Technology & Work 14, 1 (2012), 19–28. [3] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. 2018. Emergent complexity via multi-agent competition. In Proceedings of the Sixth International Conference on Learning Representations (ICLR). [4] Alessandro Bettini, Samuel Lang, Allison Okamura, and Gregory Hager. 2002. Vision assisted control for manipulation using virtual fxtures: Experiments at macro and micro scales. In 2002 IEEE International Conference on Robotics and Automation (ICRA). 3354–3361. [5] Jérémy Bluteau, Sabine Coquillart, Yohan Payan, and Edouard Gentaz. 2008. Haptic guidance improves the visuo-manual tracking of trajectories. PLoS One 3, 3 (2008), e1775. [6] Rolf Boink, Marinus M Van Paassen, Mark Mulder, and David A Abbink. 2014. Understanding and reducing conficts between driver and haptic shared control. In 2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 1510–1515. [7] Alexander Broad, Ian Abraham, Todd Murphey, and Brenna Argall. 2020. Datadriven Koopman operators for model-based shared control of human–machine systems. The International Journal of Robotics Research 39, 9 (2020), 1178–1195. [8] Chia-Yu Chen, Yen-Yu Chen, Yi-Ju Chung, and Neng-Hao Yu. 2016. Motion guidance sleeve: Guiding the forearm rotation through external artifcial muscles. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). 3272–3276. [9] Laura Marchal Crespo and David J Reinkensmeyer. 2008. Haptic guidance can enhance motor learning of a steering task. Journal of Motor Behavior 40, 6 (2008), 545–557. [10] Arnold W de Jonge, Jeroen GW Wildenbeest, Henri Boessenkool, and David A Abbink. 2015. The efect of trial-by-trial adaptation on conficts in haptic shared control for free-air teleoperation tasks. IEEE Transactions on Haptics 9, 1 (2015), 111–120. [11] Jack Tigh Dennerlein, David B Martin, and Christopher Hasser. 2000. Forcefeedback improves performance for steering and combined steering-targeting tasks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’00). 423–429. [12] Patrizia Di Campli San Vito, Gözel Shakeri, Stephen Brewster, Frank Pollick, Edward Brown, Lee Skrypchuk, and Alexandros Mouzakitis. 2019. Haptic navigation cues on the steering wheel. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19). 1–11. [13] Anca D Dragan and Siddhartha S Srinivasa. 2013. A policy-blending formalism for shared control. The International Journal of Robotics Research 32, 7 (2013), 790–805. [14] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic metalearning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML). 1126–1135. [15] Benjamin AC Forsyth and Karon E MacLean. 2005. Predictive haptic guidance: Intelligent user assistance for the control of dynamic tasks. IEEE Transactions on Visualization and Computer Graphics 12, 1 (2005), 103–113. [16] Takashi Goto, Swagata Das, Yuichi Kurita, and Kai Kunze. 2018. Artifcial motion guidance: an intuitive device based on Pneumatic Gel Muscle (PGM). In The 31st Annual ACM Symposium on User Interface Software and Technology Adjunct Proceedings (UIST ’18 Adjunct). 182–184. [17] Jordi Grau-Moya, Eduard Hez, Giovanni Pezzulo, and Daniel A Braun. 2013. The efect of model uncertainty on cooperation in sensorimotor interactions. Journal of The Royal Society Interface 10, 87 (2013), 20130554."
    }, {
      "page": 11,
      "region": {
        "x1": 53.79800033569336,
        "x2": 176.6275634765625,
        "y1": 63.2986946105957,
        "y2": 67.34698486328125
      },
      "text": "CHI ’21, May 8–13, 2021, Yokohama, Japan"
    }, {
      "page": 11,
      "region": {
        "x1": 53.794010162353516,
        "x2": 295.2273254394531,
        "y1": 89.0950927734375,
        "y2": 603.7359619140625
      },
      "text": "[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Of-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning (ICML). 1861–1870. [19] JR Medina Hernández, Tamara Lorenz, and Sandra Hirche. 2015. Synthesizing anticipatory haptic assistance considering human behavior uncertainty. IEEE Transactions on Robotics 31, 1 (2015), 180–190. [20] Minsik Hong and Jerzy W Rozenblit. 2016. A haptic guidance system for computerassisted surgical training using virtual fxtures. In 2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2230–2235. [21] Amin Hosseini, Florian Richthammer, and Markus Lienkamp. 2016. Predictive haptic feedback for safe lateral control of teleoperated road vehicles in urban areas. In 2016 IEEE 83rd Vehicular Technology Conference (VTC Spring). 1–7. [22] Ioanna Iacovides, Anna L Cox, and Thomas Knoll. 2014. Learning the game: breakdowns, breakthroughs and player strategies. In CHI ’14 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’14). 2215–2220. [23] Akira Kanazawa, Jun Kinugawa, and Kazuhiro Kosuge. 2019. Adaptive motion planning for a collaborative robot based on prediction uncertainty to enhance human safety and work efciency. IEEE Transactions on Robotics 35, 4 (2019), 817–832. [24] Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in bayesian deep learning for computer vision?. In Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS). 5574–5584. [25] Soheil Kianzad, Yuxiang Huang, Robert Xiao, and Karon E MacLean. 2020. Phasking on paper: Accessing a continuum of physically assisted sketching. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ’20). 1–12. [26] Ravi Kuber, Wai Yu, and Graham McAllister. 2007. Towards developing assistive haptic feedback for visually impaired internet users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’07). 1525–1534. [27] Roel J Kuiper, Dennis JF Heck, Irene A Kuling, and David A Abbink. 2016. Evaluation of haptic and visual cues for repulsive or attractive guidance in nonholonomic steering tasks. IEEE Transactions on Human-Machine Systems 46, 5 (2016), 672–683. [28] Thanh Mung Lam, Harmen Wigert Boschloo, Max Mulder, and Marinus M Van Paassen. 2009. Artifcial force feld for haptic feedback in UAV teleoperation. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 39, 6 (2009), 1316–1330. [29] Robert W Lindeman, John L Sibert, Erick Mendez-Mendez, Sachin Patil, and Daniel Phifer. 2005. Efectiveness of directional vibrotactile cuing on a buildingclearing task. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’05). 271–280. [30] Franck Mars, Mathieu Deroo, and Jean-Michel Hoc. 2014. Analysis of humanmachine cooperation when driving with diferent degrees of haptic shared control. IEEE Transactions on Haptics 7, 3 (2014), 324–333. [31] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, and Georg Ostrovski. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529. [32] Hee-Seung Moon and Jiwon Seo. 2019. Prediction of human trajectory following a haptic robotic guide using recurrent neural networks. In 2019 IEEE World Haptics Conference (WHC). 157–162. [33] Hee-Seung Moon and Jiwon Seo. 2020. Dynamic difculty adjustment via fast user adaptation. In Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology (UIST ’20 Adjunct). 13–15. [34] Hee-Seung Moon and Jiwon Seo. 2020. Sample-efcient training of robotic guide using human path prediction network. (2020). arXiv:2008.05054 [35] Mark Mulder, David A Abbink, and Erwin R Boer. 2008. The efect of haptic guidance on curve negotiation behavior of young, experienced drivers. In 2008 IEEE International Conference on Systems, Man and Cybernetics (SMC). 804–809. [36] Mark Mulder, David A Abbink, and Erwin R Boer. 2012. Sharing control with haptics: Seamless driver support from manual to automatic control. Human Factors 54, 5 (2012), 786–798. [37] Carolina Passenberg, Antonia Glaser, and Angelika Peer. 2013. Exploring the design space of haptic assistants: The assistance policy module. IEEE Transactions"
    }, {
      "page": 11,
      "region": {
        "x1": 465.5400085449219,
        "x2": 558.2008056640625,
        "y1": 63.2986946105957,
        "y2": 67.34698486328125
      },
      "text": "Hee-Seung Moon and Jiwon Seo"
    }, {
      "page": 11,
      "region": {
        "x1": 317.9604797363281,
        "x2": 559.2738037109375,
        "y1": 89.0982666015625,
        "y2": 101.62322998046875
      },
      "text": "on Haptics 6, 4 (2013), 440–452. [38] Carlos Jesús Pérez-del Pulgar, Jan Smisek, Victor F Munoz, and André Schiele."
    }, {
      "page": 11,
      "region": {
        "x1": 317.95794677734375,
        "x2": 559.38427734375,
        "y1": 105.03619384765625,
        "y2": 594.1060180664062
      },
      "text": "2016. Using learning from demonstration to generate real-time guidance for haptic shared control. In 2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 3205–3210. [39] Johannes Pfau, Jan David Smeddinck, Ioannis Bikas, and Rainer Malaka. 2020. Bot or not? User perceptions of player substitution with deep player behavior models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ’20). 1–10. [40] Susanne Poeller, Max V Birk, Nicola Baumann, and Regan L Mandryk. 2018. Let me be implicit: Using motive disposition theory to predict and explain behaviour in digital games. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). 1–15. [41] Maura Power, Hedyeh Rafi-Tari, Christos Bergeles, Valentina Vitiello, and GuangZhong Yang. 2015. A cooperative control framework for haptic guidance of bimanual surgical tasks based on learning from demonstration. In 2015 IEEE International Conference on Robotics and Automation (ICRA). 5330–5337. [42] Louis B Rosenberg. 1993. Virtual fxtures: Perceptual tools for telerobotic manipulation. In Proceedings of IEEE Virtual Reality Annual International Symposium. 76–82. [43] Edward Schmerling, Karen Leung, Wolf Vollprecht, and Marco Pavone. 2018. Multimodal probabilistic model-based planning for human-robot interaction. In 2018 IEEE International Conference on Robotics and Automation (ICRA). 1–9. [44] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML). 1889–1897. [45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. (2017). arXiv:1707.06347 [46] Dexter RR Scobee, Vicenc Rubies Royo, Claire J Tomlin, and S Shankar Sastry. 2018. Haptic assistance via inverse reinforcement learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 1510–1517. [47] Mario Selvaggio, P Robufo Giordano, F Ficuciellol, and Bruno Siciliano. 2019. Passive task-prioritized shared-control teleoperation with haptic guidance. In 2019 International Conference on Robotics and Automation (ICRA). 430–436. [48] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. 2017. Mastering the game of go without human knowledge. Nature 550, 7676 (2017), 354–359. [49] Jan Smisek, Emmanuel Sunil, Marinus M van Paassen, David A Abbink, and Max Mulder. 2016. Neuromuscular-system-based tuning of a haptic shared control interface for UAV teleoperation. IEEE Transactions on Human-Machine Systems 47, 4 (2016), 449–461. [50] Shintaro Tada, Kohei Sonoda, and Takahiro Wada. 2016. Simultaneous achievement of workload reduction and skill enhancement in backward parking by haptic guidance. IEEE Transactions on Intelligent Vehicles 1, 4 (2016), 292–301. [51] Akiko Teranishi, Timothy Mulumba, Georgios Karafotias, Jihad Mohamad Alja’Am, and Mohamad Eid. 2017. Efects of full/partial haptic guidance on handwriting skills development. In 2017 IEEE World Haptics Conference (WHC). 113–118. [52] Zheng Wang, Zhanhong Yan, and Kimihiko Nakano. 2019. Comfort-oriented haptic guidance steering via deep reinforcement learning for individualized lane keeping assist. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC). 4283–4289. [53] Zheng Wang, Rencheng Zheng, Tsutomu Kaizuka, Keisuke Shimono, and Kimihiko Nakano. 2017. The efect of a haptic guidance steering system on fatiguerelated driver behavior. IEEE Transactions on Human-Machine Systems 47, 5 (2017), 741–748. [54] Dongbo Xiao and Roger Hubbold. 1998. Navigation guided by artifcial force felds. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’98). 179–186. [55] Ehsan Zahedi, Javad Dargahi, Michael Kia, and Mehrdad Zadeh. 2017. Gesturebased adaptive haptic guidance: A comparison of discriminative and generative modeling approaches. IEEE Robotics and Automation Letters 2, 2 (2017), 1015– 1022."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 317.9549865722656,
        "x2": 387.3695983886719,
        "y1": 243.46005249023438,
        "y2": 250.31097412109375
      },
      "text": "REFERENCES"
    }
  }]
}