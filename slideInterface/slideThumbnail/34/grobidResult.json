{"authors": "Hee-Seung Moon; Jiwon Seo", "pub_date": "", "title": "Optimal Action-based or User Prediction-based Haptic Guidance: Can You Do Even Beter?", "abstract": "Figure 1: Overview of this research. (a) We built a virtual air hockey task environment controlled by a haptic device. Haptic guidance that assists a user's task performance is delivered through this device. (b) We implemented optimal action-based and user prediction-based haptic guidance using deep learning-based approaches, proposed a combined haptic guidance for better performance, and experimentally compared them.", "sections": [{"heading": "INTRODUCTION", "text": "With the recent advancements in artifcial intelligence and robotics technology, it is becoming increasingly common for users to be assisted by robots or computing machineries. In the context of physical human-robot interaction (pHRI), haptic modality has a high use potential; haptic feedback can directly pass through the neuromuscular system without going through a high-level recognition process [1]; therefore, a user can respond faster than when other modalities (e.g., visual feedback) are used. Recently, the haptic guidance (HG) system, also called the haptic shared control system, has been accepted as a promising approach for human-machine interfaces or pHRI situations [2]. The HG system is defned as a method in which the control input determined through physical interaction between the force exerted by a human operator and the guiding force of a robot is applied to the target system [1]. Compared to the previous support systems using haptic cueing [12,29] or input-mixing shared control [13], an advantage of the HG system is that the user can not only recognize the intention of the robot (e.g., direction and strength), but also choose to what extent this intention to be refected. For example, a user usually follows the robot's guidance, but when his/her choice is necessary, he/she can apply a force that exceeds the HG to perform the desired control.\nHG technologies presented in earlier studies can be classifed into two categories according to their design method: optimal actionbased haptic guidance (OAHG) and user prediction-based haptic guidance (UPHG). OAHG is designed to convey the optimal movement for performing a task in the current state (e.g., guiding the user toward the center of the road in a steering task [15,35,53] or through the movement of a skilled expert in a peg-in-hole task [38]). Meanwhile, UPHG is designed to provide proactive guidance in the direction the user intends to move based on their behavior prediction. For example, in a steering task, it guides the users to their individually preferred courses rather than the center of the road [10]. Both types of HG systems have been proven to have several positive efects in terms of task performance improvement [5,10,15,19,27,35,55], user workload reduction [19,27,35], and user subjective satisfaction [15,19,27] in various recent studies.\nWhile most of the previous studies deal with the design methods and efects of OAHG and UPHG, very few studies have clearly compared OAHG and UPHG. The two HGs are expected to have diferent characteristics because each goal behavior is diferent. OAHG informs the user of the most optimal action for the current task. However, if the guidance is in confict with the user's intention (i.e., a disagreement occurs), it will lead to an undesired physical interaction between the user and the robot, which can induce discomfort and frustration for the user [13,19]. On the other hand, UPHG supports comfortable movements of the users by reducing trajectory mismatch with the robot [10], but has a limitation in that it cannot present more optimal movements, although they may exist. In this context, this study aims to answer the following questions, which have important implications for pHRI design, \"What is the diference between OAHG and UPHG in terms of user acceptance?\" and \"Is it possible to design a better HG by combining OAHG and UPHG?\"\nTo impartially compare OAHG and UPHG, we need to implement each type of HG to achieve its best performance for a given application. Consequently, we present the following implementation methods to improve the performance of each HG based on deep learning approaches, which are attracting attention in recent HG studies [7,46,52]. First, a deep reinforcement learning algorithm is used to train an optimal action model (for OAHG) to learn the optimal policy (i.e., the optimal way of behaving from a specifc state during a task) through self-play between AI agents [3] (the upper fow in Figure 1(b)). Second, we train a user prediction model (for UPHG) in a supervised manner with multiple user's behavior data. To deal with individual diferences between human operators, we apply a meta-learning approach to enable to adapt the model parameters according to the current user (the lower fow in Figure 1(b)). Third, both the optimal action model and the user prediction model are designed to infer the model uncertainty from their outputs. By making HGs consider their model uncertainty at every timestep, we aim to prevent the decrease in HG performance because of an inaccurate model.\nAnother essential purpose of our study is to explore the possibility of complementing each HG type. If OAHG and UPHG are in a trade-of relationship with each other, it can be expected to produce HG that achieves optimal performance in terms of objective and subjective metrics by properly harmonizing the two HG types.\nIn particular, we focus on the fact that the disagreement between the guiding force and the user intention reduces the OAHG performance [13,19]. Therefore, we devised a method to implement the combined haptic guidance (CombHG) utilizing the similarity of guiding forces generated by the two HG types every timestep to minimize the disagreement. With this similarity-based method, the combined guiding force is adjusted according to the diference between the two guiding force directions.\nWe conducted a user experiment with 20 participants to verify how each type of HG has diferent efects on objective and subjective evaluations. We built a task environment for confronting an AI agent in an air hockey game that can be operated and assisted through a haptic device, as shown in Figure 1(a). Each participant played the air hockey game with the HGs we implemented, and subjective evaluations including user interviews were conducted after each task. Our user experiment results indicate the following important points. First, all types of HG-OAHG, UPHG, and CombHG-led to a signifcant improvement in objective user performance compared to the case where the user did not receive any HG, but there was no signifcant diference in objective performance between the three HG types. Second, UPHG and CombHG elicited a signifcantly higher score in subjective metrics, such as perceived naturalness and comfort, than OAHG. Finally, CombHG signifcantly lowered the disagreement between the user intention and HG compared to the cases of OAHG and UPHG, without reducing the objective and subjective scores.\nOverall, this paper has following three key contributions:\n\u2022 We present deep learning-based approach to implement OAHG and UPHG to achieve their best performance, applying a self-play-based reinforcement learning framework for OAHG and a meta-learning framework for UPHG. In particular, we propose and verify two novel implementation methods-uncertainty-based thresholding and user adaptation-to improve the HG performance. \u2022 We propose and verify a combined approach (CombHG) of OAHG and UPHG that can complement each HG type, utilizing our similarity-based combination method. To the authors' knowledge, this is the frst attempt to combine OAHG and UPHG to achieve better performance. \u2022 We experimentally compare OAHG, UPHG, and CombHG in terms of objective and subjective metrics through a user study and interview (n=20).", "n_publication_ref": 33, "n_figure_ref": 3}, {"heading": "RELATED WORK 2.1 Haptic Guidance", "text": "Early HG studies were based on control assistance methods in virtual or teleoperation environments, such as a virtual fxture [42] and an artifcial force feld [54], which help users accurately move toward goals and prevent access to dangerous areas. Robotic devices enabled the delivery of haptic feedback generated by the virtual fxture or artifcial force feld techniques to users, allowing them to perform the tasks with improved stability [4,28]. Over the past few decades, the implementation of HG has made signifcant progress, and it has been embedded in a variety of forms, including haptic devices [5,10,38,55], sleeve devices [8,16], pen devices [25], and steering wheels [9,21,35,50,53]. Accordingly, the range of HG applications has also expanded, such as in surgical assistance [20,41,55], driving assistance [21,35,50,53], teleoperation of robots [38,47] and UAVs [28,49], and desktop computer interfaces [11,26]. A majority of the previous studies have reported positive efects of HG, such as improvement in task performance and user comfort, and reduction of user workload. Nevertheless, some elements of HG that hamper usability remain. As the most representative example, a confict between the user and the HG can lead to a temporary increase in the force exerted by the user [6,36], or even a decrease in performance [37]. In addition, if the interference of HG is excessive for a user, then the user requires more physical efort, which leads to a deteriorated user evaluation (e.g., low comfort and controllability) [30]. A detailed analysis of the factors afecting HG will bring useful implications in designing a user-friendly HG. In this study, we aim to determine what users expect (and not expect) from HG through a comparison between OAHG and UPHG, which was not sufciently covered in previous studies. Furthermore, we attempt, for the frst time, to optimally combine OAHG and UPHG to achieve better performance based on the understanding of the factors that infuence HG performance.", "n_publication_ref": 33, "n_figure_ref": 0}, {"heading": "Implementing Optimal Action-based HG", "text": "The most straightforward way of implementing OAHG is to set a reference trajectory for performing a task and deliver a continuous guiding force so that the user does not leave the trajectory. In the context of a steering task such as when driving, a number of studies have implemented OAHG in the form of a guiding force directed toward the center of the path [9,15,21,35,53]. As another example, in a backward parking situation, Tada et al. [50] implemented OAHG by utilizing a Bezier curve between a start point and a target parking point as a reference trajectory. Meanwhile, when the reference trajectory could not be clearly defned in advance, a demonstration by skilled experts also served as the reference trajectory, for example, in the case of a handwriting task [5,51] and a peg-in-hole task [38].\nRecently, attempts have been made to apply reinforcement learning techniques in HG implementation to train an optimal policy for a task. Scobee et al. [46] attempted to determine the underlying value function of each observation state of a steering task from the movement of an expert operator through the inverse reinforcement learning method, and developed OAHG based on the trained value function. Meanwhile, the deep Q-network (DQN) was applied in [52] to train HG that minimizes the magnitude of the steering wheel angle during a steering task. These previous reinforcement learning-based HGs showed promising results, but had a limitation in that they cannot provide a perfectly optimal action because they were trained by data from very few (one or two) human operators. In this study, we trained an optimal action model for implementing OAHG by using a latest reinforcement learning framework based on self-play between AI agents only [3].", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Implementing User Prediction-based HG", "text": "Research on UPHG emerged later than research on OAHG and has received less attention. De Jonge et al. [10] presented UPHG according to a reference trajectory adapted to each user based on his/her previous trial in a steering task, whereas OAHG utilized a fxed reference trajectory (i.e., center of the path). Their personalized UPHG demonstrated the positive efect of reducing the confict between the human operator and HG. Meanwhile, data-driven approaches have also been used to model a user's movement. Hern\u00e1ndez et al. [19] stochastically modeled each user's individual movement based on a hidden Markov model (HMM) in a task of moving a virtual object to a target position avoiding obstacles, and presented UPHG to assist the user's movement based on the user model. Previous studies such as [41] and [55] also presented HG based on a user movement model using HMMs in a surgical task environment.\nIn this study, we implement a user model for UPHG based on deep neural networks. Such an approach has recently been highlighted for user modeling in HCI and HRI felds [32,34,39,43]. To apply UPHG tailored for an individual user, it is necessary to learn on each individual's data. However, collecting enough data to train the neural network-based model from scratch for every new user is very inefcient, particularly, when targeting numerous users. To solve this challenging problem, we apply a meta-learning approach, enabling the trained model to adapt the model parameters to a new user based on his/her short trial data, which is the frst attempt in UPHG implementation.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "HAPTIC GUIDANCE DESIGN 3.1 Target Task Environment", "text": "As a target task for the experimental investigation of various types of HG, we developed a virtual air hockey game environment that can be controlled using a haptic device (Figure 2(a)). In our air hockey game environment, a player competes with an opponent AI on a slippery surface by moving his/her paddle, which is used to hit a puck (Figure 2(b)). The player's successful task execution is judged by how many times he/she wins against the opponent AI. The player wins a round if he/she succeeds in putting a puck into the opposing goalpost, whereas he/she loses the round if the opponent puts a puck in the player's goalpost. Each paddle is set to move only within its own side (i.e., above or below the halfline). The player's paddle is controlled by a two-dimensional action vector, which corresponds to the desired paddle location on the xy coordinates with the midpoint of the player's side as the origin (Figure 2(b)). The action vector has one-to-one correspondence with the position of the end efector of the haptic device. For intuitive control of the player, the end efector is also set to move only on the 2D plane, and the moving directions of the end efector and the paddle are matched (Figure 2", "n_publication_ref": 0, "n_figure_ref": 4}, {"heading": "(a)).", "text": "A video game environment is suitable for comparing OAHG and UPHG in that each player can adopt various strategies according to his/her preference [22,40] while having a common winning formula. A player's basic strategy to win the air hockey game is to accurately smash the puck toward the opponent's goal and defend against the puck from the opponent heading inside the player's goal. However, in a detailed process, players can have several choices. A player can smash along a path that goes directly toward the goal, or he/she can choose a path through one or two refections using the wall, avoiding the opponent's paddle. In addition, in a defensive situation, that is, when the opponent has the puck, a player can press the opponent near the half-line to narrow the angle of attack or wait for the opponent's attack right in front of the goal. We expect UPHG to predict the player's choices in advance and assist him/her with the corresponding action, while OAHG informs him/her of the most optimal action he/she can choose.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Optimal Action Model and User Prediction Model", "text": "We implemented the following two models based on deep neural networks: 1) an optimal action model that outputs the most optimal action that a user can take and 2) a user prediction model that outputs the expected action to be selected by the current user.\nBecause both models use the same type of input (i.e., current game state) and output (i.e., target action vector), we designed them to have the same structure but with diferent hyperparameters (i.e., depth and size of the hidden layer) that are fne-tuned for each model's training. Each model was trained through diferent learning approaches based on reinforcement learning and meta-learning, respectively.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model", "text": "Architecture. Figure 3 shows the structure of our model. The air hockey game state-the two-dimensional position and velocity vectors of two paddles and a puck (total, size of 12)-is mapped to the two-dimensional distribution data, which are the mean and standard deviation (STD) of the distribution of each xy coordinate of the action vector (total, size of 4). The output distribution represents the distribution of the target action vector used to generate the guiding force (i.e., optimal action for OAHG or predicted action for UPHG). Notably, we designed the model to output the mean and STD of the distribution rather than a single action value. In our HG implementation, we utilize the mean of the distribution as a reference action to guide the users, and also consider the model uncertainty which can be inferred from the STD of the distribution. Fully connected (FC) hidden layers with rectifed linear unit (ReLU) activation functions are present between the input and output nodes. We used grid search to determine the optimal hyperparameters that exhibit the best learning performance in each training method; therefore, the depth and size of the hidden layers are diferent for the optimal action model and user prediction model: 2 \u00d7 64 and 4 \u00d7 80, respectively. The output STD goes through an additional sigmoid activation function.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Optimal Action Model", "text": "Training. Reinforcement learning techniques have succeeded in acquiring an optimal policy that surpasses a human player in simulated video game environments [31,48]. To train the optimal action model for our air hockey task, which is based on the competition between two players, we applied a latest learning framework based on self-play between two AI agents [3].\nIn this framework, a training agent of the frst generation initially confronts an opponent with random movements and updates the action model for increasing the expected reward. As the learning progresses, the training agent of the next generation competes with the agents of the previous generations, which also grow gradually, therefore the optimal action model can be progressively developed without human operator intervention. Meanwhile, the training agent updates the model using the sampled action from the action distribution output of the model. Therefore, when the specifc action is confdently more optimal than other actions, the model will train to output a lower STD value, that is, the STD refects the model uncertainty at current state. For implementation details, trust region policy optimization (TRPO) [44] was used as the learning algorithm that updates the policy of the training agent, because it internally had the best learning performance for our air hockey task among the latest algorithms such as proximal policy optimization (PPO) [45] and soft actor-critic (SAC) [18]. Self-play-based procedural learning was performed over 100 generations. Each generation was trained with a simulation of 200K timesteps, and batch updates were applied every 5K timesteps.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "User Prediction Model Training.", "text": "In utilizing the user prediction model, we mainly focus on how to train the model to adapt to diferent users. Inspired by the fact that the existing meta-learning frameworks train models to quickly learn to perform new unseen tasks using only a few datapoints of each new task, we applied a meta-learning approach to train the user prediction model for action taken by a player at a game state x i , and y \u02c6i and \u03c3 \u02c6i are the predicted mean and STD output by feeding x i into the model with parameter \u03b8 . With this loss function, the model can be trained to output the STD that implies the uncertainty of the predicted action; that is, the lower the STD values, the more confdent the model is about the prediction.\n\uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 Optimal Action-\nFor the user prediction model training, we collected data from nine participants aged between 22-29 (mean=25.33, STD=2.16) while performing the air hockey task without haptic guidance. Each participant was provided sufcient practice time to be familiar another user prediction model in a typical supervised manner, excluding the meta-learning approach, employing the same dataset used in the training with UA. In this case, the model trained without UA will predict generalized user movement because it is trained to use fxed network parameters for various user behaviors. An Adam optimizer with a learning rate of 0.001 was used and the batch size was 1K timesteps. The entire training was conducted for 100 epochs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Generating HGs Based on Trained Models", "text": "The trained models output the target action for assisting the user's task. A straightforward and reliable approach to generate HG is to implement a spring force toward the target action so that users can match their actions accordingly [10,15,35,46]. Meanwhile, one of the important implications from the previous studies on HG implementation is that a human operator cannot respond to HG immediately. Accordingly, a lookahead method [15] where HG should be proactively applied based on a slight future game state, considering the reaction time of a human, has been applied as a common technique. Following the method described in [15], we used the anticipated game state x lookahead , which is obtained by\nL(\u03b8 U (2) adapt , D v U al id ). min\nvirtually moving the puck and paddles with the current velocity for U a short time T lookahead , instead of the current game state x as the Through this two-fold backpropagation, the user prediction model eventually learns to rapidly adapt to unseen users. Meanwhile, we aim to train the model to output the STD of the predicted action distribution, which is impossible with the mean-squared error loss function commonly used in general regression learning. Inspired by a previous work [24] that modeled uncertainty in deep learning model input. Therefore, the guiding force of our OAHG and UPHG is generated by the following equation:\nF H G = clip(\u2212K(u \u2212 y \u02c6)),(4)\nwhere K is the stifness gain, u is the user's current action vector, and y \u02c6 is the mean of the target action vector distribution obtained by feeding x lookahead into the model. A clipping function was models for computer vision tasks, we use the following loss function to train our user prediction model:\n1 1 1 \u03c3 i \u2225 2 \u2225y i \u2212 y \u02c6i \u2225 2 + log \u2225\u03c3 \u02c6i \u2225 2 2\u2225 \u02c62\n\u00d5 applied to make the guiding force bounded, because momentary , (3) excessive force may induce safety problems to the user.\nL(\u03b8, D) = N (x i ,y i )\u2208D\nAdditionally, we propose an uncertainty-based thresholding (UT) method that adjusts the magnitude of HG by considering model uncertainty. The importance of model uncertainty has been recognized in several previous pHRI studies [17,19,23], as the results from inaccurate models of robot or human movements can lead to human discomfort or safety issues during human-robot collaborations. We infer the model uncertainty at a specifc state from the STD of the target action; that is, if the output STD is large, the model at current state is judged to be highly uncertain. Using this, we defned the uncertainty weight according to the squared magnitude of the STD as follows:\n1, if \u2225\u03c3 \u02c6 \u2225 2 < T low \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 T hi\u0434h \u2212 \u2225\u03c3 \u02c6 \u2225 2 , (5\n)\nT hi\u0434h \u2212T l ow if T low \u2264 \u2225\u03c3 \u02c6 \u2225 2 < T hi\u0434h 0, otherwise,\nwith the task prior to data collection. A total of 360K timesteps of W unc = recorded behavioral data were employed to train the user prediction model. During our meta-learning process, the adaptation was conducted with an inner learning rate of 0.1, and the meta-update where \u03c3 \u02c6 is the output STD, and T low and T hi\u0434h are threshold values. was conducted using an Adam optimizer with a learning rate of 0.001. The batch size of D U and D U was 1K timesteps, and demo val id the entire training was conducted for 200 epochs.\nTo verify the practical efectiveness of the UA method, in a subsequent user study, we experimentally compared the performance of two UPHGs based on models trained with and without UA. For the UPHG implementation without using the UA method, we trained With the UT method, calculated F OAH G or F U P H G from (4) is multiplied by the uncertainty weight, which ranges from 0 to 1, and then provided to users. In other words, only when the STD is less than a certain level (i.e., T hi\u0434h ), we determine the HG to be confdent in its purpose and can assist the users. Figure 4(a) summarizes the process of generating OAHG and UPHG, including the UT method. A straightforward approach to implement CombHG is to use the average of two guiding force vectors from OAHG and UPHG, therefore make the user to be assisted by both HG types simultaneously. This approach is intuitive in that the combined HG guides the user to the midpoint between the two target actions to be guided by OAHG and UPHG. Accordingly, we implement the simplest CombHG as follows:\nF CombH G = (F O AH G + F U P H G )/2.(6)\nHowever, simply taking the average of two guiding forces may worsen the results, guiding the user to an unintended third direction if the two HGs have diferent directions. To solve this problem, we propose a conservative combination method, namely similaritybased combination (SC), that considers the similarity of two HGs based on the angle between the two guiding forces and reduces the magnitude of HG when the similarity is low. The main purpose of the SC method is to provide an appropriate guiding force only when the target action is optimal and meets the user's intention, that is, when the directions of the two HGs match. To do this, we defne the similarity weight as follows:\nW sim = cos 2 (\u03d5/2), (7\n)\nwhere \u03d5 is the angle between the two guiding forces according to OAHG and UPHG. The similarity weight ranges from 0 to 1, which corresponds to the angle \u03d5 from \u03c0 (opposite direction) to 0 (matched direction). With the SC method, F CombH G from ( 6) is provided to the user after multiplying by the similarity weight. Figure 4(b) shows the implementation of CombHG using OAHG and UPHG, including the SC method.", "n_publication_ref": 11, "n_figure_ref": 2}, {"heading": "USER STUDY DESIGN 4.1 Research Questions", "text": "The objective of this research is divided into two main parts: a presentation of the implementation methods for three types of HG, that is, OAHG, UPHG, and CombHG, and an experimental investigation into how each type of HG difers in the context of user acceptance. We implemented the HGs by generating a guiding force based on the real-time output of the optimal action model and the user prediction model we trained. To achieve better performance of HG, we applied the three proposed methods (i.e., UT, UA, and SC), according to the HG type specifed in Table 1. To aid future HG studies, this user study frst aims to experimentally confrm whether the methods actually lead to a HG performance improvement. Subsequently, by using HGs to which all the applicable methods are employed, we determine whether each HG assists users when compared to a no HG (NHG) condition. Further, we investigate how the three types of HG assist users diferently in terms of objective and subjective evaluations, which is our primary research objective. Therefore, we formulate the following four research questions:\n\u2022 RQ1: Do the UT, UA, and SC methods that we propose and apply to the HG implementation contribute to an improvement in HG performance? \u2022 RQ2: Do OAHG, UPHG, and CombHG improve users' task performance when compared to NHG? \u2022 RQ3: What diferences do OAHG and UPHG have in users' objective and subjective evaluations? \u2022 RQ4: Can CombHG, which integrates OAHG and UPHG, complement each HG or provide better efects in users' objective and subjective evaluations?", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Method", "text": "We conducted an indoor laboratory experiment to measure the actual assisting performance of the implemented HGs. We recruited 20 participants (4 females and 16 males) aged between 21-30 (mean= 25.58, STD=2.09) for this user study. All participants were righthanded, and none of them reported perception defects in their vision and touch. Figure 5 presents the experimental setup. The air hockey task environment was composed of a haptic interface device (Omega.7, Force Dimension) and a 24-inch monitor, which were connected to a PC. Each participant was instructed to sit down and perform the air hockey task by holding the end efector of the haptic device with his/her dominant hand.\nWe selected HGs under the following nine conditions for the user experiment for verifying the efectiveness of the HG implementation methods (RQ1) and comparing the efect of each HG type (RQ2-RQ4): Our experimental procedure is as follows. First, all participants were informed that they would experience nine diferent HGs, and interviews would be conducted about their impressions received from each HG, immediately after experiencing each HG. Second, each participant was provided with as much practice time as he/she wanted to be familiar with the task environment. Third, for the purpose of updating the user prediction model parameters (i.e., UA), each participant performed one game in the NHG condition, which was not refected in the evaluation. One game basically consists of seven rounds (one round ends when either a player or an AI scores a goal), but if it ends earlier than the minimum play time that we set (two minutes per task), up to three additional rounds proceed to secure more data from the player. After completing UA with the recorded data, that is, updating the parameters of the user prediction model through Equation ( 1), the participant sequentially performed all nine HG conditions in a random order with counterbalancing. All participants performed one game for each HG condition, and before the start of each game, they were allowed 30 seconds to adapt to the given HG, which was not refected in the evaluation. Following the end of each game, participants assessed the subjective scores for the HG they had just assisted with, and a short interview with a supervising researcher was conducted. All participants were able to rest as much as they wanted between each game. The total duration of the experiment was between 1-1.5 hours, depending on the participants.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Measured Variables and Metrics", "text": "We measured the assisting performance of each HG condition in terms of objective metrics automatically measured by the system and subjective metrics based on participants' evaluations. Four objective metrics were used: win rate, mean smash speed, and defense rate, which indicate the degree of high task performance; and mean disagreement, which indicates the degree of high confict between a user and HG. The metrics are defned as follows:\n\u2022 Win rate: the ratio of the participant winning the opponent in one game (7-10 rounds). \u2022 Mean smash speed: the average speed of the puck hit by the participant over the opponent's side (i.e., smashed). Note that, the speed was measured in relative fgures because the air hockey task was built in a virtual environment without specifc units. \u2022 Defense rate: the proportion of the pucks blocked by the participant among the pucks headed into the participant's goal. \u2022 Mean disagreement: the average of the disagreement between the participant and HG, which is proposed in [19] and defned as follows (\u2206u denotes the user's action change during a timestep after receiving HG).\n\uf8f1 \uf8f4 H G \u2022\u2206u \uf8f2 \u2212 F T , if F T \u2022 \u2206u < 0 \u2225\u2206u \u2225 H G disa\u0434reement = \uf8f4 \uf8f3 0, otherwise.\nFor the subjective evaluation, we selected the following four items and asked the participants to rate each HG condition in terms of the items on a 7-point Likert score:\n\u2022 Helpfulness: how much the participants felt the HG helped them perform the task. \u2022 Naturalness: how natural the participants felt the assistance of the HG. \u2022 Controllability: how well the participants felt they were able to control the paddle under the HG. \u2022 Comfort: how comfortable the participants were with the assistance of the HG.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "RESULTS", "text": "Figure 6 shows the objective and subjective evaluation results for each HG condition. To answer RQ1, we frst investigated how assisting performance varies according to implementation methods within each HG type. Next, we examined how each HG type provides diferent assistance for the user (RQ2-RQ4), using the results of the HG conditions to which all applicable implementation methods were employed, that is, OAHG (UT), UPHG (UT+UA), and CombHG (UT+UA+SC).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Comparison Within HG Types", "text": "We frst compared the evaluated results of the two conditions within the OAHG type according to the implementation methods (i.e., vanilla vs. UT). A paired t-test revealed that the use of the UT method in the OAHG type signifcantly reduces the participants' mean disagreement (t = 12.143, p < .001). On the other hand, no signifcant diference was found in the remaining seven metrics between the OAHG (vanilla) and OAHG (UT) conditions.\nTo compare the three conditions within the UPHG type (i.e., vanilla, UA, UT+UA), a repeated measures ANOVA with a Greenhouse-Geisser correction was used. The analysis revealed that there were signifcant efects of the UPHG implementation methods on the following metrics: win rate (F 2,38 = 6.430, p = .004, \u03b7 2 = .253), mean disagreement (F 2,38 = 19.856, p < .001, \u03b7 2 = .511), helpfulness (F 2,38 = 3.422, p = .044, \u03b7 2 = .153), naturalness (F 2,38 = 5.156, p = .013, \u03b7 2 = .213), controllability (F 2,38 = 10.188, p = .001, \u03b7 2 = .349), and comfort (F 2,38 = 7.012, p = .006, \u03b7 2 = .270).\nHowever, there was no signifcant efect on the mean smash speed and defense rate. For the metrics with signifcant efects, we conducted post hoc tests with a Bonferroni correction to investigate a signifcant mean diference between the conditions. Table 2 summarizes the comparisons of the conditions in which signifcant mean diferences exist (i.e., p < .05). According to the analysis, the use of methods such as UA and UT contributed individually or together to improve objective evaluations (e.g., an increase in win rate and a decrease in mean disagreement) and subjective evaluations (e.g., an increase in helpfulness, naturalness, controllability and comfort) from participants.\nWe also conducted a repeated measures ANOVA with a Greenhouse-Geisser correction to compare the results of the conditions representing the three implementation methods within the CombHG type (i.e., UA, UA+SC, and UT+UA+SC). There were signifcant efects of the implementation methods on the following metrics: mean smash speed (F 2,38 = 5.733, p = .011, \u03b7 2 = .232), mean disagreement (F 2,38 = 79.038, p < .001, \u03b7 2 = .806), helpfulness (F 2,38 = 3.295, p = .049, \u03b7 2 = .148), naturalness (F 2,38 = 7.883, p = .002, \u03b7 2 = .293), controllability (F 2,38 = 5.607, p = .008, \u03b7 2 = .228), and comfort (F 2,38 = 4.057, p = .027, \u03b7 2 = .176). However, there was no signifcant efect on win rate and defense rate. Post hoc tests with a Bonferroni correction were also conducted, and comparisons between conditions with signifcant mean differences are summarized in Table 2. Similar to the analysis of the UPHG type, the post hoc analysis indicated that the methods we present for CombHG (i.e., UT and SC) contribute to improving the objective and subjective aspects of assisting performance for users.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Comparison of HG Types", "text": "To compare the assisting performance between HG types, we used the OAHG (UT), UPHG (UT+UA), and (UT+UA+SC) conditions, which showed the best assisting performance for each \n(i) > (g) .001 (f) > (d) .004 (i) > (h) .043 Comfort (f) > (d) .001 Controllability (i) > (g) .009 Comfort (i) > (g) .023\nHG type in Section 5.1. Hereafter, the above three conditions will represent each HG type. A repeated measures ANOVA with a Greenhouse-Geisser correction was conducted to determine the efect of HG types (i.e., NHG, OAHG, UPHG, and CombHG) on each metric. As an exception, since mean disagreement cannot be calculated under the NHG condition, the analysis for mean disagreement was conducted only between the other three HG types. The ANOVA analysis revealed that the HG type has a signifcant efect on all metrics and statistical values of each metric are as follows: win rate (F 3,57 = 8.112, p < .001, \u03b7 2 = .299), mean smash speed (F 3,57 = 5.740, p = .002, \u03b7 2 = .232), defense rate (F 3,57 = 3.014, p = .041, \u03b7 2 = .137), mean disagreement (F 2,38 = 13.586, p < .001, \u03b7 2 = .417), helpfulness (F 3,57 = 8.614, p < .001, \u03b7 2 = .312), naturalness (F 3,57 = 30.107, p < .001, \u03b7 2 = .613), controllability (F 3,57 = 30.083, p < .001, \u03b7 2 = .613), and comfort (F 3,57 = 7.108, p = .001, \u03b7 2 = .272). Post hoc tests with a Bonferroni correction were conducted to fgure out whether HG type pairs had a statistically signifcant diference, and the results are summarized in Table 3.\nBased on the post hoc analysis, we can conclude the following: All three HG types, that is, OAHG, UPHG, and CombHG, led to signifcantly higher win rates of participants than NHG, which demonstrates the objective efectiveness of the HGs presented in this study. Specifcally, CombHG induced a signifcantly higher mean smash speed than NHG, while OAHG induced a signifcantly higher defense rate than NHG. UPHG also induced a marginally higher smash speed than NHG, but it was not statistically significant (p = .073). Meanwhile, CombHG showed a lower mean disagreement than both UPHG and OAHG. In terms of the subjective metrics, UPHG and CombHG scored signifcantly higher in helpfulness than NHG, whereas OAHG did not. UPHG and CombHG received similar levels of subjective evaluation from participants, scoring signifcantly better than OAHG for the remaining three metrics, that is, naturalness, controllability, and comfort. NHG received the highest scores in naturalness and controllability, but this can be interpreted as an inevitable result of not exerting any artifcial force on the users.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION", "text": "We summarize the fndings of this paper as answers to our research questions based on the analysis results and comments from the user interviews. In addition, we discuss the implications of our HG design regarding its generalization and utilization outside of the air hockey environment.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Answers to Research Questions", "text": "RQ1: Do the UT, UA, and SC methods that we propose and apply to the HG implementation contribute to an improvement in HG performance?\nThe experimental results showed that the three methods proposed in this study contributed to the enhancement of the assisting performance in several metrics. First, the application of UT led to a reduction in mean disagreement for all HG types. This can be explained by the fact that the confict between HG and a user because of an inaccurate model output, that is, with a high uncertainty, was efectively prevented by the uncertainty weight. The user interviews provided more details. Five participants (P6, P7, P11, P12, and P17) commented on OAHG (vanilla) that \"The interference of the guidance was excessive,\" but only two participants (P8, P11) made such comments on OAHG (UT). A similar tendency was observed for UPHG. When comparing cases of non-applied and applied UT (i.e., UPHG (UA) vs. UPHG (UT+UA)), the number of participants commenting that \"The interference frequency of the guidance was appropriate\" increased from zero to fve (P7, P10, P16, P17, and P20).\nOn the other hand, the application of UA improved the users' evaluation of helpfulness and controllability. Moreover, when UA worked with UT on UPHG, there were enhancements in win rate, naturalness, and comfort when compared to UPHG (vanilla). This enhancement can be explained by UA providing the efect of adjusting HG to suit an individual user's playstyle, as we intended. In the interviews, three participants (P7, P8, and P13) reported on UPHG (UA) that, \"The guidance understood my ofensive and defensive intentions,\" whereas no one reported this on UPHG (vanilla).\nThe application of SC reduced the mean disagreement, which is similar to the efect of UT. This can be explained by the fact that the HG was fully delivered to the user only when OAHG and UPHG have a matched direction, thereby reducing the degree of HG interference. Note that there was no deterioration in other metrics, even though the HG interference was controlled this way. Rather, when SC worked with UT on CombHG (i.e., CombHG (UT+UA+SC)), the mean smash speed and all subjective evaluations were enhanced when compared to CombHG (UA). This suggests that users may Furthermore, we analyzed the distribution of W unc and W sim used in the UT and SC methods to determine the degree to which each method adjusted the guiding force to improve the HG performance. First, we calculated the mean and STD of the distribution of each weight within the same game. After averaging the mean and STD values over all games executed, the distribution (mean \u00b1 STD) of W unc for OAHG and UPHG were 0.600 \u00b1 0.390 and 0.751 \u00b1 0.306, respectively. Because a lower weight implies a stronger decrease in guiding force, the result indicated that the UT method more aggressively controlled the guiding force for OAHG than for UPHG. Meanwhile, the distribution of W sim was measured as 0.515 \u00b1 0.343.\nRQ2: Do OAHG, UPHG, and CombHG improve users' task performance when compared to NHG?\nIn the user experiment, all three types of HG led to a signifcantly higher win rate for users than NHG, which clearly indicates that the user's haptic task performance was improved by the guiding force. The specifc diferences in assisting performance between the HG types are discussed in more detail in RQ3 and RQ4.\nRQ3: What diferences do OAHG and UPHG have in users' objective and subjective evaluations?\nThere was no signifcant diference in participants' win rates between OAHG and UPHG. However, diferences between the two HG types were revealed in other objective metrics. OAHG induced a signifcant increase in defense rate, whereas UPHG induced a marginal increase in mean smash speed when compared to NHG. This tendency was also observed in the user interviews. Six participants (P1, P2, P3, P5, P6, and P19) commented on OAHG that \"The guidance helped to defend the puck toward our goal,\" which is the highest number among the other HG types. P6 additionally mentioned that \"It pinpointed the spot to be blocked for important defense,\" which indicates that OAHG reinforced the user's insufcient defensive ability. On the other hand, the most frequent comment participants (P1, P3, P4, P7, P10, P13, P15, and P20) mentioned about UPHG was that \"The guidance assisted me in the direction I am moving,\" which may induce an increase in the smash speed. This can be explained as the trained user prediction model could easily expect the user to continue to move in a direction when he/she starts moving, thereby providing an assisting guidance in that direction. Furthermore, P3 remarked that, \"It informed me where to stop smashing during the smashing process, so I was able to smash stable,\" which indicates that UPHG assisted the user's smashing motion rather than simply accelerating in the moving direction.\nIn subjective metrics, user evaluation of OAHG and UPHG exhibited a clear diference. UPHG was evaluated to be signifcantly higher in most metrics, that is, in naturalness, controllability, and comfort, and marginally higher in helpfulness, when compared to OAHG. We looked for the cause of this one-sided subjective evaluation in the user interviews. Participants mentioned several negative comments on OAHG as follows: \"In an ofensive situation, the guidance over-asserted its intention\" (P5, P7, P9, P11, and P17); \"It followed the puck too hard\" (P3, P10, P11, P13, and P15); \"It did not ft my intention sometimes\" (P13, P14, P19, and P20). P3 remarked in detail, \"When the puck was on the opponent's side, I wanted to wait, but the guidance preferred to move from side to side along the puck.\" These comments indicate that, even if the behavior suggested by OAHG is optimal, it could inconvenience the users when it does not match their intentions. On the other hand, it can be seen that received higher subjective evaluations in that it did not harm the intention of the participants. P7 remarked on UPHG, \"When I wanted to stay still, I could stay still, and when I tried to move toward the puck, I was assisted,\" and fve participants (P7, P10, P16, P17, and P20) also positively commented that \"The interference frequency of the guidance was appropriate.\" Meanwhile, there was also a skeptical view on UPHG. P13 mentioned that \"I tried to stop in front of the puck, but as I was assisted in the direction of movement, it moved further and touched the puck,\" indicating that an occasional inaccurate guidance could induce user's inconvenience.\nRQ4: Can CombHG, which integrates OAHG and UPHG, complement each HG or provide better efects in users' objective and subjective evaluations?\nCombHG signifcantly lowered the mean disagreement than OAHG and UPHG, without reducing other objective and subjective metrics. This implies that CombHG succeeded in assisting users with less interference by efectively combining OAHG and UPHG. In detail, CombHG received a positive comment from six participants (P1, P3, P6, P13, P16, and P20), the highest number along with OAHG, that \"The guidance helped to defend the puck toward our goal.\" Additionally, it also scored high subjective evaluations along with UPHG. Several user comments can summarize the advantages of CombHG. P3 remarked on CombHG that \"The guidance was actively helping me in the defensive situation and allowing me attack freely in the ofensive situation, which felt ideal for me to play.\" P19 also mentioned, \"I felt like the guidance was chasing the puck but giving me a choice to attack.\" Through these interviews, we judged that CombHG satisfed participants by providing adequate guidance in situations where it is necessary (e.g., defense against the fast-approaching puck) and ensuring participants' autonomy in situations where various strategies are possible.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Generalization", "text": "The proposed OAHG and UPHG implementation methods (i.e., UT and UA) are applicable to other pHRI tasks if the optimal action and user behavior for the given task can be modeled as any neural network structure that receives the task state as input and outputs the action distribution (e.g., Figure 3). Because the UT method utilizes W unc calculated using Equation ( 5) based on the output STD from the model, it is applicable to any neural network structure that outputs the STD of the action distribution. Because the UA method is based on the parameter update of the MAML with the user demonstration data (Equation ( 1)), it is applicable to any neural network structure that can be trained by the MAML algorithm. Therefore, our framework is not limited to the video game, but it can be easily extended to other pHRI tasks because data-driven modeling of the optimal action or user behavior has been successfully demonstrated in various HG scenarios (e.g., robot-assisted surgery [41,55] and steering task [46]). To model the optimal action, we can either apply reinforcement learning in simulated environments, which is not limited to the self-play-based learning, or use the movement of skillful experts as in [5,38,51]. To model the user behavior, we can apply the MAML algorithm utilizing sufcient motion data from multiple users.\nIn addition, our CombHG implementation method (i.e., SC) is further general because it is not limited to the specifc implementation methods of OAHG and UPHG. Any OAHG and UPHG implementations (e.g., the traditional OAHG method using a fxed reference path [15]) can be combined by the SC method because it requires only the similarity of guiding force vectors from OAHG and UPHG.", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "CONCLUSION", "text": "In this paper, we proposed deep learning-based novel implementation methods for OAHG and UPHG, applying a self-play-based reinforcement learning framework for OAHG and a meta-learning framework for UPHG to achieve their best performance. Further, we proposed CombHG that aimed to complement each HG type and provide better performance than OAHG UPHG. In detail, the three proposed implementation methods (i.e., UT, UA, and SC) were applied to the given problem and demonstrated clear performance enhancement. Through the user study, we validated the assisting performance of each HG for users conducting a haptic task and investigated the diference in the user's subjective evaluation for each HG. The user study results indicated that UPHG and CombHG elicited signifcantly better subjective scores than OAHG. In addition, CombHG exhibited a further decrease in user disagreement compared to OAHG and UPHG, without reducing any objective and subjective scores. The comparison of each HG type based on our experimental analyses and user interviews can suggest the criteria for general HG design based on the aspects of HG that positively or negatively afect users. Considering that the generalization of the proposed HG implementation methods for other HG applications is straightforward, our fndings are expected to contribute to the design of other HG-based pHRI applications beyond the video game environment considered in this study.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Neuromuscular analysis as a guideline in designing shared control", "journal": "", "year": "2010", "authors": "A David; Mark Abbink;  Mulder"}, {"title": "Haptic shared control: smoothly shifting control authority? Cognition", "journal": "Technology & Work", "year": "2012", "authors": "A David; Mark Abbink; Erwin R Mulder;  Boer"}, {"title": "Emergent complexity via multi-agent competition", "journal": "", "year": "2018", "authors": "Trapit Bansal; Jakub Pachocki; Szymon Sidor"}, {"title": "Vision assisted control for manipulation using virtual fxtures: Experiments at macro and micro scales", "journal": "IEEE International Conference on Robotics and Automation", "year": "2002", "authors": "Alessandro Bettini; Samuel Lang; Allison Okamura; Gregory Hager"}, {"title": "Haptic guidance improves the visuo-manual tracking of trajectories", "journal": "PLoS One", "year": "2008", "authors": "J\u00e9r\u00e9my Bluteau; Sabine Coquillart; Yohan Payan; Edouard Gentaz"}, {"title": "Understanding and reducing conficts between driver and haptic shared control", "journal": "", "year": "2014", "authors": "Rolf Boink; Mark Marinus M Van Paassen; David A Mulder;  Abbink"}, {"title": "Datadriven Koopman operators for model-based shared control of human-machine systems", "journal": "The International Journal of Robotics Research", "year": "2020", "authors": "Alexander Broad; Ian Abraham; Todd Murphey; Brenna Argall"}, {"title": "Motion guidance sleeve: Guiding the forearm rotation through external artifcial muscles", "journal": "", "year": "2016", "authors": "Chia-Yu Chen; Yen-Yu Chen; Yi-Ju Chung; Neng-Hao Yu"}, {"title": "Haptic guidance can enhance motor learning of a steering task", "journal": "Journal of Motor Behavior", "year": "2008", "authors": "Laura Marchal Crespo; David J Reinkensmeyer"}, {"title": "The efect of trial-by-trial adaptation on conficts in haptic shared control for free-air teleoperation tasks", "journal": "IEEE Transactions on Haptics", "year": "2015", "authors": " Arnold W De Jonge; G W Jeroen; Henri Wildenbeest; David A Boessenkool;  Abbink"}, {"title": "Forcefeedback improves performance for steering and combined steering-targeting tasks", "journal": "", "year": "2000", "authors": " Jack Tigh Dennerlein; B David; Christopher Martin;  Hasser"}, {"title": "Haptic navigation cues on the steering wheel", "journal": "", "year": "2019", "authors": "Patrizia Di Campli San; G\u00f6zel Vito; Stephen Shakeri; Frank Brewster; Edward Pollick;  Brown"}, {"title": "A policy-blending formalism for shared control", "journal": "The International Journal of Robotics Research", "year": "2013", "authors": "D Anca; Siddhartha S Dragan;  Srinivasa"}, {"title": "Model-agnostic metalearning for fast adaptation of deep networks", "journal": "", "year": "2017", "authors": "Chelsea Finn; Pieter Abbeel; Sergey Levine"}, {"title": "Predictive haptic guidance: Intelligent user assistance for the control of dynamic tasks", "journal": "IEEE Transactions on Visualization and Computer Graphics", "year": "2005", "authors": "A C Benjamin; Karon E Forsyth;  Maclean"}, {"title": "Artifcial motion guidance: an intuitive device based on Pneumatic Gel Muscle (PGM)", "journal": "", "year": "2018", "authors": "Takashi Goto; Swagata Das; Yuichi Kurita; Kai Kunze"}, {"title": "The efect of model uncertainty on cooperation in sensorimotor interactions", "journal": "Journal of The Royal Society Interface", "year": "2013", "authors": "Jordi Grau-Moya; Eduard Hez; Giovanni Pezzulo; Daniel A Braun"}, {"title": "Soft actor-critic: Of-policy maximum entropy deep reinforcement learning with a stochastic actor", "journal": "", "year": "2018", "authors": "Tuomas Haarnoja; Aurick Zhou; Pieter Abbeel; Sergey Levine"}, {"title": "Synthesizing anticipatory haptic assistance considering human behavior uncertainty", "journal": "IEEE Transactions on Robotics", "year": "2015", "authors": "Tamara Jr Medina Hern\u00e1ndez; Sandra Lorenz;  Hirche"}, {"title": "A haptic guidance system for computerassisted surgical training using virtual fxtures", "journal": "", "year": "2016", "authors": "Minsik Hong; Jerzy W Rozenblit"}, {"title": "Predictive haptic feedback for safe lateral control of teleoperated road vehicles in urban areas", "journal": "", "year": "2016", "authors": "Amin Hosseini; Florian Richthammer; Markus Lienkamp"}, {"title": "Learning the game: breakdowns, breakthroughs and player strategies", "journal": "", "year": "2014", "authors": "Ioanna Iacovides; Anna L Cox; Thomas Knoll"}, {"title": "Adaptive motion planning for a collaborative robot based on prediction uncertainty to enhance human safety and work efciency", "journal": "IEEE Transactions on Robotics", "year": "2019", "authors": "Akira Kanazawa; Jun Kinugawa; Kazuhiro Kosuge"}, {"title": "What uncertainties do we need in bayesian deep learning for computer vision", "journal": "", "year": "2017", "authors": "Alex Kendall; Yarin Gal"}, {"title": "Phasking on paper: Accessing a continuum of physically assisted sketching", "journal": "", "year": "2020", "authors": "Soheil Kianzad; Yuxiang Huang; Robert Xiao; Karon E Maclean"}, {"title": "Towards developing assistive haptic feedback for visually impaired internet users", "journal": "", "year": "2007", "authors": "Ravi Kuber; Wai Yu; Graham Mcallister"}, {"title": "Evaluation of haptic and visual cues for repulsive or attractive guidance in nonholonomic steering tasks", "journal": "IEEE Transactions on Human-Machine Systems", "year": "2016", "authors": "J Roel;  Kuiper; J F Dennis; Irene A Heck; David A Kuling;  Abbink"}, {"title": "Artifcial force feld for haptic feedback in UAV teleoperation", "journal": "", "year": "2009", "authors": "Thanh Mung Lam; Harmen Wigert Boschloo; Max Mulder; Marinus M Van Paassen"}, {"title": "Efectiveness of directional vibrotactile cuing on a buildingclearing task", "journal": "", "year": "2005", "authors": "John L Robert W Lindeman; Erick Sibert; Sachin Mendez-Mendez; Daniel Patil;  Phifer"}, {"title": "Analysis of humanmachine cooperation when driving with diferent degrees of haptic shared control", "journal": "IEEE Transactions on Haptics", "year": "2014", "authors": "Franck Mars; Mathieu Deroo; Jean-Michel Hoc"}, {"title": "Human-level control through deep reinforcement learning", "journal": "Nature", "year": "2015", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Andrei A Rusu; Joel Veness; G Marc; Alex Bellemare; Martin Graves; Andreas K Riedmiller; Georg Fidjeland;  Ostrovski"}, {"title": "Prediction of human trajectory following a haptic robotic guide using recurrent neural networks", "journal": "", "year": "2019", "authors": "Hee-Seung Moon; Jiwon Seo"}, {"title": "Dynamic difculty adjustment via fast user adaptation", "journal": "", "year": "2020", "authors": "Hee-Seung Moon; Jiwon Seo"}, {"title": "Sample-efcient training of robotic guide using human path prediction network", "journal": "", "year": "2020", "authors": "Hee-Seung Moon; Jiwon Seo"}, {"title": "The efect of haptic guidance on curve negotiation behavior of young, experienced drivers", "journal": "", "year": "2008", "authors": "Mark Mulder; A David; Erwin R Abbink;  Boer"}, {"title": "Sharing control with haptics: Seamless driver support from manual to automatic control", "journal": "Human Factors", "year": "2012", "authors": "Mark Mulder; A David; Erwin R Abbink;  Boer"}, {"title": "Exploring the design space of haptic assistants: The assistance policy module", "journal": "", "year": "2013", "authors": "Carolina Passenberg; Antonia Glaser; Angelika Peer"}, {"title": "Using learning from demonstration to generate real-time guidance for haptic shared control", "journal": "", "year": "2016", "authors": "Carlos Jes\u00fas P\u00e9rez-Del Pulgar; Jan Smisek; F Victor; Andr\u00e9 Munoz;  Schiele"}, {"title": "Bot or not? User perceptions of player substitution with deep player behavior models", "journal": "", "year": "2020", "authors": "Johannes Pfau; Jan David Smeddinck; Ioannis Bikas; Rainer Malaka"}, {"title": "Let me be implicit: Using motive disposition theory to predict and explain behaviour in digital games", "journal": "", "year": "2018", "authors": "Susanne Poeller; V Max; Nicola Birk; Regan L Baumann;  Mandryk"}, {"title": "A cooperative control framework for haptic guidance of bimanual surgical tasks based on learning from demonstration", "journal": "", "year": "2015", "authors": "Maura Power; Hedyeh Rafi-Tari; Christos Bergeles; Valentina Vitiello; Guang-Zhong Yang"}, {"title": "Virtual fxtures: Perceptual tools for telerobotic manipulation", "journal": "", "year": "1993", "authors": "B Louis;  Rosenberg"}, {"title": "Multimodal probabilistic model-based planning for human-robot interaction", "journal": "IEEE International Conference on Robotics and Automation (ICRA)", "year": "2018", "authors": "Edward Schmerling; Karen Leung; Wolf Vollprecht; Marco Pavone"}, {"title": "Trust region policy optimization", "journal": "", "year": "2015", "authors": "John Schulman; Sergey Levine; Pieter Abbeel; Michael Jordan; Philipp Moritz"}, {"title": "Proximal policy optimization algorithms", "journal": "", "year": "2017", "authors": "John Schulman; Filip Wolski; Prafulla Dhariwal; Alec Radford; Oleg Klimov"}, {"title": "Haptic assistance via inverse reinforcement learning", "journal": "", "year": "2018", "authors": " Dexter Rr Scobee; Claire J Vicenc Rubies Royo; S Shankar Tomlin;  Sastry"}, {"title": "Passive task-prioritized shared-control teleoperation with haptic guidance", "journal": "", "year": "2019", "authors": "Mario Selvaggio; Robufo Giordano; F Ficuciellol; Bruno Siciliano"}, {"title": "Mastering the game of go without human knowledge", "journal": "Nature", "year": "2017", "authors": "David Silver; Julian Schrittwieser; Karen Simonyan; Ioannis Antonoglou; Aja Huang; Arthur Guez; Thomas Hubert; Lucas Baker; Matthew Lai; Adrian Bolton"}, {"title": "Neuromuscular-system-based tuning of a haptic shared control interface for UAV teleoperation", "journal": "IEEE Transactions on Human-Machine Systems", "year": "2016", "authors": "Jan Smisek; Emmanuel Sunil;  Marinus M Van Paassen; A David; Max Abbink;  Mulder"}, {"title": "Simultaneous achievement of workload reduction and skill enhancement in backward parking by haptic guidance", "journal": "IEEE Transactions on Intelligent Vehicles", "year": "2016", "authors": "Shintaro Tada; Kohei Sonoda; Takahiro Wada"}, {"title": "Efects of full/partial haptic guidance on handwriting skills development", "journal": "", "year": "2017", "authors": "Akiko Teranishi; Timothy Mulumba; Georgios Karafotias"}, {"title": "Comfort-oriented haptic guidance steering via deep reinforcement learning for individualized lane keeping assist", "journal": "", "year": "2019", "authors": "Zheng Wang; Zhanhong Yan; Kimihiko Nakano"}, {"title": "The efect of a haptic guidance steering system on fatiguerelated driver behavior", "journal": "IEEE Transactions on Human-Machine Systems", "year": "2017", "authors": "Zheng Wang; Rencheng Zheng; Tsutomu Kaizuka; Keisuke Shimono; Kimi-Nakano "}, {"title": "Navigation guided by artifcial force felds", "journal": "", "year": "1998", "authors": "Dongbo Xiao; Roger Hubbold"}, {"title": "Gesturebased adaptive haptic guidance: A comparison of discriminative and generative modeling approaches", "journal": "IEEE Robotics and Automation Letters", "year": "2017", "authors": "Ehsan Zahedi; Javad Dargahi; Michael Kia; Mehrdad Zadeh"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: The air hockey game environment we developed. (a) Example of control based on a haptic device. (b) In-game screen and composition of our air hockey game.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Overview of the model structure applied to both the optimal action model and the user prediction model.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: The process of implementing (a) optimal actionbased haptic guidance (OAHG) and user prediction-based haptic guidance (UPHG), and (b) combined haptic guidance (CombHG). The three proposed methods (i.e., UT, UA, and SC), whose efectiveness is investigated through a subsequent user experiment, are highlighted in gray shades.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Experimental setup of our user study. A participant performs a virtual air hockey task with a haptic device.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "(a) NHG: no haptic guidance. (b) OAHG (vanilla): OAHG from Equation (4) with no additional method. (c) OAHG (UT): OAHG applying the uncertainty-based thresholding in addition to OAHG (vanilla). (d) UPHG (vanilla): UPHG from Equation (4) using a model not trained with the meta-learning algorithm, and therefore without the user adaptation. (e) UPHG (UA): UPHG from Equation (4) using a model trained with the meta-learning algorithm (i.e., UA applied). (f) UPHG (UT+UA): UPHG applying the uncertainty-based thresholding in addition to UPHG (UA). (g) CombHG (UA): CombHG from Equation (6) based on OAHG (vanilla) and UPHG (UA). Note that because the basic concept of UPHG is assisting a user according to personalized prediction, we use UPHG (UA) instead of UPHG (vanilla) to implement the underlying CombHG. (h) CombHG (UA+SC): CombHG applying the similaritybased combination on OAHG (vanilla) and UPHG (UA). (i) CombHG (UT+UA+SC): CombHG applying the similaritybased combination on OAHG (UT) and UPHG (UT+UA).", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Experimental results of the objective (top) and subjective (bottom) metrics for the nine HG conditions (a)-(i). The HG conditions of the same HG type are indicated by bars of the same color. Error bars represent 95% confdence intervals.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "based or User Prediction-based Haptic Guidance: Can You Do Even Beter? CHI '21, May 8-13, 2021, Yokohama, Japan HG to quickly adapt to diferent users. We demonstrated in our prior study[33] that model-agnostic meta-learning (MAML)[14], a recent mainstream meta-learning algorithm, can be used to train a model that mimics user behavior for dynamic difculty adjustment and efectively adapts to a new user with his/her minimal demonstration data. In the current study, we present a user adaptation (UA) method that trains the user prediction model based on the MAML algorithm and updates the model parameters for each user.The training process of our user prediction model is as follows. The training data consists of a set of task execution data D U of diferent users U , and D U is divided into D U for model adaptation, and", "figure_data": "demo for meta-update. First, the algorithm updates the model parameter \u03b8 by a single gradient descent that reduces L(\u03b8, D U D U val id ), demo which is the model loss function for D U demo . Therefore,\u03b8 U adapt= \u03b8 \u2212 \u03b1 \u2207 \u03b8 L(\u03b8, D U demo ),(1)where \u03b1 denotes the inner learning rate. Subsequently, a meta-update is performed to reduce model loss for D U val id updated parameters. Overall, the objective of our meta-learning is using theas follows:\u00d5"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Proposed methods applied to HG implementation.", "figure_data": "Abbrev Full FormOAHG UPHG CombHGUTUncertainty-based thresholding\u2713\u2713\u2713UAUser adaptation\u2713\u2713SCSimilarity-based combination\u2713\u2713A checkmark indicates whether it was used for the corresponding HG type."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Comparisons showing statistically signifcant diferences between implementation methods within HG type.", "figure_data": "Type MetricsComparisonpTypeMetricsComparisonpOAHG Mean disagreement(b) > (c).000 CombHG Mean smash speed(i) > (g).003UPHG Win rate(f) > (d).009(i) > (h).028Mean disagreement(d) > (f).000Mean disagreement(g) > (h).000(e) > (f).000(g) > (i).000Helpfulness(e) > (d).041(h) > (i).000Naturalness(f) > (d).032Helpfulness(i) > (g).023Controllability(e) > (d).028Naturalness"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparisons showing statistically signifcant diferences between HG types. It felt positive that the guidance assisted me only when I needed it,\" can supplement this.", "figure_data": "MetricsComparisonpMetricsComparisonpWin rateOAHG > NHG.014 NaturalnessNHG > OAHG.000UPHG > NHG.002NHG > UPHG.025CombHG > NHG .001UPHG > OAHG.001Mean smash speedCombHG > NHG .023CombHG > OAHG .000CombHG > OAHG .045 ControllabilityNHG > OAHG.000Defense rateOAHG > NHG.038UPHG > OAHG.000Mean disagreement OAHG > CombHG .000CombHG > OAHG .000UPHG > CombHG .002 ComfortUPHG > OAHG.003HelpfulnessUPHG > NHG.009CombHG > OAHG .001CombHG > NHG .001prefer to reduce such unnecessary HG interference. A remark fromP3 to CombHG (SC+UA), \""}], "doi": "10.1145/3411764.3445115"}