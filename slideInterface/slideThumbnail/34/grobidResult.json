{"authors": "Gianluca Memoli; Letizia Chisari; Jonathan P Eccles; Mihai Caleap; Bruce W Drinkwater; Sriram Subramanian", "pub_date": "", "title": "VARI-SOUND: A Varifocal Lens for Sound", "abstract": "Figure 1: VARI-SOUND is a method to design the equivalent of optical devices for sound, based on acoustic metamaterials. Our contributions include: (a) how to design compact acoustic lenses; (b) how to transform a standard speaker into a directional one; (c) how to build and adjustable focus acoustic lens i.e. the equivalent of a zooming objective for light.", "sections": [{"heading": "INTRODUCTION", "text": "The shaping of acoustic fields is a major technological advance which underpins high-fidelity sound reproduction [60], tactile feedback in consumer electronic devices [6], particle manipulation [10], non-destructive testing [24] and noninvasive therapies (e.g. for essential tremor [43] and cancer [65]). Traditionally, this is achieved by controlling the intensity or the phase of the generating source through phased arrays [36,57,58]. Phased arrays offer real-time control of sound, but are often bulky and expensive, with cost and complexity scaling with the number of channels [37]. Despite these limitations, phased arrays are in widespread use. In the human-computer interaction (HCI) community, in particular, they have paved the path towards applications like acoustic levitation [23,33,39] and mid-air haptics [6].\nThe way we manage light, however, is different: in theatres, videogames and virtual reality (VR) audience's immersivity can easily be augmented by using passive devices, like lenses and filters. When the properties of lenses and filters can be changed on demand, we get devices like autofocus cameras, liquid crystal displays and VR headsets. This is still not possible for sound: arrays of speakers are used for applications like surround sound because acoustic lenses [23] are bulky, with a physical size far thicker than the wavelength (i.e. which is \u223c35 cm at 1 kHz, the peak frequency for human hearing perception). This technological difference could explain in part the prevalence of visual (optical) technologies over acoustic ones in our modern, space-hungry world.\nAcoustic metamaterials may offer a way forward [11,31]. These are normal materials (i.e. glass, wood, 3D printer plastic), but engineered to control, direct, and manipulate waves in uncommon ways. However, their use in HCI is currently impeded by three key limitations: acoustic metamaterial devices are thick (e.g. one wavelength in [27,28,35,41,59,66]), static and often operate over a limited frequency range (e.g. 10% of the central frequency in [35]).\nIn this work, we propose a method to design metasurfaces that behave like converging lenses for sound. We address two of the limitations mentioned above, showing how these lenses can be fabricated to be as thin as 1/3rd of their wavelength of operation and how they can be combined in user-controllable devices that can be mechanically adjusted.\nIn particular, having demonstrated that some design tools commonly used in optics are also valid in acoustics when using metasurfaces, we build the acoustic equivalent of some key optical devices (i.e., a collimator, a magnifying glass, a telescope and a vari-focal lens) and test them at 5-6 kHz, where each lens is 2 cm thick. We discuss real-life applications based on these devices, how to overcome their remaining limitations (e.g. bandwidth) and the potential ways to use them in innovative, sound-based interfaces. We want to share with the HCI community how we believe acoustic metamaterials may revolutionize the way we think, design and experience sound, hoping to excite others into using them.\nOur contributions.\n\u2022 We show how to design metamaterial lenses for sound that are sufficiently small to be of practical use for the HCI community (Figure 1a). \u2022 Coupling our acoustic lenses with a generic speaker, we demonstrate and test devices equivalent to optical ones i.e. magnifying glasses, telescopes, lighthouses. . . for sound (Figure 1b). \u2022 We design and test the first dynamic metamaterial device: the equivalent of a zoom objective -i.e. a varifocal lens, but for sound (Figure 1c).", "n_publication_ref": 24, "n_figure_ref": 3}, {"heading": "RELATED WORKS Designing for sound", "text": "The design of rooms and public spaces has a binary approach to acoustics: in a cinema or a concert hall there is either the same sound for everyone or none. In practice, current sound design does not have a personal component, without headphones. Light, however, is managed differently: a theatre director or an architect can populate a scene with focused or diffused light, a spotlight that follows a character, alternation of light and shadows. Light engineers can create experiences targeted to different users with light, either by shaping light beams through passive devices (e.g. lenses, holographic filters) or by direct beamforming (e.g. spatial light modulators). Acoustics, however, is catching up: the quests for 3D spatial audio (which creates the illusion of localised sources for the audience) and localised audio (which uses highly directional sources to deliver different sounds to parts of the audience) are moving our everyday lives towards localised, high quality audio messages beyond headphones. A world like the one imagined by Philip K. Dick in \"Minority Report\" (later a movie directed by Steven Spielberg), where the main character received personalised, directional audio adverts while walking in the street, does not seem so far away.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Spatial sound", "text": "Sound is all around us, with no direction having precedence over the others, but acoustic cues have a profound directional nature. This is evident, for instance, with the sound of a car approaching from behind. Digital content, conversely, is broadcasted from sources (either headphones of loudspeakers) which have a fixed position relative to the audience.\n\"Spatial sound\" is a method of filling this need-gap [49]: playing with piloted delays and intensity differences in the real sources, this technique gives to a specific audience the illusion that sounds come from different locations around.\nUsing spatial sound techniques, creative designers can now populate scenes with numerous virtual sources, each with its own location and attributes , relying on their position (relative to the audience) for narrative (e.g. the movie \"Pearl\" [12]) and immersive purposes [63]. They can design soundscapes using object-oriented toolkits, commonly found in packages like Unity 3D (e.g. Season Traveller [47]) or Unreal.\nThe first method of delivering spatial sound is through headphones. The main advantage here is that the sound can be adjusted to move with the listener, which is ideal for sound-based navigation systems (e.g. gpsTunes [54] and SWAN [15]) and one-to-one interactions with learners (e.g. AudioChile [50]) or visually-impaired listeners [55]. The user, however, needs to be completely removed from the real space, with interactions (e.g. comments among co-workers attending the same VR meeting) only possible in the virtual world.\nThe second method, based on deploying arrays of speakers where needed, is at the basis of the surround sound systems in cinemas and home theatres (e.g. Dolby Digital, Sony SDDS). These applications marry with the room in creating the audio experience, which can affect multiple users, but the \"surround\" effect is restricted to a narrow listening area (i.e. a \"sweet spot\") and sometimes does not provide for up-down cues [1]. This is not sufficient for audio-visual VR [21]. A way forward is offered by Wave Field Synthesis [2], which aims at local reproduction of \"virtual sources\" near the listener using speakers in a linear array (e.g. modern soundbars). This method leads to better immersivity (as it reacts to where the listener is), but the finite distance between the speakers and their physical size introduce effects of spatial aliasing.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Directional sound", "text": "An alternative towards personalised multi-user audio experiences, which minimises spatial aliasing, comes from directional speakers [3]. Directional speakers have been used in HCI for multi-sensorial art displays (e.g. Tate Sensorium [38]) or to send audio messages to a specific individual or region of space (e.g. Holographic Whisper [40] or Project Telepathy [5]). The most common type (e.g. Acouspade, SoundLazer) exploits an array of ultrasonic transducers to produce a highly directional carrier wave, which is then modulated with audible signals [13]. Other speakers (i.e. Holosonics) produce the ultrasonic beam through a vibrating plate. All these speakers exploit the nonlinear effects of air to produce audible sounds [45] and can be used as audio spotlights [62], but steering the beam requires physically moving the speaker.\nAudio spotlights are not low-cost technology (costs are typically around 2,000 USD for a decent-sized system) and come with a main limitation i.e. the bandwidth of the sound they can transmit. This is typically \u223c10-20% of the driving ultrasonic frequency of operation -e.g. 4 kHz for a parametric speaker operating at 40 kHz -and typically lacks the low-frequency components: it is therefore sufficient to pass public announcements and badly reproduced music [22], but not high-quality sounds. Furthermore, as most applications involving phase control of multiple speakers, audio spotlights are prone to thermal losses and therefore have a poor response to high sound volume.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Acoustic lenses", "text": "Acoustic lenses appeared for the first time in 1930s, as a by-product of the Bell labs. They used to be inserted in loudspeakers to spread their high-frequency directional emission, but gradually fell out of favor through the 1970s and 1980s. Lens designs at the time were in fact fragile and cumbersome i.e. an accident-waiting-to-happen for the music industry, always on the move. Today, however, the technology is reemerging: acoustic lenses can be found to help beam-shaping in ultrasonic transducers and in some high-end home audio systems (e.g. Bang & Olufsen). These lenses, however, are still designed to be much larger than the wavelength (i.e. 1 metre at 340 Hz): their use is therefore confined to the higher part of the acoustic spectrum. More compact solutions, usable at lower frequencies, are highly desirable.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acoustic metamaterials", "text": "Acoustic metamaterials [11,31] are normal materials (i.e. made of glass, wood, paper, plastic, LEGO bricks), but with an internal structure engineered to control, direct, and manipulate waves in uncommon ways [16,17,35,41]. They are made of a collection of sub-wavelength structures (i.e. \"unit cells\"), each capable of locally manipulating the impinging wave in terms of its phase and/or intensity [11,35].\nThere are many unit cell designs in the literature, either based on labyrinthine structures [59], helical structures [66], space-coiling [27,28], multi-slits [34], or Helmholtz resonators. Selecting the right one depends on design constraints, like the available space to deploy them, the manufacturing techniques and the desired frequency response. This is because most unit cells typically operate over a small bandwidth 1 \u2206f 2dB on both sides of their design frequency f 0 (with \u2206f 2dB \u2264 0.2f 0 ). In addition, unit cells' dimensions increase as f 0 gets lower, with thicknesses 2 as large as \u223c 1 m at 340 Hz. Of these two limitations, we believe size to be the most crucial for the HCI community.\nNarrow-band devices, in fact, are sufficient for many applications. In audio reproduction, for instance, it is normal to cascade speakers optimised for different frequency ranges to cover the audible spectrum, even if well known songs like \"Nessun dorma\" often span little more than 1 octave 3 . In buildings and factories, with human hearing perception peaked at 1-2 kHz, controlling the delivery of one octave may be sufficient to quench annoyance due to an air conditioning unit or to deliver personalised alarm messages. Furthermore, the literature already presents acoustic metamaterials with larger bandwidths [20,26,29,52], obtained combining unit cells of different sizes. Li et al. [26], for instance, used basic 3D-printed cuboids as unit cells (i.e. Acoustic Voxels), scaled and joined to create wind instruments spanning \u223c2 octaves. Additional solutions will be discussed in section 6.\nUnit cells are then assembled into larger 3D structures, with a bottom-up \"metamaterial design\" concept recently used to create mechanical actuators (i.e. Metamaterial Devices [16,17]) and user-controllable 3D-printed structures (e.g. Coded Skeleton [18]). Particularly interesting for beamshaping applications is the case of metasurfaces: closelypacked structures of phase shifters whose thickness is smaller than or comparable to the wavelength of operation and can therefore considered to be 2D [25,56,64].\nOnce a metamaterial device is designed, however, its function is fixed, while speaker arrays can change on demand. This is another key limitation for HCI applications, which has been addressed so far using hybrid systems (devices where phased arrays are used to manipulate shapes created using metasurfaces [37]) or motorised belts (to position metasurfaces in front of a directional speaker and steer its emission [19]). Both these solutions, however, rely on a phased array to begin with. Different dynamical metamaterials have been proposed (e.g. [7,46]), with some solutions as large as a room [30], but none has the simplicity of a system of lenses. Here, we demonstrate a method to design compact acoustic devices, inspired by optics, that can be used with standard, low cost, computer-sized audio speakers.", "n_publication_ref": 28, "n_figure_ref": 0}, {"heading": "METASURFACE DESIGN", "text": "There are four key steps in designing a metasurface: 1) choosing its function (i.e. what it does to the input field); 2) transforming this information into an analogic phase/ intensity distribution on the metasurface (Figure 2a); 3) selecting the unit cells to use; 4) fabricate the metasurface, taking into account constraints in terms of space and frequency response (Figure 2b). In this section, we show how to design acoustic metasurfaces to be used in transmission. Some of our more general considerations, however, apply also to reflecting metasurfaces and other types of waves [8,9,14]. 3 To visualise the frequency response \u2206f 2dB of a metamaterial, the reader should keep in mind the 12 keys on a piano keyboard that form an octave. For reference, a typical piano has 7 octaves and a minor third, from A 0 = 27.5 Hz to C 8 = 4186 Hz, while most human voices span a maximum of 3 octaves.", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "Steps 1 & 2: From the desired field to a phase distribution", "text": "Assigning a function to a metasurface means deciding how the distribution of the acoustic pressure will look like after passing through it, both in terms of geometry and of intensity distribution. Li et al. [28] suggest to address this step as a problem of acoustic ray tracing [51]: the desired far-away field gets back-propagated to the metasurface itself, thus giving a required phase/intensity distribution at its exit. This will be the phase/intensity distribution to be encoded by the unit cells on the input wave. More details on this step for a generic field have been given elsewhere [37], so in this work we will only discuss the case of a converging lens.\nA converging lens is characterised by two quantities: its focal length and its physical extension (i.e. how many unit cells it contains). Once the desired focal length f is set along the axis of the lens (\u1e91), the phase distribution \u03c6(x, y) on the metasurface (assumed to be in the z = 0 plane) is obtained by imposing that all the contributions from the unit cells arrive in phase at (0, 0, f ). In this work, we choose to design our lenses using a parabolic phase profile [42]:\n\u03c6(r ) = \u03c6 0 \u2212 A 2 (x 2 + y 2 )(1)\nwhere \u03c6(x, y) is local phase (assigned to a unit cell), A is a constant (related to the local curvature of the phase profile), \u03bb 0 is the design wavelength and \u03c6 0 is an arbitrary constant.\nIn optics, this choice leads to more compact lenses (i.e. GRIN lenses [61]) and, as shown in Figures 2a and 3, allows the parameter A to be easily related to the \"curvature\" of the lens: a larger value of A corresponds to a more focusing lens.\nStep 3: Unit cell selection Once \u03c6(x, y) is known, the designer has to choose the unit cells to implement it. As mentioned above, many designs are available in the literature, but all of them have one point in common: the smaller the frequency, the larger is the cell. The same applies to the 16 unit cell designs proposed by Memoli et al. [35]: rectangular cuboids, \u223c 4.3 \u00d7 4.3 \u00d7 8.6 mm in size, designed to have maximum transmission (\u223c97% of the input sound) at f 0 \u00b1 \u2206f 2dB = 40 \u00b1 1 kHz. A simple way to use these designs at a different frequency f is to scale each cuboid until its thickness is equal to the new wavelength \u03bb = c 0 / f (where c 0 \u223c 343 m/s is the speed of sound in air). At the new frequency, each of the 16 scaled unit cells will encode the same phase delay (between 0 and 2\u03c0 ) and have the same transmission and bandwidth they had at f 0 . With this method, Jackowski-Ashley et al. [19] manipulated the emission of a parametric speaker operating at 64 kHz.\nThe cell design in [35], however, offers a possibility not fully explored by the authors: a cuboid designed for f 0 also has the same transmission at other frequencies (see Figure 2b). These frequencies will be given by:\nf j = f 0 \u2212 j \u2022 c 0 /L e f f(2)\nwhere j = 0, 1, 2 \u2022 \u2022 \u2022 N are integers, L e f f is a design parameter of the specific unit cell 4 and N = round(L e f f /\u03bb 0 ) is the (integer) number of times L e f f contains the wavelength.\nIt is therefore possible to operate the unit cells at one of the frequencies f j (see Figure 2c), maintaining a similar transmission to the one in f 0 , but considering that the phase encoded at f j is different from the one at f 0 : a look-up table is necessary. In particular, our simulations show that the maximum phase these sub-resonant structures can achieve is 2\u03c0 /2 j .\nStep 4: Lens fabrication\nIn this work, we operate at f 0 = 5, 600 Hz (i.e. a frequency close to F8, with a wavelength of \u223c6 cm). We selected this frequency due to restrictions in our manufacturing capabilities (i.e. it was the largest size that could be made on our 3D-printer) but, since everything can be scaled, this choice does not limit the conclusions. We use two type of lenses:\nType A lenses. Obtained by scaling the unit cells from Memoli et al. [35], so that their first resonance (j = 0) is 5.6 kHz and the thickness is equivalent to \u03bb 0 (i.e. 60 mm). Each of this lenses is made by a 8\u00d78 array of unit cells and is 240\u00d7240\u00d760 mm in size (see Figure 1a, left). Their bandwidth is the same as in [35] i.e. 2 \u2022 \u2206f 2dB \u223c 0.05 \u2022 f 0 or 2 piano keys. 4 Related to the length of the maze-like structure, L e f f sets by how much the sound is delayed going through the cell [31] Type B lenses. Obtained by scaling the unit cells from Memoli et al. [35], so that their second resonance -i.e. f 2 in equation ( 2) -is equal to 5,600 Hz. Each of this lenses is made by 10\u00d710 array of unit cells and is 104 \u00d7 104 mm in size and 20.8 mm (i.e. \u223c \u03bb 0 /3) in thickness (see Figure 1a, right).\nType 2 lenses also have a larger bandwidth: our COMSOL simulations (see e.g. Figure 2b) show that the width of the peak with j = 2 is larger than the one for j = 0: by definition, 2 \u2022 \u2206f 2dB \u223c 0.28 \u2022 f 0 i.e. 5 piano keys. The major disadvantage with Type B lenses is that, since the 16 cell designs now only span a limited part of the phase space, only a limited number of focal lenghts can be realised with a lens of a fixed size. Simulations in Figure 3 show, for instance, that in the case of a 10 \u00d7 10 Type B lens with radial phase profile from equation (1), the maximum focal length is 57 mm. To achieve larger focal lengths, it is necessary either to extend the lens (e.g. to 12 \u00d7 12 unit cells) or to use the techniques in section 5.", "n_publication_ref": 14, "n_figure_ref": 6}, {"heading": "BASIC TOOLS FOR DEVICE DESIGN", "text": "In this section, we find that the basic design tools available in optics are also valid in acoustics, when metamaterial lenses are involved. This discovery simplifies the realisation of metamaterial based devices (section 5) and leads to solving some of the limitations of the metamaterials described so far.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "The thin lens approximation", "text": "Optical devices typically require a lens to be placed between the source and the receiver, until the former is imaged on the latter. For most applications, mutual distances are calculated Focal lenghts are reported as a distance from the center of the lens. For each value of the curvature, COMSOL simulations were used to predict the resulting field with an input plane wave at normal incidence. The graph also reports the measured focal lenghts of the two Type B lenses we realised for this study (see below).\nusing the thin-lens equation [4]:\n1/p + 1/q = 1/f (3)\nwhere f is the focal length of the lens, p is the distance between the source and the lens, q the distance between the lens and the image of the source. Equation (3) is based on the same hypotheses used to design the metasurface and should directly apply when the thickness is much smaller than the wavelength [4], but its validity has not been tested before.\nTo do this, we run the following experiment: (a) we design a lens ans simulate (COMSOL) its focal length (Figure 3); (b) we mount the set-up in Figure 4a measure the acoustic sound pressure (SPL) in 1/3rd octaves at different distances from a speaker emitting a tone at 5.6 kHz, without the lens; (c) we repeat the measurements, after inserting the lens in the acoustic path, at a distance p from the speaker; (d) we look for the \"image\" of the speaker, defined as the position where the intensity changes the most due to the lens and record the distance as q ; (e) we plot 1/q vs. 1/p (see Figure 4b) and fit with a line of angular coefficient -1 from equation (3) to find the focal length f . As shown in Figure 4a (inset), the selected type B lens was held in a mask during the tests, at different distances from the speaker. Measurements at large distances (Figure 4a) were conducted in a non-reverberant environment outdoors, using a Norsonic 121 class I sound-level meter and a \u00bc\" microphone (Norsonic, Nor-1225). More detailed 2D scans of the area in front of metasurfaces (120 \u00d7 300 mm) were taken using a modified 3D-printer, a 1/8\" microphone (B&K, model 4138) and an amplifier (B&K, Nexus conditioning amplifier) directly connected to a Picoscope and then to the PC. The 5.6 kHz signal originated from a .wav file prepared in-house (using Audacity) and was amplified using a car stereo amplifier.\nFigure 4b reports the results for two acoustic metasurfaces, designed to have different focal lengths (A 1 = 0.44 mm \u22121 for f 1 = 53 mm and A 2 = 0.75 mm \u22121 for f 2 = 38 mm). We found a very good agreement between design and experiments for f sim =53 mm, but 16% difference for the smaller focal length. In the optical case, the accuracy decreases when the focal length gets smaller, as lenses cannot be considered \"thin\": we will therefore limit this study to f > 53mm.\nHaving verified the thin-lens equation leads naturally to designing systems of two lenses, like telescopes and microscopes. The focal length F of such a system is given by [4]:\n1/F = 1/f 1 + 1/f 2 \u2212 D/(f 1 \u2022 f 2 )(4)\nwhere f 1 and f 2 are the focal lengths of the two lenses and D is the distance between the two lenses. This equation has been largely exploited to design zoom objectives and, by the HCI community, in designing the first models of VR headsets (see references in [53]). Once F is known, equation (3) can be applied, in the form 1/P + 1/Q = 1/F where all the distances (i.e. P, Q and F ) are measured from two known reference planes, called \"principal planes\" [4].\nTo test the validity of equation ( 4) in the acoustic case, using metasurfaces, we simulated a system of two lenses (both with f = 53 mm) using finite elements (COMSOL Multiphysics) and adjusted the mutual distance D, recording the position of the image relative to the last lens. Figure 5 shows that this quantity, known as the \"Back Focal Length\", increases with the distance between the two lenses 5 as predicted from equation ( 4). Experimentally, we realised the two lenses and positioned the first at 50 mm from the speaker. We then changed D, noting the position of the maximum intensity. Good agreement was found with equation (4).", "n_publication_ref": 5, "n_figure_ref": 7}, {"heading": "DEVICE REALISATION", "text": "Here, we exploit the capabilities acquired in the previous sections to prototype three lens-based acoustic devices. For these, we describe conceptual application scenarios, to be evaluated in future works through user studies.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acoustic collimator", "text": "The possibility of designing an acoustic lens with a selected focal length (and the validity of equation ( 3)) allowed us to build a collimator: a system that corrects the geometric divergence of a source, so that the output sound is spatially contained in a directional beam (Figure 1b). Collimators are  in optics after the lamp in a slide projector (i.e. to make rays parallel) or in lighthouses (i.e. to project the light over larger distances). They are also used in theatres for producing spotlights. In these applications, a converging lens is positioned at a distance from the source equal to its focal length, transforming the impinging wave into a parallel beam.\nour realisation, we used a Type A lens (f 150 mm) positioned at 150\u00b12 mm from the speaker. Figure 6a shows that, while the decrease with distance typical of a spherical source is maintained (i.e. -6 dB when doubling the distance), the acoustic pressure measured at different distances is consistently larger than the one in absence of the lens. The angular emission, sampled at 4.24 m (Figure 6b), shows that the angle of divergence of the speaker (defined as the width 10 dB below the peak) was reduced from \u00b1 1\u00b0to 27 \u00b1 1\u00b0.\nThe metamaterial lens modified the emission of our lowcost speaker, making it comparable with the top-of-the-range audio spotlight by Holosonics , which has an emission anof \u223c 30\u00b0 [44,48]. In our tests outdoors (Figure 1b), we also found that the sound, which could only be heard up to 10 m away without the lens, was perceived up to 40 m away when the collimator was present (test conducted with passer-bys, a local green area). Theoretical predictions based on tion (3) that the divergence angle could be further reduced by finer adjustments to the mutual position of the and the lens (e.g. provided by an automated positioning system). As detailed in section 2, the HCI community has used expensive audio spotlights for many applications: our method hints to the same applications, in a more costeffective way. In addition, as shown by Norasikin et al. [37], metamaterials can be used to give non-conventional shapes to sound, and this opens to additional usage scenarios, like:\nPersonalised experiences in shared spaces. include sending specific sounds only to parts of an audience (see Figure 6c); having different acoustic cues depending on the location in a space (e.g. with large real-walking virtual reality experiences confined to limited tracking volumes [32], a few metamaterial corrected speakers may be able to map whole virtual mansions); creating different sound zones in cars or\n(a) (b) (c) (d)\nFigure 6: Effect of a collimator on the emission from a computer speaker, when a lens is positioned at the focal length from it: (a) intensity at different distances and (b) angular emission at 4.24m from the speaker. Also reported are two conceptual drawing detailing the use of a collimator (c) in an auditorium and (d) to correct the emission of two speakers mounted back to back, to cover holes in their emission.\non a sofa that passengers and driver can listen to different sounds); sending sound behind corners.\nIncrease the spatial performance of sound systems. concerts, or in cinemas, speakers are arranged to minimize the changes in level and tonality, but there is always a minority of the audience who does not have an optimal acoustic experience. Figure 6d reports the example of two mounted in a symmetrical coupled point source arrangement, highlighting the gap in front of them: a dedicated speaker, made directional by a collimating lens, may be used to fill such gaps. Similar considerations can be applied to smart speakers, like Google Home or Amazon Echo, whose is due to an array of speakers.\nModify the spatial sensitivity of acoustic sensors. can also be used in detection, transforming generic acoustic sensors into highly directional ones.", "n_publication_ref": 4, "n_figure_ref": 7}, {"heading": "Acoustic magnifying glass", "text": "The art of glass blowing introduced the concept of magnifying glasses in the 13th century: application where the between the observer and the sample to be observed fixed, and the user positions the lens in between until the image is \"in focus\". In our realisation (Figure 7a), we pothe speaker and the microphone along a line and inserted a lens in between, adjusting its position until the signal on the microphone was maximised.\nAccording to equation for a lens of fixed focal length there is only one optimal distance from the source where this happens. In this configuration, the lateral extension of the image (a.k.a. the magnification also depends on the of p: real images with |M | > 1 are formed when < p < 2f , while |M | < 1 when p > 2f (i.e. the magnification on the position of the lens: the farther is the lens, smaller the image). The possibility of creating the real image of an acoustic and to vary its size according to the position of a lens (between the source and the listener) allows:\nModifying the apparent position of the source. possibility be to create the image of a speaker in front of the user him/her seating on a sofa, with the original speaker where the television is) and thus the feeling that the sound is coming from a localised source (see Figure 7b). This effect has some similarity to spatial sound (1D, so far), and should evaluated by user studies in comparison to that.\nthe range of haptic devices. The HCI community is familiar with using ultrasonic transducer arrays for midair haptics [6] and levitation [33], even behind objects [37].\neffects, however, lose in definition as the distance from the source increases. Using an appropriate lens may help moving these effects at larger distances, so that the source may be located far from the end-user (see Figure 7c).\nSimilar methods have been used in holographic trapping [42] and is no reason why they should not be applicable to 3D shapes made of sound.\nAugmenting reception. A lens may be used on the receiver to modify the spatial performance a microphone. As shown in Figure 7d, using the lens to image certain areas instead of others help detecting alarming noises from certain parts of a machine (e.g. the cog that tends to break all the time, or the hissing sound of a gas leak) or selected areas of the house (e.g. a burglar breaking a window instead of a dog barking in the background). Similar arrangements may make personal assistants (e.g. Amazon Echo) sensitive only to orders issued in certain areas of the house, with advantages over background noise.\nThe change of magnification with distance may be an when the source is very small or very far from the In addition, sources located at the periphery of the lens may appear distorted, when imaged. A solution to these two issues, often used in optics (e.g. machine vision), is offered by telecentric systems [4]: an additional entrance pupil, positioned at the focal length of the lens, allows to select only the central rays and maintain the image size with object displacement, provided stays within the \"depth of field\". In the acoustic case, a telecentric system would allow user to listen to two different sources (e.g. two audience members in an auditorium) with even if they are located at different distances, without changing the In a conference, it may reduce the need to fetch around the microphone during questions time.", "n_publication_ref": 4, "n_figure_ref": 4}, {"heading": "Acoustic telescope", "text": "The introduction of the refracting telescope, in the 16th century, allowed scientists and to monitor objects at much larger distances. Unique telescope designs have been developed over the centuries, in the effort of increasing the field of view and magnification, but all are based on the of two lenses at mutual distance D. our realisation (see Figure 1c), we demonstrated a Keplerian telescope (1611) for sound, based on two convergent lenses positioned at a (variable) distance (see Figure 8). The lenses, that we picked of the same focal length for sim-(f = 53 mm), were mounted on a rail and their mutual distance could be adjusted (with 1 mm precision) using an Arduino Nano a stepper motor.\nTelescopes solve the limitation of Type B lenses: the focal lengths that cannot be achieved with one lens will be achieved with two, at an appropriate distance D. Applications then include those of magifying glasses, but without of deploying the lens in the field. A telescope, in fact, is a vari-focal lens: the distance can be changed to create a lens of the desired focal length. One user scenario would be listening to a source among others (e.g. zooming a single person in a crowd -see Figure 8b -to either deliver or receive acoustic messages). Other applications of our telescope include acoustic displays (e.g. dancers following an acoustic spot in a disco like a cat would follow a laser pointer) or music with a dynamic spatial component.\nModern cameras, however, all feature auto-zooming objectives. A similar solution for sound would be capable of following a source in the field of view and, in VR, the same speaker could be used to deliver location-specific sounds to multiple users moving in the same virtual world, without headphones. As a first step towards an auto-zoom lens (see supplemental video), we positioned a receiving microphone at an unknown position in front of the speaker and adjusted the distance between the lenses (with 1 mm precision) using an Arduino Nano and a stepper motor until the signal on it was maximised (i.e. using the reading of the sound-level meter as feedback for the positioning system). We found that, when the speaker was imaged on the microphone, the meter 7 dB more than the level it would have measured in the same position without metamaterials. User studies an automatic feedback loop will be needed to decide whether this increase is sufficient, especially in otherwise environments.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "DISCUSSION", "text": "the previous sections we have described many conceptual of converging metasurfaces, highlighting how our prototype devices be used to advance their realisation. Here we summarise the advantages of our approach, and describe how remaining limitations can be overcome. mentioned earlier (section 2), metamaterihave a limited bandwidth (5 piano keys for our Type B lenses). While this may be already sufficient for delivering alarms and conveying personal audio messages, the human audible range cover 11 octaves: larger bandwidths are highly desirable for consumer audio. This is a hot research topic: more and more solutions are appearing on the horizon.\nthis work, we have shown how using unit cells at subresonance frequencies extends the bandwidth. This observation hints to a \"rule-of-thumb\", connecting the transmission of single unit cells and their bandwidth, suggesting that larger bandwidths may be obtained already at the single cell level by accepting a certain amount of transmission loss . With visual LCD displays only requiring a 20% transmission, may be a minor issue for some applications. possibility of using smaller Type B cells, however, can be exploited also by designing multi-frequency structures occcupy the same space as a Type A cell (e.g. a unit cell made of Type B cells, mounted in a \u00d7 2 array, like the RGB crystals forming pixels in a LCD display). This is the route followed by Jim\u00e9nez et al. who have reached a high absorption over a broad frequency band in deepsubwavelength thickness panels by stacking side-by-side unit cells of different size, with close frequency response.\nFinally, the problem of a limited bandwidth can be tackat the device level. Achromatic lenses, for instance, are realised by stacking two different lenses (in the direction of propagation), so that two wavelengths (typically red & blue) focus on the same plane. Having proven that the thin-lens equation also applies to acoustics, when metamaterials are similar solutions may be imagined for sound . Comparison with speaker arrays. Acoustic metamaterials which shape sound are not a competing, but a complementing approach compared with traditional sound emitters [37]. Metamaterials are smaller, cheaper and easier to manufacture than phased arrays: they can even be recyclable materials. Metamaterial devices lead to less aberrations than speaker arrays, even over limited bandwidths. Metamaterials solutions, however, are static. systems [37], mechanically actuated metamaterial devices (like our zooming lens or [19]) and fully active meta- [30] will be the future, but require further study. Our varifocal lens, for instance, could be integrated with a motion tracking system (e.g. Leapmotion T M or RGB-D camera) to keep the object is \"in focus\" while it moves. The limitation due to 1D mechanical actuation will be easily exceeded thanks to the integration of multiple actuators: not only on the second lens that makes up the telescope system, but also on the speaker-first lens component. of metasurfaces. As shown by Memoli et al. [35] and reinforced in this work, acoustic metasurfaces may be layered into unique devices to obtain a detailed shaping of the sound, with each layer adding complexity and functionalities to the end-user experience. This work is only a first step into multi-layered acoustic metamaterials, that needs to be addressed in future studies.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Scalability and Compactness.", "text": "Scalability is an important isas the size of a metasurface may vary from small to very large. While we have demonstrated the possibility of making compact, sub-wavelength thick metasurfaces, the challenge meeting user-defined space constraints is still open. Due to their compactness metamaterial structures could be incorporated into speakers or headphones for example, paving the way for multiple applications.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CONCLUSIONS", "text": "Designing experiences based on sound delivery is challenging: not only sound is invisible, so taht \"holes\" in the delivery be easily spotted, but it is difficult to have an \"eagle view\" i.e. the perception of how the sound is \"felt\" at all points of the space at same time. While metamaterials cannot shape sound on demand yet, our approach adds simple, but powerful tools to the ones available to acoustic designers, presenting them in a way that builds on centuries of visual in the HCI community. Next steps include on the technical side more complex shapes (e.g. Zhu et al. [66]), larger and on-demand sound control (i.e. a \"space sound modulator\"). In terms design, user studies (in collaboration with musicians and psychologists) will be needed to determine the potential effects of personal sound on audiences.\nOur prototypes, while simple, lower the access threshold to designing novel sound experiences: devices based on acoustic metamaterials will lead to new ways of delivering, and even thinking of sound.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "GM, LC and JPE acknowledge funding from the Engineerand Physical Sciences Research Council (EPSRC-UKRI) through grant EP/S001832/1. SS acknowledges funding from the Royal Academy of Engineeringhrough their Chair in Emerging Technology grant CiET1718/14. MC acknowledges funding from the Royal Academy of Engineeringhrough grant The authors would also like to thank the anonymous referees for the useful comments.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "3D Sound for Human-computer Interaction: Regions with Different Limitations Elevation Localization", "journal": "ACM", "year": "2009", "authors": "Kenneth John Barreto; Malek Faller;  Adjouadi"}, {"title": "Acoustic control by wave field synthesis", "journal": "The Journal of the Acoustical of America", "year": "1993", "authors": "A Berkhout; D De Vries; P Vogel"}, {"title": "Virtual Reality by Sound Reproduction Based on Wave Field Synthesis", "journal": "", "year": "1996", "authors": "M Marinus; Edwin N Boone; G Verheijen;  Jansen"}, {"title": "Principles of Optics", "journal": "Pergamon Press", "year": "1970", "authors": "K Born; W Wolf"}, {"title": "", "journal": "Project Telepathy", "year": "2018-08", "authors": "Anne-Claire Bourland; Peter Gorman; Jess Mcintosh; Asier Marzo"}, {"title": "UltraHaptics: Mid-air Haptic Feedback for Touch Surfaces", "journal": "ACM", "year": "2013", "authors": "Tom Carter; Sue Ann Seah; Benjamin Long; Bruce Drinkwater; Sriram Subramanian"}, {"title": "Feedforward control of sound transmission using an active acoustic meta-Smart Materials Structures 26", "journal": "", "year": "2017", "authors": "Jordan Cheer; Cameron Daley"}, {"title": "Enhanced sensing and conof ultrasonic Rayleigh waves by elastic metasurfaces", "journal": "", "year": "2017", "authors": "Victoria Colombi; Richard J Ageeva; Adam Smith; Rikesh Clare; Matt Patel;  Clark; Philippe Colquitt; Sebastien Roux; Richard V Guenneau;  Craster"}, {"title": "", "journal": "Reports", "year": "2017", "authors": ""}, {"title": "A seismic metamaterial: The resonant metawedge", "journal": "Scientific Reports", "year": "2016", "authors": "Andrea Colombi; Daniel Colquitt; Philippe Roux; Sebastien Guenneau; Richard V Craster"}, {"title": "Independent trapping and manipulation microparticles using dexterous acoustic tweezers", "journal": "Applied Physics Letters", "year": "2014", "authors": "R P Courtney; Christine E M Demore; Hongxiao Wu; Alon Grinenko; Paul D Wilcox; Sandy Cochran; Bruce Drinkwater"}, {"title": "Controlling sound with acoustic metamaterials", "journal": "Nature Reviews Materials", "year": "2016", "authors": "A Steven; Johan Cummer; Andrea Christensen;  Al\u00f9"}, {"title": "The Making of Pearl, a 360&Deg; Google Spotlight Story", "journal": "ACM", "year": "", "authors": "Cassidy Curtis; David Eisenmann; Rachid El Guerrab; Scot Stafford"}, {"title": "A review of acoustic array in air", "journal": "Applied Acoustics", "year": "2012", "authors": "Woon-Seng Gan; Jun Yang; Tomoo Kamakura"}, {"title": "Metasurfaces: From microwaves to visible", "journal": "Physics Reports", "year": "2016", "authors": "B Glybovski; Sergei A Tretyakov; Pavel A Belov; Yuri S ; Constantin R Simovski"}, {"title": "Au-dioGPS: Spatial Audio Navigation with Minimal Attention Interface", "journal": "Personal Ubiquitous Comput", "year": "2002-01", "authors": "Simon Holland; David R Morse; Henrik Gedenryd"}, {"title": "Metamaterial Mechanisms", "journal": "ACM", "year": "2016", "authors": "Alexandra Ion; Johannes Frohnhofen; Ludwig Wall; Robert Kovacs; Jack Alistar; Pedro Lindsay; Hsiang-Ting Lopes; Patrick Chen;  Baudisch"}, {"title": "Digital Mechanical Metamaterials. In of the 2017 CHI Conference on Human Factors in Computing Systems (CHI '17)", "journal": "ACM", "year": "2017", "authors": "Alexandra Ion; Ludwig Wall; Robert Kovacs; Patrick Baudisch"}, {"title": "Coded Programmable Deformation Behaviour for Shape Changing", "journal": "", "year": "2016", "authors": "Miyu Iwafune; Taisuke Ohshima; Yoichi Ochiai"}, {"title": "Article 1, 2 pages", "journal": "", "year": "", "authors": ""}, {"title": "Haptics and Directional Audio Using Acoustic Metasurfaces", "journal": "ACM", "year": "2017", "authors": "Louis Jackowski-Ashley; Gianluca Memoli; Mihai Caleap; Nicolas Slack; Bruce W Drinkwater; Sriram Subramanian"}, {"title": "Rainbow-trapping absorbers: Broadband, perfect and asymmetric sound absorption by subwavelength panels for problems", "journal": "Scientific Reports", "year": "2017", "authors": "Vicent Jim\u00e9nez; Vincent Romero-Garc\u00eda; Jean-Philippe Pagneux;  Groby"}, {"title": "Subjective Effect of Synthesis Conditions in 3D Sound Field System Using a Few Transducers and Wave Field Synthesis", "journal": "ACM", "year": "2009", "authors": "Toshiyuki Kimura; Munenori Naoe; Yoko Yamakata; Michiaki Katsumoto"}, {"title": "", "journal": "Local Control of Audio Environment: A Review of Methods and Applications. Technologies", "year": "2014", "authors": "Jussi Kuutti; Juhana Leiwo; Raimo E Sepponen"}, {"title": "Field conjugate lenses for ultrasound hyperthermia", "journal": "Transactions on", "year": "1993", "authors": "R J Lalonde; A Worthington; J W Hunt"}, {"title": "", "journal": "Ferroelectrics, and Frequency Control", "year": "1993", "authors": ""}, {"title": "The inspection of anisotropic single-crystal components using a 2-ultrasonic array", "journal": "Transactions on Ultrasonics, Ferroelectrics, and Frequency Control", "year": "2010-12", "authors": "J L Lane; A K Dunhill; B W Drinkwater; P D Wilcox"}, {"title": "", "journal": "", "year": "", "authors": "Fabrice Lemoult; Nad\u00e8ge Kaina; Mathias Fink; Geoffroy Lerosey"}, {"title": "", "journal": "Soda Cans Metamaterial: A Subwavelength-Scaled Phononic Crystal. Crystals", "year": "", "authors": ""}, {"title": "Acoustic Voxels: Computational Optimization of Modular Acoustic Filters", "journal": "ACM Trans. Graph", "year": "2016-07", "authors": "David I W Li;  Levin; Changxi Matusik;  Zheng"}, {"title": "Experimental Realization of Full Control of Reflected Waves with Subwavelength Acoustic Metasurfaces", "journal": "Phys. Rev. Applied", "year": "2014-12", "authors": "Yong Li; Xue Jiang; Bin Liang; Xin-Ye Zou; Jian-Chun Lei-Lei Yin;  Cheng"}, {"title": "Reflected wavefront manipulation based on ultrathin planar metasurfaces", "journal": "Scientific Reports", "year": "2013", "authors": "Yong Li; Bin Liang; Zhong-Ming Gu; Xin-Ye Zou; Jian-Chun Cheng"}, {"title": "Acoustic metamaterials with broadband and wide-angle impedance matching", "journal": "Phys. Rev. Materials", "year": "2018-04", "authors": "Chenkai Liu; Jie Luo; Yun Lai"}, {"title": "Shaping reverberating sound fields with an actively tunable meta", "journal": "Proceedings of the National Academy of Sciences", "year": "2018", "authors": "Guancong Ma; Xiying Fan; Ping Sheng; Mathias Fink"}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "Acoustic metamaterials: From local resonances to broad horizons", "journal": "Science Advances", "year": "2016", "authors": "Guancong Ma; Ping Sheng"}, {"title": "Scenograph: Fitting Real-Walking VR Experiences into Various Tracking Volumes", "journal": "ACM", "year": "2018", "authors": "Sebastian Marwecki; Patrick Baudisch"}, {"title": "Holographic acoustic elements for manipulation of levitated objects", "journal": "Communications", "year": "2015", "authors": "Asier Marzo; Sue Ann Seah; Bruce W Drinkwater;  Deepak Ranjan; Benjamin Sahoo; Sriram Long;  Subramanian"}, {"title": "Controllable transmission and total reflection through an impedance-matched acoustic metasurface", "journal": "Journal of Physics", "year": "2014", "authors": "Jun Mei; Ying Wu"}, {"title": "Metamaterial bricks and quantization of meta-surfaces", "journal": "Nature Communications", "year": "2017", "authors": "Gianluca Memoli; Mihai Caleap; Michihiro Asakawa; R Deepak; W Sahoo; Sriram Drinkwater;  Subramanian"}, {"title": "Design and Evaluation of Two Dimensional Phased Array Ultrasonic Transducers", "journal": "AIP Conference Proceedings", "year": "2005", "authors": "C Mondal; P D Wilcox; B W Drinkwater"}, {"title": "Yutaka Tokuda, and Sriram Subramanian", "journal": "", "year": "2018", "authors": "Diego Martinez Mohd Adili Norasikin; Spyros Plasencia; Gianluca Polychronopoulos;  Memoli"}, {"title": "Marcel van Brakel, and Frederik Duerinck", "journal": "ACM", "year": "2017", "authors": "Marianna Obrist; Grace Boyle"}, {"title": "Pixie Dust: Graphics Generated by Levitated and Animated Objects in Computational Acoustic-potential Field", "journal": "ACM", "year": "2014", "authors": "Yoichi Ochiai; Takayuki Hoshi; Jun Rekimoto"}, {"title": "Holographic Whisper: Rendering Audible Sound Spots in Three-dimensional Space by Focusing Ultrasonic Waves", "journal": "ACM", "year": "2017", "authors": "Yoichi Ochiai; Takayuki Hoshi; Ippei Suzuki"}, {"title": "A three-dimensional actuated origamiinspired transformable metamaterial with multiple degrees of free", "journal": "Nature Communications", "year": "2016", "authors": "N Y York;  B Usa ; T;  Overvelde; A Twan; Yanina De Jong;  Shevchenko; - A Ser; George M Becerra; James C Whitesides; Chuck Weaver; Katia Hoberman;  Bertoldi"}, {"title": "Step-by-step guide to the realization of advanced optical tweezers", "journal": "Opt. Soc. Am. B", "year": "2015-05", "authors": "Giorgio Pesce;  Volpe; M Onofrio; Philip H Marag\u00f3; Sylvain Jones; Antonio Gigan; Giovanni Sasso;  Volpe"}, {"title": "Focused ultrasound as a non-invasive intervention for neurological disease: a review", "journal": "Journal of Neurosurgery", "year": "2016", "authors": "Rory J Piper; Mark A Hughes; Carmel M Moran; Jothy Kandasamy"}, {"title": "Akustische Vermessung parametrischer Lautsprecherarrays im Kontext der Transauraltechnik", "journal": "", "year": "2014", "authors": "F Pokorny; F Graf"}, {"title": "The Use of Airborne Ultrasonics for Generating Audible Sound Beams", "journal": "", "year": "1998", "authors": ""}, {"title": "Active acoustic metamaterials reconfigurable in real time", "journal": "Phys. Rev", "year": "2015-06", "authors": "Durvesh Shinde; Adam Konneker; Steven A Cummer"}, {"title": "Season Traveller: Multisensory Narration for Enhancing the Virtual Reality Experience", "journal": "ACM", "year": "2018", "authors": "Nimesha Ranasinghe; Pravar Jain; Nguyen Thi Ngoc Tram; Koon Raymond Koh; David Tolley; Shienny Karwita; Lin Lien-Ya; Yan Liangkun; Kala Shamaiah;  Chow Eason Wai; Ching Tung; Ellen Yi-Luen Chiuan Yen;  Do"}, {"title": "Short overview in parametric loudspeakers array technology and its implications in spatialization in electronic music. International Computer Music Conference", "journal": "Michigan Publishing", "year": "2016", "authors": "Jaime Reis"}, {"title": "Spatial Audio (Music Technology)", "journal": "", "year": "2013", "authors": "Francis Rumsey"}, {"title": "3D Sound Interactive Environments for Problem Solving", "journal": "Assets", "year": "2005", "authors": "Jaime S\u00e1nchez; Mauricio S\u00e1enz"}, {"title": "", "journal": "ACM", "year": "", "authors": ""}, {"title": "Overview of geometrical room acoustic modeling techniques. The of the", "journal": "America", "year": "2015", "authors": "U Savioja;  Peter Svensson"}, {"title": "Broadband fractal acoustic metamaterials for low-frequency attenuation", "journal": "Applied Physics Letters", "year": "2016-06", "authors": "Yong Gang; Qiang Song; Bei Cheng; Hui Yuan Huang; Tie Dong"}, {"title": "A review of adjustable lenses for head mounted displays", "journal": "", "year": "2017", "authors": "E Stevens; N L Thomas; Ilinca S Jacoby; Daniel P Aricescu;  Rhodes"}, {"title": "GpsTunes: Controlling Navigation via Audio Feedback", "journal": "ACM", "year": "2005", "authors": "Roderick Steven Parisa Eslambolchilar; Stephen Murray-Smith; Sile O' Hughes;  Modhrain"}, {"title": "Presentation of Directional Information by Sound Field Control", "journal": "ACM", "year": "2012", "authors": "Yutaka Takase; Shoichi Hasegawa"}, {"title": "Anomalous refraction of airborne sound through metasurfaces", "journal": "Scientific Reports", "year": "2014", "authors": "Chunyin Tang; Manzhu Qiu; Jiuyang Ke; Yangtao Lu; Zhengyou Ye;  Liu"}, {"title": "Beam steering with pulsed transducer arrays", "journal": "IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control", "year": "1991-07", "authors": "H Turnbull; F S Foster"}, {"title": "Influence of phased array elesize on beam steering behavior", "journal": "", "year": "1998", "authors": "Chang Shi; Yijun Wooh;  Shi"}, {"title": "", "journal": "", "year": "", "authors": ""}, {"title": "Wavefront modulation and subwavelength diffractive acoustics with an acoustic metasurface", "journal": "Nature Communications", "year": "2014", "authors": "Wenqi Wang; Huanyang Chen; Adam Konneker; Bogdan-Ioan Popa; Steven A Cummer"}, {"title": "Acoustic beamforming of a parametric speaker ultrasonic transducers", "journal": "Sensors and Actuators A: Physical", "year": "2005", "authors": "Jun Yang; Woon-Seng Gan; Khim-Sia Tan; Meng-Hwa Er"}, {"title": "GRIN lens and lens array with diffusion-driven photopolymer", "journal": "Lett", "year": "2008-11", "authors": "Chunfang Ye; Robert R Mcleod"}, {"title": "The audio spotlight: An application of nonlinear interaction of sound waves to a new type of loudspeaker design", "journal": "Journal of the Acoustical Society of America", "year": "1983", "authors": "Masahide Yoneyama; Jun-Ichiroh Fujimoto; Yu Kawamo; Shoichi Sasabe"}, {"title": "Using Spatialized Audio to Improve Human Spatial Knowledge Acquisition in Virtual Reality", "journal": "ACM", "year": "2018", "authors": "Seraphina Yong; Hao-Chuan Wang"}, {"title": "Light Propagation with Phase Discontinuities: Generalized Laws of Reflection and Refraction", "journal": "", "year": "2011", "authors": "Nanfang Yu; Patrice Genevet; Mikhail A Kats; Francesco Aieta; Federico Jean-Tetienne; Zeno Capasso;  Gaburro"}, {"title": "Focal therapy using resonance image-guided focused ultrasound in patients with localized prostate cancer", "journal": "", "year": "2016", "authors": "An Yuh; Robert Liu; Alexander Beatty; Jeffrey Y C Jung;  Wong"}, {"title": "Implementation of dispersion-free slow acoustic wave propagation and phase engineering with helicalmetamaterials", "journal": "Nature Communications", "year": "2016", "authors": "Xuefeng Zhu; Kun Li; Peng Zhang; Jie Zhu; Jintao Zhang; Chao Tian; Shengchun Liu"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Key moments of the design process used in this work: (a) comparison of two different phase profiles, with A 1 < A 2 , highlighting how the unit cells encode the phase distribution \u03c6(x, y) (each color corresponds to a different phase); (b) COMSOL simulation for the transmission of unit cell #15 from [35], scaled so that its base is 10.4 mm, highlighting the resonances described by equation (2); (c) working principle of a Type B unit cell, highlighting the delay the input wave experiences in its presence (COMSOL simulation at 5.6 kHz, base size 10.4 mm); .", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: Simulated focal lenghts for a 10 \u00d7 10 unit cells, 104 mm Type B lens used at 5.6 kHz (j = 2), as a function of the lens curvature A. Focal lenghts are reported as a distance from the center of the lens. For each value of the curvature, COMSOL simulations were used to predict the resulting field with an input plane wave at normal incidence. The graph also reports the measured focal lenghts of the two Type B lenses we realised for this study (see below).", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Measurements for testing the thin-lens equation: (a) outdoor set-up for positioning the microphone at fixed distances from the loudspeaker; (b) results for two different lenses,comparing f sim (directly obtained from COMSOL simulations) with f mis (obtained by fitting measurements with (3)). Reporting the results in terms of 1/p and 1/q allows to obtain the focal length from a linear fit (from equation (3)).", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Results for the configuration: depenof the back focal length on the mustual distance between the lenses (COMSOL simulations for two Type B (f = 53 mm).", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Applications of a single metamaterial lens: (a) realisation of the device, using a speaker; (b) an acoustic lens creating acoustic objects for end-users; (c) an acoustic lens used to extend range of mid-air haptics; (d) an acoustic lens to increase sensitivity to certain parts of a machinery; (e) two equally-sized screws separated by 100 and observed two different types of optical lenses.", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Acoustic telescope using two converging lenses of the same focal length: (a) side view and (b) a potential connecting acoustically with a single person in a crowd. Front view Figure 1.", "figure_data": ""}], "doi": "10.1145/3290605.3300713"}