{
  "abstractText": {
    "page": 0,
    "region": {
      "x1": 53.36800003051758,
      "x2": 295.5574645996094,
      "y1": 409.2030029296875,
      "y2": 572.68701171875
    },
    "text": "ABSTRACT We propose a method that generates a virtual camera layout of a 3D animation scene by following the cinematic intention of a reference video. From a reference video, cinematic features such as the start frame, end frame, framing, camera movement, and the visual features of the subjects are extracted automatically. The extracted information is used to generate the virtual camera layout, which resembles the camera layout of the reference video. Our method handles stylized as well as human characters with body proportions diferent from those of humans. We demonstrate the efectiveness of our approach with various reference videos and 3D animation scenes. The user evaluation results show that the generated layouts are comparable to layouts created by the artist, allowing us to assert that our method can provide efective assistance to both novice and professional users when positioning a virtual camera."
  },
  "figures": [{
    "caption": "Table 2: User study results from subjective rating of framed layouts generated by three diferent methods. For each method, 56 shots were rated by 15 participants on scale of 1 (low) to 5 (high). Total of 840 ratings were made per method. % positive refers to percentage of layouts that were rated above 3 (neutral).",
    "captionBoundary": {
      "x1": 317.38116455078125,
      "x2": 559.7340087890625,
      "y1": 173.80203247070312,
      "y2": 234.60333251953125
    },
    "figType": "Table",
    "imageText": ["Artist", "27", "64", "104", "292", "353", "77%", "Ours", "37", "57", "134", "295", "317", "73%", "Toric", "110", "164", "169", "227", "170", "47%", "1", "2", "3", "4", "5", "%", "positive"],
    "name": "2",
    "page": 6,
    "regionBoundary": {
      "x1": 343.0,
      "x2": 533.0,
      "y1": 250.0,
      "y2": 298.0
    }
  }, {
    "caption": "Figure 2: Camera layout components: (a) six diferent framing types, and (b) 13 diferent camera movements.",
    "captionBoundary": {
      "x1": 317.95489501953125,
      "x2": 559.8056030273438,
      "y1": 302.0875549316406,
      "y2": 319.0609436035156
    },
    "figType": "Figure",
    "imageText": ["(b)", "Camera", "movement", "(a)", "Framing"],
    "name": "2",
    "page": 2,
    "regionBoundary": {
      "x1": 318.0,
      "x2": 559.0,
      "y1": 139.0,
      "y2": 285.49273681640625
    }
  }, {
    "caption": "Figure 6: Results from our method for shots from Back to the Future using 3D scenes from Galvane et al. [12]. Here, (a) shows the frst frames of the shots from the reference video, and (b) shows the panning movement following Goldie. The frames in the frst rows of (a) and (b) are from Back to the Future (R. Zemeckis, 1985) ©Universal Pictures.",
    "captionBoundary": {
      "x1": 53.788970947265625,
      "x2": 558.1939086914062,
      "y1": 167.71255493164062,
      "y2": 195.6429443359375
    },
    "figType": "Figure",
    "imageText": ["(a)", "Static", "shots", "(b)", "A", "shot", "with", "the", "camera", "panning", "left"],
    "name": "6",
    "page": 7,
    "regionBoundary": {
      "x1": 57.0,
      "x2": 555.0,
      "y1": 83.0,
      "y2": 151.114013671875
    }
  }, {
    "caption": "Figure 7: Generated virtual camera layouts from the Counseling dataset. Here, (a) shows reference shots, (b) and (c) show generated camera layouts using our method with similar as well as diferent body proportions to/from those of humans, and (d) shows results from the Toric space method. Characters and background assets, respectively, in (c) and (d): ©Mix and Jam, ©Jeremy Vikery and Alex Mateo.",
    "captionBoundary": {
      "x1": 53.233306884765625,
      "x2": 559.296875,
      "y1": 598.8825073242188,
      "y2": 637.7698364257812
    },
    "figType": "Figure",
    "imageText": ["(d)", "Toric", "space", "method", "with", "stylized", "characters", "(c)", "Our", "method", "with", "stylized", "characters", "(b)", "Our", "method", "with", "human", "characters", "(a)", "Reference", "video"],
    "name": "7",
    "page": 7,
    "regionBoundary": {
      "x1": 104.0,
      "x2": 508.0,
      "y1": 210.0,
      "y2": 582.2839965820312
    }
  }, {
    "caption": "Table 1: Correspondence for framing. Each framing type is determined based on the corresponding keypoints. The camera framing in a virtual scene is generated using the corresponding skeletal parts.",
    "captionBoundary": {
      "x1": 317.6590881347656,
      "x2": 559.8057250976562,
      "y1": 293.04052734375,
      "y2": 331.92779541015625
    },
    "figType": "Table",
    "imageText": ["Framing", "Keypoints", "Skeletal", "Parts", "CU", "Eye", "Head", "MCU", "Shoulder", "Spine", "MS", "Hip", "MLS", "Knee", "Leg", "FS", "Ankle", "Toe", "LS"],
    "name": "1",
    "page": 3,
    "regionBoundary": {
      "x1": 355.0,
      "x2": 519.0,
      "y1": 347.0,
      "y2": 429.0
    }
  }, {
    "caption": "Figure 3: The proposed framework initially divides the input reference video into a sequence of shots using a shot boundary detection method. The shots are then analyzed to extract the camera layout information, whose results are stored in a shot list. Using the shot list, 3D assets, and matched characters, the initial virtual camera layout is generated in the 3D scene, which matches the semantics of the camera layout of the reference shot. The user can fne-tune the virtual camera position further if desired. The reference video on the top left are from Back to the Future (R. Zemeckis, 1985) ©Universal Pictures.",
    "captionBoundary": {
      "x1": 53.56488037109375,
      "x2": 295.64874267578125,
      "y1": 348.2755432128906,
      "y2": 463.86138916015625
    },
    "figType": "Figure",
    "imageText": [],
    "name": "3",
    "page": 3,
    "regionBoundary": {
      "x1": 54.0,
      "x2": 293.0,
      "y1": 208.0,
      "y2": 335.0
    }
  }, {
    "caption": "Figure 8: Comparison with the artist’s layout [14] of two shots from Back to the Future. The reference frames are from Back to the Future (R. Zemeckis, 1985) ©Universal Pictures.",
    "captionBoundary": {
      "x1": 53.79795837402344,
      "x2": 295.00311279296875,
      "y1": 179.18557739257812,
      "y2": 218.07696533203125
    },
    "figType": "Figure",
    "imageText": [],
    "name": "8",
    "page": 8,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 295.0,
      "y1": 83.0,
      "y2": 166.0
    }
  }, {
    "caption": "Figure 12: Average rating scores for layouts generated for each character pair.",
    "captionBoundary": {
      "x1": 317.9549865722656,
      "x2": 558.3890380859375,
      "y1": 369.9985656738281,
      "y2": 386.9719543457031
    },
    "figType": "Figure",
    "imageText": [],
    "name": "12",
    "page": 8,
    "regionBoundary": {
      "x1": 359.0,
      "x2": 517.0,
      "y1": 229.0,
      "y2": 357.0
    }
  }, {
    "caption": "Table 3: User preference results for layouts generated by three diferent methods: Artist, our method, and the Toric space method. Each cell of table shows % of row preferred over column.",
    "captionBoundary": {
      "x1": 317.6590881347656,
      "x2": 558.4517822265625,
      "y1": 404.2412414550781,
      "y2": 443.1285095214844
    },
    "figType": "Table",
    "imageText": ["row", ">", "column", "Artist", "Ours", "Toric", "Artist", "-", "53%", "74%", "Ours", "47%", "-", "74%", "Toric", "26%", "26%", "-"],
    "name": "3",
    "page": 8,
    "regionBoundary": {
      "x1": 363.0,
      "x2": 513.0,
      "y1": 458.0,
      "y2": 507.0
    }
  }, {
    "caption": "Figure 11: Average rating scores for layouts generated by each method.",
    "captionBoundary": {
      "x1": 317.9549865722656,
      "x2": 558.4517822265625,
      "y1": 207.92355346679688,
      "y2": 224.89697265625
    },
    "figType": "Figure",
    "imageText": [],
    "name": "11",
    "page": 8,
    "regionBoundary": {
      "x1": 366.0,
      "x2": 511.0,
      "y1": 83.0,
      "y2": 195.0
    }
  }, {
    "caption": "Figure 9: Ablation study for optimization terms. EV , EH , and EM are the visibility, the headroom, and the horizontal arrangement term, respectively. Yellow hoodie character by ©Mario Nagamura.",
    "captionBoundary": {
      "x1": 53.225128173828125,
      "x2": 295.649658203125,
      "y1": 315.5925598144531,
      "y2": 354.48187255859375
    },
    "figType": "Figure",
    "imageText": [],
    "name": "9",
    "page": 8,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 295.0,
      "y1": 234.0,
      "y2": 302.0
    }
  }, {
    "caption": "Figure 10: Using virtual character pairs from (a), we compare virtual camera layout created by artist, our method, and the Toric space method refecting the reference shot (b). Both reference subjects are in MS. Characters in Pair A and the left character in Pair C: ©Karim Kashef, ©AnimSchool, ©Mario Nagamura. Character pairs in Pair B and Pair D: ©Mix and Jam, ©KYOWON",
    "captionBoundary": {
      "x1": 53.24208068847656,
      "x2": 295.6487121582031,
      "y1": 594.4945068359375,
      "y2": 666.2526245117188
    },
    "figType": "Figure",
    "imageText": ["(b)", "Virtual", "camera", "layouts", "produced", "by", "diferent", "methods.", "(a)", "Four", "diferent", "pairs", "of", "target", "characters", "used", "in", "user", "study."],
    "name": "10",
    "page": 8,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 295.0,
      "y1": 371.0,
      "y2": 577.89599609375
    }
  }, {
    "caption": "Figure 4: The on-screen visibility of body parts can difer depending on the character’s unique body proportions, even if the on-screen position and the on-screen ratio of the head are identical. A Character on the right column ©KYOWON.",
    "captionBoundary": {
      "x1": 317.9549865722656,
      "x2": 558.3890991210938,
      "y1": 249.86557006835938,
      "y2": 288.7528991699219
    },
    "figType": "Figure",
    "imageText": ["(b)", "On-screen", "visibility", "of", "body", "parts", "(a)", "On-screen", "position", "and", "ratio", "of", "the", "head"],
    "name": "4",
    "page": 4,
    "regionBoundary": {
      "x1": 340.0,
      "x2": 535.0,
      "y1": 83.0,
      "y2": 233.26702880859375
    }
  }, {
    "caption": "Figure 5: On-screen visibility refers to the vertical space occupied by the character on the screen, whereas headroom refers to the vertical space between the top of the character’s head and the upper boundary of the screen. The horizontal on-screen midpoint is the u coordinate of the center of the two characters’ on-screen positions, pA(uA,vA) and pB (uB ,vB ).",
    "captionBoundary": {
      "x1": 317.51068115234375,
      "x2": 559.8058471679688,
      "y1": 578.551513671875,
      "y2": 651.906982421875
    },
    "figType": "Figure",
    "imageText": [],
    "name": "5",
    "page": 4,
    "regionBoundary": {
      "x1": 317.0,
      "x2": 559.0,
      "y1": 429.0,
      "y2": 565.0
    }
  }],
  "sections": [{
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 53.79800033569336,
        "x2": 186.69361877441406,
        "y1": 593.0138549804688,
        "y2": 599.2319946289062
      },
      "text": "∗Both authors contributed equally to the paper"
    }, {
      "page": 0,
      "region": {
        "x1": 53.31680679321289,
        "x2": 294.8095703125,
        "y1": 621.4981079101562,
        "y2": 707.3472290039062
      },
      "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. CHI ’21, May 8–13, 2021, Yokohama, Japan © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445437"
    }, {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.8067016601562,
        "y1": 409.2030029296875,
        "y2": 441.1789245605469
      },
      "text": "CCS CONCEPTS • Computing methodologies → Computer graphics; Graphics systems and interfaces; Computer vision tasks."
    }, {
      "page": 0,
      "region": {
        "x1": 317.10198974609375,
        "x2": 559.5502319335938,
        "y1": 455.3059997558594,
        "y2": 540.4630126953125
      },
      "text": "KEYWORDS Virtual Camera, Content Analysis, Cinematography ACM Reference Format: Jung Eun Yoo, Kwanggyoon Seo, Sanghun Park, Jaedong Kim, Dawon Lee, and Junyong Noh. 2021. Virtual Camera Layout Generation using a Reference Video. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3411764.3445437"
    }]
  }, {
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 317.9549560546875,
        "x2": 559.715087890625,
        "y1": 569.9859619140625,
        "y2": 674.4689331054688
      },
      "text": "Camera layout is an important component of cinematography to deliver the emotion and suspense of a scene. The director creates a guideline, known as a shot list, to communicate with a 3D animation layout artist. The layout artist uses the shot list as a reference to position the virtual camera in order to best deliver the cinematic intention of the director. However, for most novice users, it is difcult to express intentions precisely through a virtual camera due to its high degree of freedom (DOF). For professional artists, it is time-consuming to position numerous virtual cameras repeatedly, as is common during the production of a TV series."
    }, {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2058715820312,
        "y1": 679.574951171875,
        "y2": 707.3469848632812
      },
      "text": "Automatic methods have been proposed to analyze the camera layout of a reference video and to generate a virtual camera layout for a given scene. These methods typically use computer vision"
    }, {
      "page": 1,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.0308532714844,
        "y1": 87.79393005371094,
        "y2": 159.4019775390625
      },
      "text": "and machine learning techniques to classify types of framing [4, 7, 33] and camera movement [5, 10, 17]. However, the framing and camera movement types they can handle are limited to only a small subset of various types, meaning that their practical use is highly limited. Furthermore, previous studies mostly focus on designing a specifc language model to generate the virtual camera layout [3, 11, 26, 31, 35], which thwarts its use by novice users."
    }, {
      "page": 1,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5602722167969,
        "y1": 164.50596618652344,
        "y2": 301.8680114746094
      },
      "text": "To ease the virtual camera layout process, we designed our method considering the common practices performed in TV or animation studios, where a reference video or images are heavily used in the early stages of the production process or for previz purposes. Our method works with 3D scenes for which the staging and animation are similar to those of the reference video. Specifcally, we analyze the intention of a reference video and create the virtual camera layout of a given scene using a shot list as a communication medium to transfer the cinematic intention. Because reference videos such as live-action movie clips have well-established cinematography, imitating the camera layout in these cases will help novice users as well as experts in this process to generate a satisfying initial virtual camera layout without much efort."
    }, {
      "page": 1,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.42572021484375,
        "y1": 306.971923828125,
        "y2": 477.2099914550781
      },
      "text": "The workfow of the proposed method consists of the following three steps. First, the reference video is segmented into a sequence of shots. Second, we analyze each shot to extract information related to the camera layout, which includes the types of framing and camera movements, and the visual features of the subjects. Next, we match the subject to the character in the 3D scene. All of this information is stored in the shot list and is then used to generate the virtual camera layout. Third, based on this information, the virtual camera is positioned and animated with respect to the characters in the 3D scene. A stylized or exaggerated character with a diferent body proportion from that of a human is common in 3D animation. To handle these characters when applying the intended framing type, we utilize the skeletal information of the target character. Once the camera is positioned based on the framing type, we create a smooth camera path between the start and end frames to follow the camera movement type."
    }, {
      "page": 1,
      "region": {
        "x1": 53.796905517578125,
        "x2": 295.56365966796875,
        "y1": 482.3139343261719,
        "y2": 619.676025390625
      },
      "text": "We demonstrate the efectiveness of our approach by visually comparing the generated virtual camera layout with that of the reference video. We also conducted a user study to evaluate the quality of our framing results on exaggerated characters by comparing them with the results from the previous method [25] and from the artist. We successfully confrmed that the results with the proposed method were preferred over those from the previous method and that the quality was comparable to that of the artist’s layout. We conducted an additional user study of the efectiveness of our method. This study confrmed that both amateurs and professional layout artists spent much less time when replicating a reference video using our method as compared to relying on the conventional manual process."
    }, {
      "page": 1,
      "region": {
        "x1": 53.36800003051758,
        "x2": 295.5574951171875,
        "y1": 624.7799682617188,
        "y2": 707.3469848632812
      },
      "text": "In summary, our method can automatically generate a virtual camera layout for a 3D animation scene. The generated virtual camera position efectively assists the user in achieving the fnal layout result that refects the cinematic intention of the reference video. We also propose a customized solution for stylized or exaggerated characters that are commonly used in 3D animation. Our method is able to reproduce the intended semantics of the camera layout from the reference video for characters with signifcantly diferent"
    }, {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2006225585938,
        "y1": 87.79393005371094,
        "y2": 104.60797119140625
      },
      "text": "body proportions from those of a human using optimization based on skeletal parts and the framing type."
    }, {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7156982421875,
        "y1": 109.71195983886719,
        "y2": 126.5260009765625
      },
      "text": "The main contributions of the paper can be summarized as follows:"
    }, {
      "page": 1,
      "region": {
        "x1": 328.8606262207031,
        "x2": 559.7149658203125,
        "y1": 138.5309295654297,
        "y2": 243.0159912109375
      },
      "text": "(1) A system that analyzes a reference video to generate a virtual camera layout for a 3D animation and that is capable of adapting to stylized characters with non-realistic body proportions. (2) A means to analyze and extract framing and camera movements from a reference video. (3) A novel optimization scheme that positions a virtual camera for both stylized as well as human characters. (4) A camera layout generation better than or comparable to the Toric space method and the artist’s camera layout."
    }],
    "title": {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 420.68597412109375,
        "y1": 554.8219604492188,
        "y2": 562.1419677734375
      },
      "text": "1 INTRODUCTION"
    }
  }, {
    "paragraphs": [],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 421.7441711425781,
        "y1": 268.947021484375,
        "y2": 276.26702880859375
      },
      "text": "2 RELATED WORK"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.71533203125,
        "y1": 299.8009338378906,
        "y2": 360.45098876953125
      },
      "text": "To construct a shot list of a reference video, the following information is needed: the start frame, end frame, framing type, and the camera movement type. In this section, we focus on framing and camera movement. Regarding the extraction of the start and end frames of a shot from a video, please refer to the survey by Smeaton et al. [34]."
    }, {
      "page": 1,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7188720703125,
        "y1": 365.554931640625,
        "y2": 579.6290283203125
      },
      "text": "Framing is determined by the distance from the camera to the subject. Conventional methods [4, 7] use hand-crafted features, while a more recent method [33] uses a convolutional neural network (CNN) to classify framing into various types. While these methods have reported some success in classifying the framing of input images, they are limited to three types of framing. Camera movement can be analyzed using a sequence of frames in each shot. Most of the previous methods do not include the movement type [5, 10, 17], which expresses the direction of the motion. Although Derue et al. [10] is an exception, their method uses only two consecutive frames to predict the camera movement type. Recently, SGNet [29] used a CNN to analyze the camera framing and movement in a given shot jointly. This method classifes framing into fve types (extreme close-up,close-up, medium shot,full shot, and long shot) and movements into four types (static, motion, push, and pull). In this work, we use six diferent scales for framing and 13 diferent types of camera movement to represent translational, rotational, or curved motions with the direction. This faithfully refects the basic camera movement semantics, which are widely used in practice (Fig. 2)."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 463.8423767089844,
        "y1": 284.6343078613281,
        "y2": 291.9543151855469
      },
      "text": "2.1 Camera Layout Analysis"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7225952148438,
        "y1": 613.8209838867188,
        "y2": 707.3469848632812
      },
      "text": "Reproducing a camera layout using a shot specifcation has been studied in the felds of virtual cinematography. One popular approach is to provide a shot specifcation through a high-level camera composition language that describes established flming styles and techniques. These languages, often delivered in the form of text, describe how the shot is composed and what the setup constraints are for virtual camera placement [11, 23, 26, 31, 35]. This method is direct and clear yet requires a certain degree of cinematic knowledge. Shot specifcations can be given through images as well. Bares"
    }, {
      "page": 2,
      "region": {
        "x1": 53.569366455078125,
        "x2": 295.5594177246094,
        "y1": 87.79393005371094,
        "y2": 159.4019775390625
      },
      "text": "et al. [3] designed an interface for a user to visualize camera compositions through storyboard frames. This allows the user intuitively to design a shot composition, and the fnished storyboard frame is later translated into an established language for virtual camera placement. In this work, we analyze the layout information and visual features of the subjects from the reference video and use them as a medium for a specifcation."
    }, {
      "page": 2,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5554504394531,
        "y1": 164.50596618652344,
        "y2": 411.4570007324219
      },
      "text": "Various techniques have been proposed to compute the camera position and motion for both drone and virtual cinematography [9] . Studies of drone cinematography investigate drone control and path planning during aerial shots [2, 13, 15, 16, 19–21, 27]. Regarding the positioning of a virtual camera, Bares et al. [3] proposed a constraints-based approach that heuristically searches for possible camera parameter values that satisfy the constraints. Ranon and Urli [28] proposed an optimization approach that fnds the viewpoint of a camera that maximizes the objective function. Lino and Christie [24, 25] proposed a novel camera viewpoint representation, known as the Toric space, which reduces the conventional 7 DOF search space to 4 DOF. Constraints such as the size and on-screen position of the character’s head and the camera distance can be algebraically expressed in the Toric space, enabling intuitive placement as well as control of the camera by the user. Utilizing the Toric space, Wu et al. [35] calculated the position and orientation of a virtual camera based on the visual features delivered through their proposed flm language. Galvane et al. [12] attempted to generate a smooth path and movement of the camera using a character motion and userdefned framing for the start and end frames. Burg et al. [6] proposed a method that generates real-time occlusion-aware camera motion by maximizing the visibility of a target subject throughout a motion path."
    }, {
      "page": 2,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5574951171875,
        "y1": 416.5609130859375,
        "y2": 532.0050048828125
      },
      "text": "Recently, Jiang et al. [22] employed an example-driven method to control a virtual camera automatically and produce various stylistic variations of a 3D animation. Similar to Jiang et al. [22], our system takes the analyzed layout information and visual features from a reference video as input. While Jiang et al. [22] focus on generating continuous camera motions only for human characters, we focus on reproducing the framing of a scene, which works with various targets including stylized characters with non-realistic body proportions matching those common in computer animations. Furthermore, with our method shot-level editing can be achieved easily given that the virtual camera is generated for each shot."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 464.40966796875,
        "y1": 598.6580200195312,
        "y2": 605.97802734375
      },
      "text": "2.2 Virtual Cinematography"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 53.79798889160156,
        "x2": 295.0302429199219,
        "y1": 569.9859619140625,
        "y2": 707.3469848632812
      },
      "text": "In this paper, we focus on two main components of the camera layout (Fig. 2): framing and camera movement. For a complete list of camera layout components, please refer to the book by Arijon [1]. Framing is the artistic style of placing a subject in the shot. It can be classifed into diferent types based on how much of the subject is included on-screen. While framing can be defned for any subject, we assume that the subject in the reference video is human, as observed in most live-action movies. Framing can be categorized into six diferent types based on the representative body parts of a human subject. A close-up (CU) includes the subject’s face area only. A medium close-up (MCU) includes both the subject’s face and shoulders. A medium shot (MS) includes the waist and upper parts of the subject’s body. A medium long shot (MLS) includes the"
    }, {
      "page": 2,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.1912231445312,
        "y1": 87.79393005371094,
        "y2": 126.5260009765625
      },
      "text": "knees and upper parts of the body. A full shot (FS) flls the frame with the entire body of the subject, from the head to the feet. A long shot (LS) positions the camera farther away than a FS does, with the subject occupying only part of the frame."
    }, {
      "page": 2,
      "region": {
        "x1": 317.5249938964844,
        "x2": 559.7158203125,
        "y1": 335.9319152832031,
        "y2": 473.29400634765625
      },
      "text": "Camera movement creates dynamics in a shot. While numerous flm techniques related to camera movement exist, we chose seven basic camera movements, 13 in total considering the direction. Static refers to a stationary camera. Tilt and pan have rotational motion about the x-axis and y-axis of the camera coordinate frame, respectively. Crane has rotational motion about the x-axis of the camera coordinate frame and translational motion along the y-axis of the world coordinate frame. Orbit has rotational motion about the horizontal axis of the world coordinate frame, resulting in an arc motion. Track and dolly have translational motion along the x-axis and the z-axis of the camera coordinate frame, respectively. With directional information included, the camera movement types are complete enough to generate virtual camera movement."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 53.79800033569336,
        "x2": 220.89268493652344,
        "y1": 554.8219604492188,
        "y2": 562.1419677734375
      },
      "text": "3 A CAMERA LAYOUT PRIMER"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.5827026367188,
        "y1": 515.1909790039062,
        "y2": 707.3469848632812
      },
      "text": "Our method generates a virtual camera layout by following the cinematic intention of the reference video. Given an input reference video with an arbitrary length, frst we detect the shot boundaries using the method of Zhang and Wang [36]. The method is trained with the TRECVID dataset [34] for the labeling of the start and end frames of the shots in the video. It is important to segment the video into a sequence of shots because every single shot has a diferent camera layout, resulting in a unique visual style and emotional tone. Using the detected shot boundaries, we enter the start and end frame numbers of each shot on the shot list. Next, in the Camera Layout Analysis (CLA) stage (Section 4.1), we use a shot from the previous stage as input and utilize computer vision techniques to extract the subjects’ visual features (head orientation, area, and position), the framing, and the camera movement. Additionally, by comparing the reference video and the staged 3D assets, we manually associate the subjects with the characters. All of this information above is stored as a shot list which will be used to generate a virtual camera. In the Virtual Camera Layout Generation (VCLG) stage (Section"
    }, {
      "page": 3,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.4273986816406,
        "y1": 87.79393005371094,
        "y2": 192.27899169921875
      },
      "text": "4.2), we calculate the virtual camera position in the Toric space [25] and further optimize it to satisfy the visual constraints of the reference shot. Finally, the camera motion is generated based on the information from the shot list. The user can also fne-tune the camera position, as is done during the 3D animation work fow using 3D software. An overview of our system is shown in Fig. 3. In this work, we focus on identifying the framing and camera movement of the reference video to construct a shot list and then to generate a virtual camera layout using the shot list. Therefore, we will discuss the CLA and VCLG in detail in the following sections."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 554.5733642578125,
        "y1": 487.07598876953125,
        "y2": 507.3450927734375
      },
      "text": "4 VIRTUAL CAMERA LAYOUT GENERATION FRAMEWORK"
    }
  }, {
    "paragraphs": [{
      "page": 3,
      "region": {
        "x1": 53.36800003051758,
        "x2": 294.04644775390625,
        "y1": 508.9739074707031,
        "y2": 536.7459716796875
      },
      "text": "With the detected shot boundaries for a given video, we separate the video into shots and analyze each shot to extract the camera layout."
    }, {
      "page": 3,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5581970214844,
        "y1": 548.0679321289062,
        "y2": 707.3469848632812
      },
      "text": "4.1.1 Framing. When positioning the camera, there are a few important factors to consider, specifcally the screen position and framing type of the subject. The framing type of the subject can be determined using the visibility of the estimated human keypoints or the skeletal parts of the subject, as described in Section 3. The screen position of the subject can be extracted using an of-theshelf 2D human pose estimation method, LCR-Net [30]. However, in some cases, LCR-Net fails to detect or inaccurately estimate the keypoints. In addition, classifying the framing type when two or more subjects are present in a scene can lead to ambiguous results. In this case, we select as the main subject one who is front-facing for the analysis or classify the subject based on its detected region by LCR-Net [30]. We employ and fne-tune a pretrained CNN model, ResNet [18], for framing type classifcation, as was done in Savardi et al. [33]. Because no such dataset for the six framing"
    }, {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.431640625,
        "y1": 87.79393005371094,
        "y2": 137.4840087890625
      },
      "text": "types defned in Section 3 exists, we manually collected and labeled live-action movies. In total, 6180 and 697 frames were labeled for training and testing, respectively. Please refer to the supplementary document for more details. At inference time, the network classifes the framing type given an input frame."
    }, {
      "page": 3,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.1896362304688,
        "y1": 142.5879364013672,
        "y2": 280.90899658203125
      },
      "text": "We also estimate the head size and orientation of the subject, a process which is essential for positioning the virtual camera in the Toric space. The head size is calculated by constructing a bounding circle of the subject’s head. The center coincides with the head position phead , and the radius is computed as r = | |phead −pneck | |2 where pneck is the position of the neck joint. The ratio between the head area and the image resolution, sr atio , is then calculated using the radius r and the height and width of the image. Lastly, from the given head area, we estimate the head orientation of the subject in the camera coordinate using a method devised by Ruiz et al. [32]. The framing information, specifcally the framing type, head position, head area, and the head orientation, are added to the shot list for later use."
    }, {
      "page": 3,
      "region": {
        "x1": 317.6409912109375,
        "x2": 559.7213745117188,
        "y1": 449.43792724609375,
        "y2": 707.3469848632812
      },
      "text": "4.1.2 Camera Movement. Given a shot, we analyze the camera movement by constructing a motion vector and training a feedforward neural network as supervised learning. To construct the motion vector for a shot, we follow the method of Derue et al. [10]. A dense optical fow is initially computed for every consecutive frame of the shot. To improve the accuracy when capturing the camera motion, we mask out dynamic objects, in our case humans, using a semantic segmentation method [8]. Subsequently, we construct a N -bin histogram using the orientation of the optical fow for all frames after optical fows with low magnitudes are discarded. Here, the camera movement direction (i.e. dolly in and out) cannot be diferentiated if only orientation values are used because the values do not convey spatial information. To analyze both spatial and temporal information in the entire shot, we divide the dense optical fows into nine equal sections using the rule of thirds. For each region, a histogram is computed, and all nine sections are concatenated to form a single motion vector M . M is further normalized with respect to the length of the shot, as every single shot difers in terms of the total number of frames. Using M as the input, a three-layer fully connected neural network is trained for camera movement classifcation. The model is trained on a synthetic dataset built using the Unreal Engine. Please refer to the supplementary document for more details. At the inference time, the network inputs M and classifes the camera movement type of the shot. The"
    }, {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.27459716796875,
        "y1": 87.79393005371094,
        "y2": 104.60797119140625
      },
      "text": "inferred camera movement for each shot is stored in the previously generated shot list."
    }],
    "title": {
      "page": 3,
      "region": {
        "x1": 53.79800033569336,
        "x2": 199.68536376953125,
        "y1": 493.80999755859375,
        "y2": 501.1300048828125
      },
      "text": "4.1 Camera Layout Analysis"
    }
  }, {
    "paragraphs": [{
      "page": 4,
      "region": {
        "x1": 53.36800003051758,
        "x2": 295.5574645996094,
        "y1": 136.09092712402344,
        "y2": 262.4940185546875
      },
      "text": "With a shot list consisting of the camera layout and visual features of the reference video, we generate the framing and camera movement of the virtual camera for the 3D animation scene. First, we manually match the subjects from the reference video with the names of the target virtual characters. When there are two subjects in the video, the one with better visual features (i.e., head facing the camera direction) is selected as the main subject. The corresponding matched character is considered as the main target. Using this association, the virtual camera is positioned and keyed. The following explains how the framing and camera movement of the virtual camera are determined based on the information contained in the shot list."
    }, {
      "page": 4,
      "region": {
        "x1": 53.7979736328125,
        "x2": 294.04876708984375,
        "y1": 275.5459289550781,
        "y2": 325.2359924316406
      },
      "text": "4.2.1 Framing. Given a shot list and 3D characters, we calculate the camera position and orientation in the 3D scene. Finding the camera position for a single character is straightforward. With the on-screen ratio sr atio , the distance d from the camera to the character is computed as"
    }, {
      "page": 4,
      "region": {
        "x1": 140.0457763671875,
        "x2": 294.0437316894531,
        "y1": 341.21417236328125,
        "y2": 353.8657531738281
      },
      "text": "W · R d = , (1)"
    }, {
      "page": 4,
      "region": {
        "x1": 53.46699905395508,
        "x2": 294.0487976074219,
        "y1": 351.0591735839844,
        "y2": 455.2929992675781
      },
      "text": "2 tan(ϕ/2) · r p where r = (W · H · sr atio )/π is the on-screen radius, R is the actual radius of the character’s head, and ϕ is the horizontal feld of view of the camera. W and H represent the width and the height of the screen resolution of the camera, respectively. This distance determines a spherical manifold of potential camera positions. With a given head orientation estimated by the CLA method and the head rotation of the virtual character, a point on this manifold is determined as the fnal camera position."
    }, {
      "page": 4,
      "region": {
        "x1": 161.46499633789062,
        "x2": 295.56365966796875,
        "y1": 460.3969421386719,
        "y2": 597.7579956054688
      },
      "text": "When two characters are present, we fnd the camera position in the Toric space [24, 25], where the position of a point in the space is represented by three camera-characterrelated parameters as shown in the left inset fgure: the angle α between the two vectors from the camera to the characters and the horizontal and vertical camera angles around the characters, θ and φ, respectively. α can be calculated using the on-screen positions of the characters and the"
    }, {
      "page": 4,
      "region": {
        "x1": 53.79566955566406,
        "x2": 295.560791015625,
        "y1": 602.8619384765625,
        "y2": 707.3469848632812
      },
      "text": "angle of view of the camera. θ and φ are estimated by matching the head orientation of the selected main target character to that of its corresponding reference subject. Framing stylized characters. While the method described above can handle virtual human characters with normal proportions, characters appearing in a virtual scene can have diferent body proportions from that of humans. As can be seen in the inset fgure on the right, the same on-screen position and ratio can result in diferent on-screen visibility outcomes due to the characters’ diferent body proportions."
    }, {
      "page": 4,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7215576171875,
        "y1": 311.18292236328125,
        "y2": 415.6679992675781
      },
      "text": "To address this issue, we interviewed professional layout artists with more than fve years of experience. The interview helped us to understand how the artist deals with mismatches between the body proportions of the subjects from the storyboard and those of the actual target characters. Three factors that concerned the artists were visibility, the degree to which the subject’s body part is visible; headroom, the vertical space between the subject’s onscreen head-top position and the upper boundary of the screen; and composition, the on-screen placement of the subject which focuses more on the horizontal arrangement."
    }, {
      "page": 4,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2059936523438,
        "y1": 668.615966796875,
        "y2": 707.3469848632812
      },
      "text": "We adopt the factors mentioned by the artists as constraints to layout the stylized characters. Similar to how diferent keypoints are used to classify diferent framing types, diferent skeletal parts of the target are used to calculate the camera-character distance that"
    }, {
      "page": 5,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.424072265625,
        "y1": 87.79393005371094,
        "y2": 290.90899658203125
      },
      "text": "ensures the desired level of visibility. We optimize the normalized on-screen position pA(uA,vA) ∈ [0, 1]2 and pB (uB ,vB ) ∈ [0, 1]2 of target characters A and B to determine α in the Toric representation, such that the resulting layout satisfes the constraints given by the reference subjects A ′ and B ′ specifed on the shot list. First, we modify the defnitions of some of the notations used in Equation 1. The vertical length from the character’s head to a specifc part serves as the actual size R, and the ratio between the vertical pixel length from the on-screen head position to the lower boundary of the screen and the height of the screen serves as the desired on-screen ratio sr atio . In the LS case, which has the same corresponding skeletal parts as a FS but covers a greater area of the scene, we subtract an ofset of 0.3 from the desired on-screen ratio. Thus, the target character is captured in a smaller area of the screen, leaving more room for the background. Given that the on-screen ratio is now a length ratio instead of an area ratio, the equation for the on-screen pixel length r is modifed as r = H · sr atio . Using these modifed defnitions and equations, the distance from the camera to the body feature of the character is recalculated."
    }, {
      "page": 5,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.0283508300781,
        "y1": 296.012939453125,
        "y2": 312.8269958496094
      },
      "text": "In order to follow the framing types specifed on the shot list, we add a visibility term, as follows:"
    }, {
      "page": 5,
      "region": {
        "x1": 122.81314086914062,
        "x2": 294.0489501953125,
        "y1": 323.9976806640625,
        "y2": 332.1650085449219
      },
      "text": "EV = |sA − sA′ | + |sB − sB′ |, (2)"
    }, {
      "page": 5,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.55938720703125,
        "y1": 342.4769287109375,
        "y2": 381.2090148925781
      },
      "text": "where sA and sB denote the on-screen visibility of the target characters in the current layout (Fig. 5), which can be calculated by the function derived from Equation 1. sA′ and sB′ are the desired visibility of the target characters according to the framing type."
    }, {
      "page": 5,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.0458679199219,
        "y1": 386.31292724609375,
        "y2": 403.12701416015625
      },
      "text": "To ensure that the resulting layout has a reasonable headroom size similar to that of the reference, we add a headroom term."
    }, {
      "page": 5,
      "region": {
        "x1": 106.89144897460938,
        "x2": 294.0426330566406,
        "y1": 414.2991638183594,
        "y2": 436.3299865722656
      },
      "text": "h = 1 − v − rhead , EH = | min(hA′ , hB′ ) − min(hA, hB )|. (3)"
    }, {
      "page": 5,
      "region": {
        "x1": 53.21500015258789,
        "x2": 295.5595703125,
        "y1": 446.0826721191406,
        "y2": 573.2120361328125
      },
      "text": "Here, hA, hB ∈ [0, 1] correspondingly denote the headroom areas of characters A and B, respectively, and hA′ , hB′ ∈ [0, 1] are likewise the headroom areas of reference subjects A ′ and B ′, respectively. v ∈ [0, 1] is the normalized vertical coordinate of the on-screen position of a target character whose radius of the on-screen head size is rhead . Note that there may be a case where the height diference between the target characters is not similar to or even opposite from that between the reference subjects. Therefore, instead of optimizing the headroom sizes of both characters, we select a smaller headroom area among each of the target characters and the reference subjects for the calculation. This prevents a taller character’s head from being unwantedly cut-of by the screen boundary."
    }, {
      "page": 5,
      "region": {
        "x1": 53.79646301269531,
        "x2": 294.2115478515625,
        "y1": 578.3159790039062,
        "y2": 638.9660034179688
      },
      "text": "Finally, we add a horizontal arrangement term to ensure that the on-screen horizontal arrangement of the target characters is similar to that of the subjects in the reference. For instance, if the subjects from the reference are projected on the left half of the screen, this term prevents new on-screen positions from moving towards the right half."
    }, {
      "page": 5,
      "region": {
        "x1": 107.51570129394531,
        "x2": 294.0481872558594,
        "y1": 650.1366577148438,
        "y2": 658.3040161132812
      },
      "text": "EM = |(uA + uB )/2 − (uA′ + uB′ )/2|. (4)"
    }, {
      "page": 5,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.4219665527344,
        "y1": 668.615966796875,
        "y2": 707.3469848632812
      },
      "text": "Here, uA and uB are the normalized horizontal coordinates of the on-screen positions of characters A and B, respectively, and uA′ and uB′ are likewise the normalized horizontal coordinates of the on-screen positions of the reference subjects A ′ and B ′, respectively."
    }, {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.19873046875,
        "y1": 87.79393005371094,
        "y2": 106.572021484375
      },
      "text": "The fnal optimization can be expressed as a linear combination of Equations 2 to 4 and the camera roll term Eroll :"
    }, {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.205322265625,
        "y1": 115.39457702636719,
        "y2": 174.989990234375
      },
      "text": "arg min ω1EV + ω2EH + ω3EM + ω4Eroll . (5)pA,pB Here, ω1, ω2, ω3, and ω4 are the weights for each term and are set to 0.7, 1.5, 0.7, and 1.5, respectively. Eroll penalizes the camera’s right axis tilt to prevent camera roll. We used the SLSQP algorithm for optimization."
    }, {
      "page": 5,
      "region": {
        "x1": 317.9549255371094,
        "x2": 559.7144165039062,
        "y1": 186.53294372558594,
        "y2": 345.81298828125
      },
      "text": "4.2.2 Camera Movement. Once the framing is determined, the camera movement is generated by interpolating the camera placements at the start and end keyframes of the shot. However, the initial placement often does not satisfy the constraints of the camera movement rules (i.e., fxed position for panning). Furthermore, a linear interpolation would not be applicable because our camera movements include complex types, such as an orbit that requires arc motion. Thus, we employ a rule-based approach to modify the camera movement. It recalculates the position at the end frame or adds keyframes in the middle so that the camera can correctly follow the conventional rules defned in Fig. 2. The magnitude of the camera movement is estimated according to the change of the camera-character relationship (i.e., the relative position and orientation) at the start and end frames of the shot. Please refer to the supplementary document for more details."
    }],
    "title": {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 250.66360473632812,
        "y1": 120.927001953125,
        "y2": 128.24700927734375
      },
      "text": "4.2 Virtual Camera Layout Generation"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7212524414062,
        "y1": 375.78692626953125,
        "y2": 480.2720031738281
      },
      "text": "Our system was implemented in Python and QT, and the VCLG application was built on top of Autodesk Maya 2018. This application allows the artist to adjust the shot list and the characters’ properties further if necessary. For more information on the application, please refer to the supplementary document. The computation time for the CLA method is 0.368 seconds per frame for a video of resolution 576 × 420 on a machine with an Intel Core i7-5820 processor running at 3.3GHz, 32GB of memory, and a NVIDIA GeForce GTX 980 Ti graphics card. A typical shot lasts around fve seconds. In the VCLG, the computation time is 6.85 seconds per optimization."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 433.9623107910156,
        "y1": 360.62298583984375,
        "y2": 367.9429931640625
      },
      "text": "5 IMPLEMENTATION"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.5827026367188,
        "y1": 510.2459411621094,
        "y2": 581.85498046875
      },
      "text": "To validate our method, we present visual results from our method along with the fndings from an extensive user evaluation. The user study utilized various stylized characters to compare the following: layouts generated using our method, layouts generated using the Toric space method [25], and layouts created by a professional artist. In addition, we conducted a user study of the efectiveness of the entire system with novice users and artists."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 403.77685546875,
        "y1": 495.0820007324219,
        "y2": 502.4020080566406
      },
      "text": "6 EVALUATION"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7207641601562,
        "y1": 624.7799682617188,
        "y2": 707.3469848632812
      },
      "text": "Using our system, we automatically generated virtual camera layouts for two datasets, Back to the Future [12] and Counseling, which consist of a reference video and a 3D scene with characters staged in advance. The Back to the Future video clip is 48 seconds long and has 12 mostly static shots. The length of the Counseling video is 34 seconds and it contains seven shots. The shots include static, track, dolly, orbit, and tilting camera movements. The results are shown in Fig. 6 and Fig. 7 for Back to the Future and Counseling,"
    }, {
      "page": 6,
      "region": {
        "x1": 53.57400131225586,
        "x2": 294.2750244140625,
        "y1": 87.79393005371094,
        "y2": 104.60797119140625
      },
      "text": "respectively. For more results, please refer to the supplementary video."
    }, {
      "page": 6,
      "region": {
        "x1": 53.79798889160156,
        "x2": 294.0472717285156,
        "y1": 109.71195983886719,
        "y2": 312.8269958496094
      },
      "text": "As shown in Fig. 6a, the framing results resemble the reference shots. With the visual features identifed only at the start and end frame along with the classifed camera movement type, the camera movement of the reference shot is successfully reproduced for the 3D scene, as shown in Fig. 6b. We also compared the results with the artist’s layout provided by previous work [14], as shown in Fig. 8. We fnd from the layout by the artist that they focused on the framing and headroom of the character. The Toric space method, which is the initial position of the virtual camera before optimization by our method, fails to deliver the cinematic intention of the reference shot in some cases despite the close resemblance of the target characters to the reference subjects, as shown in the bottom row of Fig. 8. This may stem from a slight diference in the characters’ heights, animation, and/or staging. The method proposed in Jiang et al. [22] is also afected by a similar limitation given that the method is learned from human characters with Toric representation. In contrast, our method was able to handle slight discrepancies between the reference and target scenes through optimization and generate a layout similar to that from the artist."
    }, {
      "page": 6,
      "region": {
        "x1": 53.57400131225586,
        "x2": 295.02996826171875,
        "y1": 317.9309387207031,
        "y2": 411.4570007324219
      },
      "text": "For the Counseling dataset, we show the results with two diferent types of characters. The frst type is human characters (Fig. 7b), and the second type is robot characters with body proportions diferent from those of humans (Fig. 7c). Note that our method works well in both cases, as shown in Fig. 7b and Fig. 7c. In contrast, when only using the Toric space method to position the camera, the virtual camera layout is vastly diferent from the reference video, as shown in Fig. 7d, when the character does not have human-like body proportions."
    }, {
      "page": 6,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.55877685546875,
        "y1": 416.5609130859375,
        "y2": 499.12799072265625
      },
      "text": "Additionally, we investigated how each term in Equation 5 affects the virtual camera layout of exaggerated characters, as shown in Fig. 9. Without EV , the framing type does not match the framing of the reference shot. While the composition and the framing type match without EH , the headroom is vastly diferent from the reference video. EM has an efect on the horizontal composition of the character. Hence, without EM , the positions of the characters can be shifted to the right or left."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 528.5006713867188,
        "y1": 596.6649780273438,
        "y2": 616.93408203125
      },
      "text": "6.1 Qualitative Evaluation for the Virtual Camera Layout"
    }
  }, {
    "paragraphs": [{
      "page": 6,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.56475830078125,
        "y1": 548.0679321289062,
        "y2": 641.593994140625
      },
      "text": "To evaluate the efectiveness of our virtual camera layout, we conducted a user study in which the participants were asked to evaluate and compare the results from the following methods: our method, the Toric space method, and the virtual camera layout created by the artist. We prepared 56 static shots, which were generated from 14 reference shots using 4 diferent pairs of target characters (Fig. 10b). The reference subjects had average human body proportions, whereas the target characters had various exaggerated body proportions, as shown in Fig. 10a."
    }, {
      "page": 6,
      "region": {
        "x1": 53.79643249511719,
        "x2": 294.58050537109375,
        "y1": 646.6979370117188,
        "y2": 707.3469848632812
      },
      "text": "Fifteen participants (6 males and 9 females; ages: 24 to 32; average age: 28.1; three artists with 3 to 5 years of 3D animation experience) flled out a questionnaire consisting of two parts: a subjective rating for user satisfaction and a 2-alternative forced choice (2AFC) test after examining the provided shots. For the subjective rating, the participants were presented with a pair of shots consisting of a"
    }, {
      "page": 6,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7144165039062,
        "y1": 87.79563903808594,
        "y2": 159.4019775390625
      },
      "text": "reference shot and the corresponding shot; these were randomly selected among the three methods. They were asked to score on a 5- Likert Scale how satisfed they were with the framing result. For the 2AFC test, framing results produced by the two diferent methods were presented in random order, along with the corresponding reference shots. The participants were asked to choose the preferred result."
    }, {
      "page": 6,
      "region": {
        "x1": 317.62298583984375,
        "x2": 558.3681030273438,
        "y1": 315.7109375,
        "y2": 507.86700439453125
      },
      "text": "The results in Table 2 verify that the virtual camera layouts generated by our method (73%) were more faithful reproductions of the reference camera layouts than were the virtual camera layouts generated by the Toric space method (47%), and similar in quality to the virtual camera layouts created by the artist (77%). For statistical analysis, we applied the Kruskal-Wallis rank sum test. The results showed that there exist signifcant diferences among the ratings of the layout methods (F (2, 2517) = 126.59, p < 0.05). A post-hoc analysis using the Dunn test with Bonferroni correction revealed that, as shown in Fig. 11, ratings on layouts by the artist and our method were signifcantly diferent from those by the Toric space method (p = 0.00), whereas ratings between these frst two methods were not signifcantly diferent (p = 0.19). Fig. 12 shows average ratings for the layouts created by each method with respect to each character pair. We applied the Kruskal-Wallis test and the Dunn test to all of the sub-data. These tests showed that the Toric space method scored signifcantly lower (p<0.05) than the other two methods for all pairs (p<0.001 for Pair B and D)."
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7176513671875,
        "y1": 512.970947265625,
        "y2": 606.4970092773438
      },
      "text": "The pairwise comparisons reported in Table 3 show that our method was preferred over the Toric space method by 74%. Compared to the layouts created by the artist, those created by our method were preferred by 47%, showing a similar preference. These results verify that our method can generate layouts for a wide range of stylized characters with diferent body proportions while faithfully reproducing the original camera layout of the reference shot. In addition, the layouts generated by our method are comparable to the virtual camera layouts created by the artist."
    }],
    "title": {
      "page": 6,
      "region": {
        "x1": 53.79800033569336,
        "x2": 291.01641845703125,
        "y1": 519.9530029296875,
        "y2": 540.2221069335938
      },
      "text": "6.2 User Study: Comparison of Virtual Camera Layout Results"
    }
  }, {
    "paragraphs": [{
      "page": 6,
      "region": {
        "x1": 317.5249938964844,
        "x2": 558.2036743164062,
        "y1": 635.7389526367188,
        "y2": 707.3397216796875
      },
      "text": "We conducted an additional user study to validate the efectiveness of the entire system. Fifteen participants were recruited (10 males and 5 females; ages: 25 to 46; average age: 32.9) and given the task of positioning and moving the virtual camera according to the reference video. Nine of the participants were novice Maya users with limited experience in 3D animation. The remaining participants were professional artists in a related industry, three"
    }, {
      "page": 7,
      "region": {
        "x1": 53.46699905395508,
        "x2": 558.3681030273438,
        "y1": 660.1149291992188,
        "y2": 687.8880004882812
      },
      "text": "of whom had more than 10 years experience. Three of the artists artists reported that they used Maya on a regular basis. The user were from an animation studio (ASA1, ASA2, ASA3); the remaining study, on average, took approximately 50 minutes. three were from a VFX studio (VA1, VA2, VA3). Four out of the six The participants were asked to replicate the camera layout of"
    }, {
      "page": 7,
      "region": {
        "x1": 317.95831298828125,
        "x2": 558.2067260742188,
        "y1": 692.9899291992188,
        "y2": 698.844970703125
      },
      "text": "a reference video into a Maya scene (i) without initialization of"
    }, {
      "page": 8,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.03057861328125,
        "y1": 690.533935546875,
        "y2": 707.3469848632812
      },
      "text": "the virtual camera, as in the conventional layout process and (ii) with the initialized virtual camera using our method. For this task,"
    }, {
      "page": 8,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7188720703125,
        "y1": 526.1499633789062,
        "y2": 652.5509033203125
      },
      "text": "we used the Counseling dataset and measured the time the participant spent to replicate the reference video. Before starting the experiment, the participants were asked to fll out a demographic questionnaire, followed by a brief explanation of our system. We asked the participants to use the virtual camera to replicate the layout observed in the reference video as closely as possible. The order of replication with or without using the initialized virtual camera layout was randomly selected. During the task, the participants actively referred to the reference video and were allowed to seek help from the proctor if needed. Once the task is completed, the participants were asked to fll out a survey about the usability of the system."
    }, {
      "page": 8,
      "region": {
        "x1": 317.7489929199219,
        "x2": 559.7157592773438,
        "y1": 657.6569213867188,
        "y2": 707.3469848632812
      },
      "text": "Overall, the time performance was greatly improved, as evidenced by the 54.9% decrease in average time spent composing the virtual camera layout (without initialization using our system: 12 min 36 sec; with initialization using our system: 6 min 56 sec). For the novice users, the average time to replicate the reference"
    }, {
      "page": 9,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.4248352050781,
        "y1": 87.79393005371094,
        "y2": 137.4840087890625
      },
      "text": "video decreased by 52.4% (without initialization using our system: 14 min 39 sec; with initialization using our system: 6 min 58 sec). The time for the professional artists to replicate the reference video decreased by 27.9% (without initialization using our system: 9 min 31 sec; with initialization using our system: 6 min 52 sec)."
    }, {
      "page": 9,
      "region": {
        "x1": 53.464080810546875,
        "x2": 295.5652770996094,
        "y1": 149.4789276123047,
        "y2": 253.9639892578125
      },
      "text": "6.3.1 Usability Survey and User Observation. The survey consists of six statements, and the participants rated how much they agreed with each statement on a fve-point Likert scale. This survey checks whether the overall system and our GUI help to alleviate the burden of and lower the barrier of entry when positioning the virtual camera. The average scores of the survey were 3.47 and 4.74 for professional artists and novice users, respectively. These results show that the novice users found our system helpful when generating the camera layout, whereas the professional artists were more neutral regarding its usability."
    }, {
      "page": 9,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5588073730469,
        "y1": 259.0679016113281,
        "y2": 418.3479919433594
      },
      "text": "We attribute this to familiarity diferences with regard to animation tools between artists and novice users. As we observed throughout the experiment, the novice users experienced some difculty when navigating the native interface of Maya, especially when searching for corresponding target characters. Our GUI provides a shot-list-like interface that allows users quickly to locate target characters and navigate shots. Furthermore, the novice users had difculty controlling the camera and were uncertain about their work when replicating the layout. As one novice user commented after replicating the layout manually, \"even if I kept spending more time revising the camera by myself, the results did not improve that much.\" Our system provides the initial layout that best follows the cinematic intention of the reference. Thus, compared to manual work from scratch, the initial layout efectively mitigates the efort needed when replicating the reference."
    }, {
      "page": 9,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5574645996094,
        "y1": 430.3429260253906,
        "y2": 633.4580078125
      },
      "text": "6.3.2 Interview of Professional Artists. The general consensus we found from the artists interview was that the system was useful because it utilizes a shot list to generate an initial virtual camera layout for animation. ASA2 said he would \"defnitely use this system\" if he had to replicate a reference video. Additionally, ASA3 commented that the system would be very useful when an artist has to work on hundreds of shots, which is common during the production of a TV series. The ASA group further unanimously commented that if a story reel can be similarly analyzed to generate a virtual camera layout, it would be very helpful in the animation studio. In addition, VA3 mentioned that our system would be very useful for directors, who have little or no knowledge of 3D animation software, allowing them to previsualize the layout using a reference video. On the other hand, both the VA group and the ASA group felt that generating a camera layout based on classifed framing and camera movements may limit the artistic style of a virtual camera layout. Thus, they would always prefer to retouch the virtual camera’s position and orientation. Nevertheless, the artists were generally optimistic about the direction of our approach."
    }],
    "title": {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 550.7116088867188,
        "y1": 620.5750122070312,
        "y2": 627.89501953125
      },
      "text": "6.3 User Study: Replicating a Reference Video"
    }
  }, {
    "paragraphs": [],
    "title": {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 134.54714965820312,
        "y1": 648.719970703125,
        "y2": 656.0399780273438
      },
      "text": "7 DISCUSSION"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 53.79798889160156,
        "x2": 295.4257507324219,
        "y1": 679.574951171875,
        "y2": 707.3438720703125
      },
      "text": "Our system can be used by both novice and professional users. From the user study and interview, we found that many novice users had difculty placing the virtual camera in a conventional environment."
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.715087890625,
        "y1": 87.79393005371094,
        "y2": 258.031982421875
      },
      "text": "In contrast, with the initial position already generated by the system, the users comfortably began to manipulate the virtual camera and move it to a desired position with a considerable time improvement. This implies that any person with little knowledge about 3D animation tools can rapidly test out diferent virtual camera layouts in the previsualization stage. For professional use, the proposed system can be highly instrumental for TV cartoon animations, during which artists must work on many shots regularly as the series continues. For example, scenes with dialogues consist of multiple shots with short-term intervals. These shots are often similar in terms of their layouts, but as the target character changes after each shot, continuing requires animators manually to locate the frame and place the camera based on the target character one by one. With our system, using the prepared shot list containing the shot information, such repeated work can be expedited, allowing more time for animators to focus on the details of the layout."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 196.9690399169922,
        "y1": 664.4072875976562,
        "y2": 671.727294921875
      },
      "text": "7.1 Use Cases of the System"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 317.5249938964844,
        "x2": 559.7213134765625,
        "y1": 290.533935546875,
        "y2": 548.4429931640625
      },
      "text": "While our method can successfully generate a virtual camera layout using a reference video, it has certain limitations. Since the CLA method relies on the performance of a previous computer vision algorithm [10, 30], the classifcation often fails for videos with over and under exposed frames. In most misclassifed framing task, the framing type difers within a single scale. With regard to camera movement classifcation, most misclassifed results stem from similar camera motions with the same direction. To remedy these instances, we provided a GUI for the user, allowing them easily to correct misclassifed information. We expect the classifcation performance of the camera layouts to improve in the future, since various computer vision algorithms are being actively studied. In addition, our method considers only the relationship between the camera and the characters. Thus, unexpected occlusion or cut-of of background assets by the screen boundary may occur. Consideration of occlusion and staging of assets [6, 26] can be an interesting future work. Lastly, we defned the camera layout in the simplest way and assumed a single type of camera movement. Many live-action movies convey a unique style of camera layout that is sometimes hard to defne as a simple type. While our method focuses more on the framing of stylized characters, incorporating an example-based camera control as in Jiang et al. [22] would allow the analysis and reproduction of a greater variety of camera motions."
    }, {
      "page": 9,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7166137695312,
        "y1": 553.5469360351562,
        "y2": 658.031982421875
      },
      "text": "An interesting direction for future work will be to directly analyze a storyboard or a story reel. Hand drawn story reels are often used in animation studios. Therefore, analyzing such a story-reel to extract camera information can be instrumental in the placement of virtual cameras. We envision that layout artists will greatly beneft from such a system because the process can be directly integrated with the current animation pipeline. We believe that our method can further foster new research ideas for interactive and intuitive camera manipulation techniques for stylized characters, an area that has largely been ignored in the community."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 487.2314758300781,
        "y1": 275.3699951171875,
        "y2": 282.69000244140625
      },
      "text": "7.2 Limitations and Future Work"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.2064819335938,
        "y1": 690.533935546875,
        "y2": 707.3469848632812
      },
      "text": "This paper introduces a method to automatically generate a virtual camera layout in a 3D scene using a reference video. To achieve"
    }, {
      "page": 10,
      "region": {
        "x1": 53.459625244140625,
        "x2": 295.4219665527344,
        "y1": 87.79393005371094,
        "y2": 258.031982421875
      },
      "text": "this task, the method classifes the camera layout and estimates visual features of subjects in the reference video. The resulting information is then stored in the form of a shot list, with which we replicate the camera layout of the shots in the 3D scene with consideration of the virtual character’s framing type. This enables our approach to be applicable to 3D scenes that contain characters with exaggerated as well as human-like proportions. From user studies, we confrmed that the results of our method are comparable to those of virtual camera layouts composed by an artist. In addition, using our system, both professional artists and novice users required less time to replicate the camera layout of a reference video in a 3D scene than was needed to position the virtual camera from scratch. The artists reported that the automatically generated shot list can be instrumental in creating an initial virtual camera layout. Moreover, the system can be useful for novice users or directors who do not have extensive knowledge of 3D animation software."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 405.7296142578125,
        "y1": 675.3699951171875,
        "y2": 682.6900024414062
      },
      "text": "8 CONCLUSION"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 53.36800003051758,
        "x2": 295.0374450683594,
        "y1": 287.04693603515625,
        "y2": 358.6549987792969
      },
      "text": "We thank the anonymous reviewers for their valuable comments; Junghee Kim and his colleagues at MOTIF for the support and helpful discussion about the animation layout; and Haemin Kim, Nicolas Nghiem, and Allen Kim for participating as actors for the reference video. This research is supported by Ministry of Culture, Sports and Tourism and Korea Creative Content Agency (Project Number: R2020040180)."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 171.35446166992188,
        "y1": 271.88299560546875,
        "y2": 279.2030029296875
      },
      "text": "ACKNOWLEDGMENTS"
    }
  }, {
    "paragraphs": [{
      "page": 10,
      "region": {
        "x1": 57.034027099609375,
        "x2": 294.1781311035156,
        "y1": 385.98211669921875,
        "y2": 398.5070495605469
      },
      "text": "[1] Daniel Arijon. 1991. Grammar of the flm language. Silman-James Press. [2] Amirsaman Ashtari, Stefan Stevšić, Tobias Nägeli, Jean-Charles Bazin, and Otmar"
    }, {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.2237854003906,
        "y1": 401.922119140625,
        "y2": 677.4600219726562
      },
      "text": "Hilliges. 2020. Capturing Subjective First-Person View Shots with Drones for Automated Cinematography. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH) 39, 5, Article 159 (Aug. 2020), 14 pages. https://doi.org/10.1145/ 3378673 [3] William Bares, Scott McDermott, Christina Boudreaux, and Somying Thainimit. 2000. Virtual 3D camera composition from frame constraints. In Proceedings of the eighth ACM international conference on Multimedia. ACM, 177–186. [4] Sergio Benini, Luca Canini, and Riccardo Leonardi. 2010. Estimating cinematographic scene depth in movie shots. In 2010 IEEE International Conference on Multimedia and Expo. IEEE, 855–860. [5] Subhabrata Bhattacharya, Ramin Mehran, Rahul Sukthankar, and Mubarak Shah. 2014. Classifcation of cinematographic shots using lie algebra and its application to complex event recognition. IEEE Transactions on Multimedia 16, 3 (2014), 686–696. [6] Ludovic Burg, Christophe Lino, and Marc Christie. 2020. Real-time Anticipation of Occlusion for Automated Camera Control in Toric Space. In Computer Graphics Forum. Wiley Online Library. [7] Luca Canini, Sergio Benini, and Riccardo Leonardi. 2013. Classifying cinematographic shot types. Multimedia tools and applications 62, 1 (2013), 51–73. [8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schrof, and Hartwig Adam. 2018. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In ECCV. [9] Marc Christie, Patrick Olivier, and Jean-Marie Normand. 2008. Camera control in computer graphics. In Computer Graphics Forum, Vol. 27. Wiley Online Library, 2197–2218. [10] François-Xavier Derue, Mohamed Dahmane, Marc Lalonde, and Samuel Foucher. 2017. Exploiting Semantic Segmentation for Robust Camera Motion Classifcation. In International Conference Image Analysis and Recognition. Springer, 173–181. [11] David K Elson and Mark Riedl. 2007. A lightweight intelligent virtual cinematography system for machinima production. (2007). [12] Quentin Galvane, Marc Christie, Chrsitophe Lino, and Rémi Ronfard. 2015. Camera-on-rails: automated computation of constrained camera paths. In Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games. ACM, 151–157. [13] Quentin Galvane, Christophe Lino, Marc Christie, Julien Fleureau, Fabien Servant, Fraņ ois-louis Tariolle, and Philippe Guillotel. 2018. Directing cinematographic"
    }, {
      "page": 10,
      "region": {
        "x1": 317.95098876953125,
        "x2": 559.3807983398438,
        "y1": 89.0950927734375,
        "y2": 101.62005615234375
      },
      "text": "drones. ACM Transactions on Graphics (TOG) 37, 3 (2018), 1–18. [14] Quentin Galvane, Rémi Ronfard, Christophe Lino, and Marc Christie. 2015. Con-"
    }, {
      "page": 10,
      "region": {
        "x1": 317.95098876953125,
        "x2": 559.3841552734375,
        "y1": 105.03509521484375,
        "y2": 681.7769775390625
      },
      "text": "tinuity editing for 3D animation. In Twenty-Ninth AAAI Conference on Artifcial Intelligence. [15] Christoph Gebhardt, Benjamin Hepp, Tobias Naegeli, Stefan Stevsic, and Otmar Hilliges. 2016. Airways: Optimization-based Planning of Quadrotor Trajectories according to High-Level User Goals. In SIGCHI Conference on Human Factors in Computing Systems (San Jose, CA) (CHI ’16). ACM, New York, NY, USA. [16] Christoph Gebhardt, Stefan Stevsic, and Otmar Hilliges. 2018. Optimizing for Aesthetically Pleasing Quadrotor Camera Motion. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH) 37, 4, Article 90 (2018), 11 pages. [17] Muhammad Abul Hasan, Min Xu, Xiangjian He, and Changsheng Xu. 2014. CAMHID: Camera motion histogram descriptor and its application to cinematographic shot classifcation. IEEE Transactions on Circuits and Systems for Video Technology 24, 10 (2014), 1682–1695. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778. [19] Chong Huang, Fei Gao, Jie Pan, Zhenyu Yang, Weihao Qiu, Peng Chen, Xin Yang, Shaojie Shen, and Kwang-Ting Tim Cheng. 2018. Act: An autonomous drone cinematography system for action scenes. In 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 7039–7046. [20] Chong Huang, Chuan-En Lin, Zhenyu Yang, Yan Kong, Peng Chen, Xin Yang, and Kwang-Ting Cheng. 2019. Learning to flm from professional human motion videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4244–4253. [21] Chong Huang, Zhenyu Yang, Yan Kong, Peng Chen, Xin Yang, and KwangTing Tim Cheng. 2019. Learning to Capture a Film-Look Video with a Camera Drone. In 2019 International Conference on Robotics and Automation (ICRA). IEEE, 1871–1877. [22] Hongda Jiang, Bin Wang, Xi Wang, Marc Christie, and Baoquan Chen. 2020. Example-Driven Virtual Cinematography by Learning Camera Behaviors. ACM Trans. Graph. 39, 4, Article 45 (July 2020), 14 pages. https://doi.org/10.1145/ 3386569.3392427 [23] Mackenzie Leake, Abe Davis, Anh Truong, and Maneesh Agrawala. 2017. Computational video editing for dialogue-driven scenes. ACM Trans. Graph. 36, 4 (2017), 130–1. [24] Christophe Lino and Marc Christie. 2012. Efcient composition for virtual camera control. In Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. Eurographics Association, 65–70. [25] Christophe Lino and Marc Christie. 2015. Intuitive and efcient camera control with the toric space. ACM Transactions on Graphics (TOG) 34, 4 (2015), 82. [26] Amaury Louarn, Marc Christie, and Fabrice Lamarche. 2018. Automated staging for virtual cinematography. In Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games. ACM, 4. [27] Tobias Nägeli, Lukas Meier, Alexander Domahidi, Javier Alonso-Mora, and Otmar Hilliges. 2017. Real-time Planning for Automated Multi-View Drone Cinematography. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH). [28] Roberto Ranon and Tommaso Urli. 2014. Improving the efciency of viewpoint composition. IEEE Transactions on Visualization and Computer Graphics 20, 5 (2014), 795–807. [29] Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and Dahua Lin. 2020. A Unifed Framework for Shot Type Classifcation Based on Subject Centric Lens. In The European Conference on Computer Vision (ECCV). [30] Grégory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. 2019. LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019). [31] Remi Ronfard, Vineet Gandhi, and Laurent Boiron. 2015. The prose storyboard language: A tool for annotating and directing movies. (2015). arXiv:1508.07593 [32] Nataniel Ruiz, Eunji Chong, and James M. Rehg. 2018. Fine-Grained Head Pose Estimation Without Keypoints. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. [33] Mattia Savardi, Alberto Signoroni, Pierangelo Migliorati, and Sergio Benini. 2018. Shot Scale Analysis in Movies by Convolutional Neural Networks. In 2018 25th IEEE International Conference on Image Processing (ICIP). IEEE, 2620–2624. [34] Alan F Smeaton, Paul Over, and Wessel Kraaij. 2006. Evaluation campaigns and TRECVid. In Proceedings of the 8th ACM international workshop on Multimedia information retrieval. ACM, 321–330. [35] Hui-Yin Wu, Francesca Palù, Roberto Ranon, and Marc Christie. 2018. Thinking Like a Director: Film Editing Patterns for Virtual Cinematographic Storytelling. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 14, 4 (2018), 81. [36] Chi Zhang and Weiqiang Wang. 2012. A robust and efcient shot boundary detection approach based on fsher criterion. In Proceedings of the 20th ACM international conference on Multimedia. ACM, 701–704."
    }],
    "title": {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 123.21257781982422,
        "y1": 372.5059814453125,
        "y2": 379.82598876953125
      },
      "text": "REFERENCES"
    }
  }]
}