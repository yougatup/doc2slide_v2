
 Hi, I am Kwanggyoon Seo, and I am JungEun Yoo. We are PhD students at Visual Media Lab, KAIST. We are here to present our work “Virtual Camera Layout Generation using a Reference Video”.
 We propose a method that generates a virtual camera layout of a 3D animation scene
 by following the cinematic intention of a reference video. Camera layout, consisting of framing and camera movement, is an important element of cinematography to deliver the emotion and suspense of a scene. Generally in 3D animation pipeline, the director creates a shot list to deliver the cinematic intention.
 Then the layout artist uses the shot list to position the virtual camera.

 While this process can be easy for profession artist, it can be problematic in the case of TV series, where the artist have to work on many shots. Also, for novice users or directors, it is difficult to position virtual camera and express the intentions due to its high degree of freedom.
 Here is our overview. Given an input reference video, we detect the shot boundaries.

 Each shot is then analyze to the camera layout within the predefined set of grammar. The user manually matches the subject and the 3D assets.
 And these information is stored in a form of a shot list.
 We calculate the virtual camera position in the Toric space and further optimize to satisfy the visual constraint of the reference shot. Finally the camera motion is generated based on the information in the shot list. In this work, we focus on the camera layout analysis and virtual camera layout generation.
 For camera layout analysis, we use off-the shelf 2D human pose estimation model to extract the position and the head ratio of the main subjects. Furthermore, we fine-tune a pre-trained ResNet50 model to classify the framing type given an input subject.

 For camera movement classification, given reference video, we first compute a dense optical flow.
 Yet, in order to accurately estimate the camera movement,
 we mask out dynamically moving objects from the optical flow. Then, we separate the video into nine regions to encode the spatial information. We then construct a motion vector by making a histogram of the orientation value of the optical flow.

 We use these vectors and train a simple three layered neural network model to predict the camera movement types.
 Based on the classified shot information, virtual camera layout is generated. Using the Toric representation, which was introduced by the previous work, virtual camera is initially placed with respect to target characters.
 Then, optimization on on-screen coordinate is conducted to make the virtual camera layout closely represent the framing type of the reference. There are three cost terms for framing and one term for preventing camera roll. First term is a visibility term, which ensures that the layout follows the framing type. Second term is a headroom term, which ensures that the layout has reasonable headroom size similar to that of the reference. Third term is a horizontal arrangement term,
 which ensures that on-screen horizontal arrangement is similar to that of the reference.
 Once the framing is determined,
 the camera movement is generated by interpolating the camera placements at the start and end keyframes of the shot.
 However, the initial placement often does not satisfy the constraints of the camera movement rules. Thus, we recalculate the position at the end frame or add keyframes in the middle
 so that the camera can correctly follow the conventional rules.
 Here are some results.
 These are the results generated using the same reference clip, but on different character types.
 It shows that our method works well with both human-like characters as well as the stylized characters.
 We also compared our results with the camera layout generated by the previous method. While the previous method better follows the on-screen head composition, our method better follows the framing type of the reference video To evaluate the user satisfaction on camera layout after framing optimization, we conducted an user study on 12 different shots taken with 4 different pairs of stylized characters. Layouts are generated using three different methods: artist’s manual work, our method, and the previous method
 The study consists of two parts:
 Users were first asked to rate how well the generated layout is with respect to the reference in 5-Likert scale. The results show that layout generated by our method are comparable with those generated by the artist.
 Then, 2-alternative forced choice test was conducted, which asked users to pick one of the two results that they think is better. Users preferred our results over that of the previous method, and show similar preference with artist’s work.
 This is it for the presentation. Thank you.
