{"authors": "Pablo Gallego Casc\u00f3n; Denys J C Matthies; Sachith Muthukumarana; Suranga Nanayakkara;  Chewit;  An", "pub_date": "", "title": "ChewIt. An Intraoral Interface for Discreet Interactions", "abstract": "Easy to Insert and Remove Take Out Using the Tongue Can be Hidden Unobtrusively Detecting Biting and Gestures Social Acceptability Interaction Capabilities a b c d 1 2 Figure 1: ChewIt is a novel intraoral interface, similar to a chewing gum, ofering new ways of discreet, hands-free interaction.", "sections": [{"heading": "INTRODUCTION", "text": "Researchers have proposed a variety of hands-free interactions [5,9,77], contributing to a greater efciency in multitasking [82]. In addition, they have also proven to be useful when applied as assistive technologies [30,34,45]. This signifcantly helps people with physical impairments to regain essential interaction capabilities, in particular patients suffering from locked-in syndrome [69]. While these interfaces can be useful for people with special needs, they are usually cumbersome and disruptive [43,46,57,63,90]. Particularly these two limitations prevent healthy people without special needs from adopting these new interaction interfaces. In contrast, potentially socially acceptable technologies ofer only a very limited input bandwidth [54,55].\nWe investigate a novel intraoral (\"in-the-mouth\") nonattached input interface, ChewIt, to strike a balance between social acceptability and a high input bandwidth. ChewIt provides discreet interactions, as users are able to hide the device inside the oral cavity. We exploit the well-pronounced dexterity of the tongue in conjunction with the jaw, teeth, and mouth cavity, to enable new input opportunities.\nTo gain a better understanding of the peculiarities of discreetness and general feasibility, we initially ran a series of user studies (Study 1: spectator's perception, S2: user's perceived obstruction, S3: understanding habits and limitations). Based on the results of these evaluations, we continued investigating potential intraoral interface designs through a further series of studies (S4: implications on dimension, S5: comparing shapes, S6: volume factor, S7: defning gestures). Informed by these investigations, we developed a prototype system, ChewIt.\nIn summary, the main contributions of this paper include the design, implementation, and validation of a non-attached intraoral interface.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "RELATED WORK Oral Input Interfaces", "text": "Speech Interfaces: Systems which utilize speech interfaces, such as intelligent personal assistants [12] or cognitive assistants [21] are gaining popularity. Some examples include Siri [41], Alexa [2], and Cortana [11], which are embedded into smartphones, computers, and standalone devices. This input modality successfully provides assistance when the hands are occupied with a primary task, such as driving and making a call. However, its drawbacks are in creating disturbances for others [20] and compromising privacy concerns.\nTackling this, Silent Speech Interfaces (SSIs) [17] are introduced aiming to maintain privacy, making speech interfaces more acceptable in public spaces. A great variety of diferent technologies [48] have been utilized, such as ultrasound [16,18], video imaging [35][36][37][38]76], audio [25], neuromuscular activity [43], electromyography [84], and electromagnetic motion [28,85]. SSIs are also useful in environments where surrounding noise inhibits audible data, as well as benefcial for people with speech impairments [22,29], who cannot use their own voice. However, current SSIs also yield a number of drawbacks. For instance, most interfaces are either intrusive [85], obtrusive [84], or require excessive processing times [37]. Apart from these technical issues, speech assistance yields a high memory burden and creates a disproportional cognitive load, for instance having to speak out a sentence when only a binary change is required [10].\nMouth Gesture Interfaces: Alternative mouth gesture interfaces have been introduced that enable a quick input mechanism, for instance controlling a computer with a simple sip and puf [62,63]. Such interfaces rely on diverse technologies, such as gnathosonics [83,86,87] and audio to recognize the sound of tooth touches [47] to implement clicks [4,59]. Other technologies to detect mouth gestures include optical sensing [15,31,52], air pressure changes [3], microradar data [49], infrared [7], and EMG [90]. These interfaces provide a signifcant beneft for people with special needs, namely those experiencing limited mobility, and for applications that require of additional input [73]. However, as these interfaces are all externally attached, they are highly noticeable by others and thereby limit social acceptability among those not requiring special assistance.\nTo overcome these issues, semi-invasive approaches have been proposed, including a swallowed probe [32] and an in-ear-placed sensor [50,53,55]. In addition, invasive technologies, such as an RFID chip under the skin [33], piercings with magnets [39], and a tongue-glued magnet [70] have been investigated in research.\nAs social acceptability of implantable technology may be questionable, we focus on semi-invasive technologies that can be easily removed by the users themselves. These semi-invasive mouth gesture interfaces are often used by people with physical impairments, such as Jouse [42] and IntegraMouse [40]. Most of these interfaces are comprised of two or more separate parts, in which one is placed inside the mouth [39,46]. Although these interfaces enable great input mechanisms, most are either located on the palatal vault space [64,71,72,74,75,81], at the buccal shelf area, or at the lower jaw [67], also creating speech impediments.", "n_publication_ref": 60, "n_figure_ref": 0}, {"heading": "User Acceptance of Technology", "text": "Social conventions [26] and context play an important role in the prevalence and acceptance [14,27,58] of novel technologies. As previously stated, using a speech assistant such as Siri [61] in public or in a meeting has the potential to create feelings of awkwardness or embarrassment for users. Keltner and Buswell [44] demonstrated that social embarrassment is a complex emotion. Several factors, such as loss of control and failure of privacy regulation, can make the early adoption of new user interfaces difcult [19,68]. Rico and Brewster also aimed a part of their work [8] at exploring social acceptability, dividing it into two viewpoints: how individuals feel while interacting; and how others perceive the user interaction. Montero et al. [60] adopted this concept and refned these two viewpoints into \"User's Social Acceptance\" and \"Spectator's Social Acceptance\", which other researchers, such as Ahlstrom et al. [1], incorporated into their studies.\nThis work focuses on an intraoral interface whereby interactions mimic an edible object, which is an acceptable behavior. When resting and not interacting, we envisage it to be invisible to others as users would be able to hide the interface at diferent places inside the mouth cavity. We conducted several studies to examine the acceptability of such an interface in terms of user perspective and spectator perspective. ", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "CHEWIT", "text": "ChewIt is a novel intraoral interface, similar to an edible object, ofering new ways to perform hands-free interactions. ChewIt can be easily inserted and removed whenever the user pleases, as it is not attached to the mouth. ChewIt's design enables the user to utilize several tongue and bite gestures that mimic the behavior of interacting with an edible object so that it does not draw increased attention of others. Furthermore, the device can be hidden in the mouth when not interacting, making it invisible to spectators. Based on the high level of dexterity and proprioceptive abilities of the tongue, once being familiar with the enabled input gestures, ChewIt can favour refexive interaction [56] potentially decreasing task load in multitasking scenarios.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Key Features", "text": "ChewIt's form factor and positioning inside the mouth provide a variety of unique features. In summary, the device:\n\u2022 can be easily hidden in the mouth (see Figure 1b) and thus remains invisible when not interacting; \u2022 is ready to interact as soon as the user takes it from its location (see Figure 1c); \u2022 can be discreetly interacted with natural tongue gestures, as the device can be easily moved around, such as changing orientation, fipping, etc; \u2022 does not obstruct speech when not in use. In some instances, users were able to drink and eat while holding the interface in the mouth; \u2022 does not require specifc user adjustment or calibration as it follows a straightforward implementation; \u2022 enables hands-free and eyes-free interaction in a mobile context, ofering an additional interaction channel or being an assistive device for users with impairments.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Implementation", "text": "Form: We used a polylactic acid flament (PLA) [23] to fabricate the casing according to the shape we determined based on our studies (see Figure 2). The electronics were populated into a fexible PCB to enable a better ft into the 3D-printed casing. The dimensions of the fnal device are 30 \u00d7 16 \u00d7 7 mm, with a weight of approximately 3.92g. As the components of our prototype are not 100% bio-compatible, we covered the device with a thin latex layer. Additionally, we attached a cotton string to the device to prevent the user from swallowing it, although this use case is highly unlikely.\nHardware: The technology behind the ChewIt prototype is based on an Inertial Measurement Unit (IMU). We used an MPU-9250 [6], a 9-axis motion tracking device that combines a 3-axis gyroscope, a 3-axis accelerometer, a 3-axis magnetometer, and a Digital Motion Processor (DMP), which are all integrated into a small 3 \u00d7 3 \u00d7 1 mm IC. Using this IMU, we determine the orientation and movements of the device. In addition, we also placed a button inside the device, which can be triggered by a biting gesture. A 2.4GHz low-power SoC (nRF51822 [51]), embedding a microcontroller and Bluetooth transceiver, handles the data stream. The IMU and the microcontroller communicate via I2C protocol. The maximum bandwidth was 1.9KB/s. The button uses a simple GPIO interface. Currently, the average energy consumption of the device is approximately 10.6\u00b5A when in idle mode and 10mA when fully operating. The hardware is powered by a CR1220 coin-cell battery, which enables it to run for approximately three hours. To save power, the prototype is resting in a low-power mode waiting for an activation, such as bite.\nGesture Recognizer: To prove technical feasibility and to gain some initial user impressions, we implemented a gesture recognizer driving a machine learning approach by using a conservative feature engineering.\nData Gathering: First, we recorded raw data from the accelerometer, gyroscope, and the button. We did not utilize the IMU's magnetometer, since ChewIt should be invariant to the absolute orientation of the user. Although the IMU is capable of providing high sample rates, we sampled the data at 100Hz in order to reduce power consumption and possibly avoid jitter. As each gesture can be executed in less than 2.5 seconds, we selected a window size of 256 samples. For our training data set, we recorded a single window containing a single gesture. We selected 9 gestures + 1 default gesture, in which the user was continuing his daily routine. We did not include the \"Swipe\"-gesture, as it is based on the surface of the device. We recorded 42-90 repetitions of each gesture (class). These classes were recorded in static and dynamic conditions such as directing the head left, right, up, and down while sitting and walking. We recorded our test set on the next two days -fnally containing 31 repetitions for each class. This way, our collected test data set is spatially and temporally separated from the training set. We decided to do this to avoid strong over-ftting efects and thus to increase the stability of the model generated by the classifer. Feature Engineering: Because our gathered input data is rather low-dimensional, as the recorded repetitions are little, we have chosen a conservative feature engineering instead of utilizing a neural network approach. Therefore, for each window, we calculated 49 diferent features per input trajectory, as we also combined axes from the accelerometer and gyroscope. Thus, each class was described with 402 computed features. The attribute selection of a J48 DT implementation determined these attributes as the most meaningful ones: meanCrossings(Button), minElement(Gx), rotationIndex-Feature(Gx), frstQuartile(Gx), meanCrossingsAll, activ-ityUnitAll, meanCrossings(Az), maxElement(Ay), max-Element(Gz), minElement(Gx), spectralLowHighBand-Quotient(Ax), rotationIndexFeature(Ay), spectralEnergy (Gy), interQuartileRange(Az), frequencyDiferenceOf-SecondAndThirdQuartile(Ax), rotationIndexFeature(Ay).\nClassifer Selection: To evaluate which classifer would provide high performance, we undertook an empirical approach comparing a Support Vector Machine (SVM), Random Forest (RF), Bayes Net (BN), K-nearest neighbours classifer (IBk), and a computational inexpensive C4.5 Decision Tree (DT) by using a leave-k instances -out method. Thereby, each instance represents a single repetition of an entire gesture. In total, the training set incorporated 617 instances, as the test set had a size of 310 instances. We built several models based on our training data set and tested the performance with our test data set. The results are displayed in Table 1.\nComparing recall rates, a one-way ANOVA for correlated samples (F 4,36 = 3.93, p<.01) revealed a statistical main efect. A Tukey's HSD revealed that both, the SVM (M = 96.77%; SD = 6.86%) and the RF (M = 97.1%; SD = 3.86%), were performing signifcantly better than the DT (M = 86.13%; SD = 18%). When unwilling to compromise on accuracy, the RF may be the best choice here. However, when aiming to implement a computational inexpensive gesture recognizer, the DT seems to provide reasonable results.\nPerformance Level: To determine where most confusions occurred, we included the confusion matrix of the DT in Figure 3. The visualization indicates that bite gestures (\"Incisor Bite\", \"Molar Bite\", and \"Peripheral Bite\") resulted in greater confusions compared to other tongue gestures. This is due to the limitations of our initial prototype, which only incorporated a single button. However, with the current prototype, we can achieve reasonable accuracy above 95% with the top fve gestures (+ default class: \"No Gesture\"). Considering this reduced set, we also implemented a real-time gesture recognizer using a DT and utilizing a sliding window approach, while shifting the window every 16 samples. ", "n_publication_ref": 2, "n_figure_ref": 2}, {"heading": "PERSPECTIVES & GENERAL FEASIBILITY", "text": "At the beginning of this research, we sought to understand the acceptance and general feasibility of an intraoral interface. Therefore, we conducted 3 user studies to investigate the spectator's perspective when an object is placed in the mouth (study 1), the user's perceived obstruction when pursuing daily tasks with an object in the mouth (study 2), and understanding user habits and limitations when using common intraoral objects, such as chewing gums (study 3).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study 1: The Spectator's Perspective", "text": "We investigated the discreetness of an intraoral object, placed inside the mouth, from a spectator point of view.\nParticipants & Procedure: We used 2 diferent object sizes, small (25 \u00d7 15 \u00d7 7 mm, 1Kmm 3 ) and large (30 \u00d7 18 \u00d7 7 mm, 3Kmm 3 ), and recorded 2 users with the large object, the small object, and no object in the mouth. These recordings were taken for talking, smiling, and rotating the head from left to right. In summary, we created the following set of images/animations:\n(1) Talking: 3 videos (2 facial side views of 2 users, 2 facial front views of 2 users) (2) Smiling: 3 still images (2 facial side views of 2 users, 2 facial front views of 2 users) (3) Rotating Head: 3 videos (1 view each from 2 users) These viewing conditions were distributed as an online survey. Participants were instructed to point the images where they could spot the devices. In case of doubt, they were asked to not make a selection. The survey was completed by 42 participants, from which 14 were female, with an age range of 20 to 55 years. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "Confdence: Although we took pictures under diferent light conditions, an ANOVA did not evidence the light condition to have a main efect on the participants' confdence level for Condition 1 (F 3,154 = 0.155, p>.05) and Condition 2 (F 3,154 = 0.214, p>.05). We also ran a t-test for Condition 3, which also did not show any statistical diference (T (76) = 0.507, p>.05). For the majority of answers, the participants were not fully confdent in identifying the image/animation which depicted an object in the mouth (M=63.66%; SD=1.75). In only M=3.15% (SD=0.56) of all cases, participants stated to be confdent in identifying the object.\nAccuracy: When drawing attention to the fact that there was a hidden device (see Table 2), the participants could correctly identify the larger size in M=36.4% (SD=6.3%) of all the cases and the smaller size to a percentage of M=22.05% (SD=1.61%). Answers that were mistaken are M=18.05% (SD=3.64%). In M=23.5% (SD=2.53%) of all cases, the spectator could not detect anything.\nIn conclusion, we found that spectators hardly noticed a chewing interface, when no interaction occurred and when the interface merely rested inside the buccal cavity. As two correct answers were included among the three images, there was a higher possibility that participants would guess with greater accuracy. However, the data appears almost normally distributed. Incorrect answers were slightly higher than 33%, as some participants did not spot any device in their responses. In terms of confdence, we can also confrm that most participants did not feel fully confdent while trying to spot the devices, once again evidencing that the device is hardly noticeable when not interacting.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study 2: User's Perspective", "text": "In this study, our goal was to determine the level of obstruction a user may perceive when carrying an intraoral object.\nParticipants & Procedure: We recruited 10 participants (3 females, 7 males) and provided them with a rectangular object (10 \u00d7 20 \u00d7 5 mm) produced from a Class IIa long-term bio-compatible resin [24]. We asked them to hold the object in their mouth between the inner cheek and the teeth (Molar or Premolar). Participants were required to perform daily ofce activities for 60 minutes, including writing and typing, etc, during this time we had conversations with them for 30 minutes and observed their behaviour throughout. After the experiment, we asked participants to rate the perceived obstruction on a 5-point Likert scale (1: not obstructing at all, 5: absolutely obstructing).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Results", "text": "Low Self-Perceived Obstruction: On average, participants did not feel disturbed with an object being placed in the mouth (M = 2.1/5; SD = 0.57). P6: \"I almost forgot having the device inside the mouth before you started talking to me. \" Two participants changed the location of the object because they were irritated by the constant pressure against the mouth tissue. All participants were still capable of clearly articulating themselves without noticeable diference. Some participants were even able to drink a beverage and consume a snack with the object in their mouth.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study 3: Understanding Habits and Limitations", "text": "In this study we aimed to investigate user habits when chewing a piece of gum.\nParticipants & Procedure: We recruited 14 participants (5 females, 9 males) and provided them with a piece of regular chewing gum. The task was to chew it until their jaws experienced fatigue. Once they fnished chewing, we asked participants to hide the gum in their mouth. Additionally, we asked them to point out the area in which they were holding the gum in their mouth (see Figure 1b: Molar <Blue>, Premolar <Yellow>, Cuspid <Red>, Incisor <Green>).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Results", "text": "Chewing Time: The minimum chewing time until a participant decided to stop chewing was 17.35 minutes (M = 41.86; SD = 18.69). Two users chew the gum for more than one hour, indicating not being bothered to continue.\nHolding Location: All but one participant used the Molar or Premolar teeth for chewing. All participants either held the chewing gum between those teeth or in the inner cheek, as the inner cheek was the preferred location.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DESIGN RATIONALE", "text": "The design of our prototype was informed by four studies, in which we focused on diferent aspects such as Dimensions, Form Factor and Gesture Feasibility.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study 4: Implications on Dimension", "text": "In this study, we sought to evaluate possible dimensions that would be most comfortable for the user and questioned whether size afects basic physiological activities. 3D printed, using the same bio-compatible resin from the previous study. We started with a rectangular-shaped object (10 \u00d7 20 \u00d7 5 mm) and incrementally expanded its size in one dimension. First, we increased the width by increments of 5mm. Participants had to rate the clarity of speech with a 5-point Likert scale and they chose the size that they were most comfortable with. the chosen object was then expanded in by an increment of 5mm. Participants had to rate the impact of the object on facial expressions using a 5-point Likert scale and chose the size that they are most comfortable with. Lastly, the thickness was expanded by an increment of 5mm and participants were asked to rate \"pufness\" of the face with a 5-point Likert scale and chose the one they felt was most comfortable (see Figure 4).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Results", "text": "Size: All participants found a width of 10mm to be comfortable. Only 6 out of 13 participants found the subsequent size of 15mm to be comfortable. A length of 20mm was reported to be comfortable by all participants. Only 6 out of 13 participants found the subsequent size of 25mm to be comfortable. Thickness of 5mm was found to be comfortable to all participants. Only 1 participant reported the next size of 10mm to be comfortable.\nClarity of Speech: Statistical diferences for (1) clarity of speech (Q = 23.52, p<.05, DF = 36) are confrmed by a Friedman test (k=3). A post-hoc analysis using Nemenyi's procedure two-tailed test found a signifcant diference (p<.05) between size A (M= 4.5; SD= 0.54) and size B (M= 3.62; SD= 0.62) and between size A and size C (M= 3.04; SD= 0.78). There was no statistical diference between B and C.\nImpact of Facial Expressions: A Friedman test indicated a statistical diference of (2) facial expressiveness (Q = 37.74, p<.05, DF = 36). A post-hoc analysis using Nemenyi's procedure (k=3) two-tailed test found a signifcant diference (p<.05) between size A (M= 4.35; SD= 0.72) and size E (M= 1.808; SD= 0.88). There was no statistical diference between D (M= 3.04; SD= 0.75) and E.\nImpact of Facial 'Pufness': Wilcoxon's test (k=3) indicated that self-perception of the \"pufness\" of the face (V = 88.5, Length Thickness p<.05) was signifcantly diferent between size A (M= 4; SD= 0.89) and size F (M= 2.15; SD= 0.8).\nBased on the results, we found the size of 10mm (width), 20mm (length), and 5mm (thickness) to be acceptable.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Study 5: Implications of Shape", "text": "In this study, we investigate how users perceive diferent geometrical shapes in terms of comfort, orientation recognition, and the ease of maneuvering it within the mouth.\nParticipants & Procedure: We recruited 12 participants (3 females, 9 males) to interact with 4 diferent shapes. These were (1) Asymmetrical Spherical Wedge (2) Spherical Cut, (3) Rectangular Prism, and (4) Triangular Prism (see Figure 5d). Building from the results of the previous study, we selected the dimensions 10 \u00d7 20 \u00d7 5 mm, All of them had a volume of 1Kmm 3 and similar proportions.\nWe asked participants to orient and rotate the object along pitch-, roll-, yaw-axis (see Figure 1d) and rate the comfort, understandability of the object's orientation, and ease of maneuverability of it on a 5-point Likert scale (1:worst, 5:best).", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Results", "text": "Orientation: Friedman's test (k=4), indicated a statistical diference (Q = 32.09, p<.05, DF = 44) between the shapes in terms of orientation 5.a. A post-hoc analysis using Nemenyi's procedure (k=3) two-tailed test found signifcant diference (p<.05) between Asymmetrical Spherical Wedge (M= 4.5; SD= 0.74) and Rectangular Prism (M= 2.53; SD= 0.65), between Spherical Cut (M= 4.5; SD= 0.74) and Rectangular Prism and between Triangular Prism (M= 3.7; SD= 1.22) and Rectangular Prism. There was no statistical diference (p>.05) between Triangular Prism and Spherical Cut.\nManeuverability: Friedman's test (k=4), indicated a statistical diference (Q = 29.17, p<.05, DF = 44) between the shapes in terms of maneuverability 5.b. A post-hoc analysis using Nemenyi's procedure two-tailed test found signifcant diference (p<.05) between Rectangular Prism (M= 3.1; SD= 0.85) and Asymmetrical Spherical Wedge (M= 1.63; SD= 0.87) and between Rectangular Prism and Triangular Prism (M= 2.3; SD= 0.89). There was no diference (p>.05) between Triangular Prism and Spherical Cut (M= 2.9; SD= 1.2).\nComfort: Friedman's test (k=4), indicated a statistical difference (Q = 12.9, p<.05, DF = 44) between the shapes in terms of comfort. A post-hoc analysis using Nemenyi's procedure two-tailed test found signifcant diference (p<.05) between Asymmetrical Spherical Wedge (M= 3.17; SD= 1.03) and Rectangular Prism (M= 1.75; SD= 0.965) and between Spherical Cut (M= 3.17; SD= 0.93) and Rectangular Prism. There was no statistical diference (p>.05) between Triangular Prism (M= 1.92; SD= 0.79) and Spherical Cut. Participants commented that fat surfaces are more comfortable ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ChewIt. An Intraoral Interface for Discreet Interactions", "text": "Figure 5: Four diferent 3D-printed objects were used to study the implication of shape. Average recorded for the understandability of orientation, ease of maneuvering and comfort (1:worst, 5: best)\nthan round surfaces when placed against the teeth. However, round surfaces were reportedly more comfortable against the cheek compared to fat surfaces. This occurs because of the anatomical features of the mouth. Users frequently reported that when placing a round surface against a non-fexible tissue, such as the teeth, the shape applies pressure onto a small area, which does not allow the object to be stable nor comfortable. However, as the inner cheek is a fexible tissue, having a round shape against it enables a better grip and increases comfort. Therefore, avoided corners and sharp edges, as they are uncomfortable and may even cut the soft mouth tissue.\nHolding Location: The Molar area is preferred (Asymmetrical Spherical Wedge: All participants; Spherical Cut: 11 out of 12 participants; Triangular Prism: 11 out f 12 participants; 9 out of such the device will be designed to ft inside the Molar cavity (see Shape ChewIt: We followed a systematic process to generate a reasonable shape for ChewIt. ChewIt's shape underwent various transformations. Based on the rectangle dimensions extracted from the previous study, we removed sharp (see Figure 6b). This was transformed to an asymmetric shape with a uniform weight distribution. These features are important to understand orientation and maneuverability (see Figure 6c). ChewIt also transformed to have a fat surface on one side and a rounded surface on the opposite, allowing it to be held with the inner cheek and to sit comfortably in the teeth (see Figure 6d). Study 6: Volume Factor In this study, we evaluate the impact of diferent volume proportions in terms of comfort and self-perceived discreetness.\nParticipants & Procedure: We recruited 13 participants (4 females, 9 males), who were given six objects with diferent volumes: (A) 0.25Kmm 3 , (B) 0.5Kmm , (C) 0.75Kmm , (D) 1Kmm 3 , (E) 2Kmm , and (F) 3Kmm . These were administered one at a time, and participants were asked to orient the object in diferent ways inside the mouth. The order of the objects was randomized.\nParticipants were asked to rate the maneuverability when they were orienting the device using a 5-point Likert scale (1: very easy, 5: very difcult). Participants were also asked to hide the object at two locations (see Figure 1b): Location 2 (at the Bottom, blue) and Location 1 (at the Top, black). After testing each location, were asked to indicate their preference for each location based on self-perceived and discreetness.", "n_publication_ref": 0, "n_figure_ref": 5}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Shape:", "text": "To analyze the diferences in maneuverability, we ran a one-way ANOVA, which confrmed a signifcant main (F 5,72 = 3.594, p<.05) between sizes (see Figure 7). A post-hoc analysis using Tukey's HSD test revealed diferences between Size D (M=4.58; SD=0.73) and Size A (M=3.3; SD=1.39) and between Size E (M=4.52; SD=0.64) and Size A. Size A was the size with the lowest mean There were no other statistical diferences (p>.05). Size D and Size E also had the higher mean score. Therefore, we chose a size between Size D and as a reasonable size.\nIn conclusion, we found smaller shapes to be easier to maneuver but excessively small ones were difcult to orient. However, largest sizes were found easy to orient but could become cumbersome to move.\nHolding Location: We did not fnd a preferred location (see Figure 1b) in terms of comfort, as 8 out of 13 participants chose Location 1 (at the Top, Black) and 5 out of 13 participants chose Location 2 (at the Bottom, Blue). However, we observed that all terms of discreetness, chose Location 2 as the most preferable position for hiding the object. As we prioritize the discreetness of the interface, the devices will be optimized to ft into Location Study 7: Defining Gestures explore potential gestures that users could use interact with an intraoral interface.\nParticipants & Procedure: We recruited 11 participants (3 females, 8 males) to study 14 gestures. The gestures are divided into 5 main groups (see Table 3). (a) Bites: performed by pressing the object with the teeth from opposite sides. We study the bites performed on the periphery of the object's surface (Peripheral Bite) and the bites performed on the center of the surface, using the Incisors and the Molars teeth (see Figure Rotations: performed on Roll, Pitch, and Yaw axis (see Figure 8b); (c) Tongue Movements: performed by placing the object on the top of the tongue and moving it from the left to the right or from the front to the back (see Figure 8c); (d) Location Changes: performed by moving the object from one place of the mouth to another (Figure 8d); (e) Tongue Drawing: performed by using the tip of the tongue to draw either Swipes or Complex Drawings, such as circles, triangles, or squares (see Figure 8e).\nTo help the participant understand the gestures, the experimenter demonstrated them with a 3D-printed replica of a mouth. After performing each gesture, the participants were asked to rate them based on (1) Ease of Use, (2) Natural Look, (3) Comfort, (4) Unobtrusiveness. The ratings were completed using a 5-point Likert scale (1:worst, 5:best). The order in which the gestures were performed were randomized. Also, to judge the 'natural look', we placed a mirror in front of the participant.\nGesture Feasibility: Our evaluation revealed that gestures have individual strengths and weaknesses (see Figure 9), depending on the observed parameter. While some gestures, such as Complex Drawings and Tongue: Middle to Front are hardly noticeable, they are difcult to perform. Across all the ratings, we found that the gestures Pitching as well as Location Middle to Front are less preferred. A one-way ANOVA showed a statistical main efect across all ratings: Ease of Use (F 13,181 = 7.84, p<.05), Natural Look (F 13,181 = 4.7, p<.05), Comfort (F 13,181 = 6.22, p<.05) and Unobtrusiveness (F 13,181 = 4.38, p<.05). The post-hoc analysis is accomplished using a Tukey's HSD to determine detailed signifcances.\nIn order to discriminate a subset for each parameter, we followed two criteria: 1) Gestures need to have a strong interpretation (i.e., mean Likert ratings in the top Quartile), and 2) The chosen gesture(s) need to be statistically diferent from as many non-strong gestures as possible.\nComfort: The subset deduced for this parameter involves two gestures: Rolling (M= 4.54; SD= 0.52) and Molar Bite(M= 4.46; SD= 0.66).\nEase of Use: The subset deduced for this parameter involves two gestures: Molar Bite (M= 4.53; SD= 0.52) and Rolling (M= 4.46; SD= 0.66).\nIncisor Bite Bites (Figure 8a) Molar Bite Peripheral Bite Pitching Rotations (Figure 8b) Rolling Yawing Front-Back Tongue Movement (Figure 8c) Left-Right Side to Side Bottom to Top Location Change (Figure 8d)\nMiddle to Side Middle to Front Swipes Tongue Drawing (Figure 8e)\nComplex: Circle, Triangle, Square Based on the users' ratings on 4 parameters (Unobtrusiveness, Natural Look and Ease of Use) we identifed two gestures that stood out from the rest: Rolling and Molar Bite. However, in case the scenario requires a input bandwidth, the range of gestures can increase up to ten without afecting the discreetness of the interactions.", "n_publication_ref": 0, "n_figure_ref": 12}, {"heading": "CONTRIBUTION AND BENEFITS", "text": "The main contribution of this paper is the idea of an intraoral interface, which is similar to an interactive edible object. Such an interface will not only impaired people, but also users in daily hands-busy situations, particularly when performing high precision tasks that require both hands [13]. Furthermore, we contribute with our design decisions and fndings on Interaction time, Holding Location, Shape Considerations, Comfort on Shape, Dimensions and Volume Considerations, and Gesture Feasibility. Minimum Interaction Time: Our studies revealed participants felt comfortable with holding a small object in their mouth for at least 15 minutes. This was assessed while the distraction continued with their daily tasks.\nTwo Holding Locations: We identifed two locations (see Figure 1b) in which the users are able to hide an introal interface. The frst location (at the Top, Black) is the buccal shelf on the Maxilla, next to the Zigomatic Bone, partially under the Masseter Muscle, and the second is at the Body of the Lower Jaw Bone, under the Molar area (at the Bottom, Blue). With the frst location, the device seems to be less visible to others.\nTwo Basic Shape Considerations: We investigated 4 types of shapes, which are an Asymmetrical Spherical Wedge, a spherical cut, a rectangular prism, and a triangular prism. We developed conclusions from such fndings: Among those shapes, we found asymmetry to be an important factor in understanding the orientation of the device; a fat surface on one side and a rounded surface on the opposite maximizes the grip and the comfort. In future, we aim to specifcally explore into texture perception and weight distribution.\nComfort on Shape: While performing the studies, we discovered users mostly preferred rounded corners. Sharp corners should be avoided at all cost, as there is a high chance of cutting the soft tissues inside the mouth. This fact is refected by the comments from users, suggesting that pressure exerted by the small corners were irritating and annoying.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Dimensions and Volume Considerations:", "text": "The dimensions of an object placed inside the buccal cavity can afect the clarity of speech, facial expression, and the self-perception of the face. Smaller sizes seem to be easier to maneuver inside the mouth. Where the size is excessively small, users will fnd it difcult to orient. Likewise, larger sizes are easy to orient but cumbersome to maneuver.\nSelf-perceived and Spectator-perceived discreetness: Users sufer from a subjective efect when wearing diferent devices, regardless of the size. This could be due to the sensation of having something inside the mouth. However, was hardly noticeable by spectators in the user's vicinity, even with the largest object size.\nGesture Feasibility: We propose 2 gestures that stand out across all the studied parameters: Rolling and Molar Bite. However, there are 8 more gestures that can be used if a particular application needs larger input bandwidth, without compromising the discreetness.\nChewIt allows for multitasking, including gestures with low levels of obstruction (see Figure 9), which will enable users to perform basic tasks, such as speaking while wearing the We observed that some users were even able to drink and eat while having ChewIt in the mouth. However, we do not recommend this, as there is a high risk of swallowing it. In future, we aim to specifcally explore how users experience drinking, eating, and other physiological activities while wearing the device, and also to fnd implementation alternatives that could beneft the user in case the device is swallowed.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "LIMITATIONS", "text": "Although ChewIt enables new opportunities for future HCI, it inevitably has certain limitations, as with every other technology.\nHygienic Aspects: Concerns regarding hygiene and sterilization are evident when placing devices inside the mouth or spitting them out. Sharing ChewIt may therefore be inappropriate or unacceptable by users. Safety Concerns: A major concern in our studies was to avoid any type of bacterial infection risk. Therefore, we used a resin that can be sterilized with alcohol after every use and can be 3D printed by the FormLabs2 Printer. When deploying ChewIt as a product, a crucial factor is the type of battery used. Silver oxide batteries are a possible option. Although they are not biocompatible, silver oxide batteries are used in colonoscopy cameras [79,80] and are still safer than Lithium batteries. Also, using electronic components which are absolutely biocompatible proves challenging.\nMaterial Properties: While the current prototype is based on a rigid material, it is desirable to use a fexible material. This material must withstand several conditions, including being biocompatible, resistant to bacteria, and stand high forces of extensive biting. We recommend the usage of a bio-compatible silicon compound.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Prototype:", "text": "The current prototype has an 86.14% of overall accuracy within all 10 possible gestures. We plan to increase the accuracy by implementing existing solutions into the next prototype iterations. These solutions take into account the orientation of the object and the inertia when walking by including: 1) a secondary IMU [65], 2) taking into account existing inertia [88] and the pendulum-like behaviour when walking [89], 3) the orientation of the device [78], and 4) a pressure matrix that detects bites on diferent regions within the surface of ChewIt. However, real-world scenarios do not demand such a high bandwidth of gestures for controlling conventional interfaces. We recommend a subset of 2 gestures: Rolling and Molar Bite for general purpose applications, such as controlling a music interface, controlling a wheelchair, or navigating through menus while engaging in another activity. As mentioned before, the accuracy of the current prototype for a subset of 2 gestures is 94.98%.\nSocial Acceptability : Using a discreet intraoral interface may be novel way to interact hands-free. We evaluated social acceptance based on whether a spectator could detect that a user is holding ChewIt, as well as if a user's perceived However, social acceptability is a complex emotion [44] that depends factors such as context, individual preferences, and culture. While our initial evaluation indicates that ChewIt ofers a variety of discreet gestures, their social acceptance remains untested.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we presented ChewIt, an intraoral interface that enables hands-free input operations. Our goal was to strike a better balance between social acceptability and expanding capabilities beyond merely providing a binary input. We studied social acceptance from a spectator's point of view, as well as self-perception, while participants used intraoral objects. We developed a prototype where design decisions were derived by a series of user studies. We view ChewIt as a novel input interface that can assist people with and without impairments.\nFor future work, we need to investigate ChewIt in public spaces, including among ethnic groups, to gain greater insights on the evolvement of user acceptance during longterm usage. In technical terms, one may consider using Electromagnetic Articulograph (EMA) to further determine the device's accuracy when performing input gestures or positioning tasks. Related research also indicates that nonattached intraoral devices, such as electric chewing gums [66], could be a new interaction modality infltrating the user's body in the future.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "work was supported by Assistive Augmentation research grant under the Entrepreneurial Universities (EU) initiative of New Zealand. We would like to thank all the participants for taking part in our user studies and anonymous reviewers their valuable comments and helpful suggestions.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Are you comfortable doing that?: studies of around-device gestures in and for public settings", "journal": "", "year": "2014", "authors": "David Ahlstr\u00f6m; Khalad Hasan; Pourang "}, {"title": "Amazon Alexa", "journal": "", "year": "2014-08-15", "authors": ""}, {"title": "CanalSense: Face-Related Movement Recognition System based on Air Pressure in Ear Canals", "journal": "ACM", "year": "2017", "authors": "Toshiyuki Ando; Yuki Kubo; Buntarou Shizuki; Shin Takahashi"}, {"title": "Bitey: An exploration of tooth click gestures for hands-free user interface control", "journal": "ACM", "year": "2016", "authors": "Daniel Ashbrook; Carlos Tejada; Dhwanit Mehta; Anthony Jiminez; Goudam Muralitharam; Sangeeta Gajendra; Ross Tallents"}, {"title": "High-level hands-free control of wheelchair-a review", "journal": "Journal of medical engineering & technology", "year": "2017", "authors": "Sharmila Ashok"}, {"title": "axis MotionTracking Inertial Measurement Unit MPU-9250", "journal": "", "year": "", "authors": ""}, {"title": "Stick it in your ear: Building an inear jaw movement sensor", "journal": "ACM", "year": "2015", "authors": "Abdelkareem Bedri; David Byrd; Peter Presti; Himanshu Sahni; Zehua Gue; Starner "}, {"title": "The gaime project: Gestural and auditory interactions for mobile environments", "journal": "British computer Society", "year": "2009", "authors": "Stephen Brewster; Roderick Murray-Smith; Andrew Crossan; Yolanda Vasquez-Alvarez; Julie Rico"}, {"title": "Blowatch: Blowable and hands-free interaction for smartwatches", "journal": "ACM", "year": "2015", "authors": "Wei-Hung Chen"}, {"title": "15 Memory load and task interference: hidden usability issues in speech interfaces", "journal": "Engineering Psychology and Cognitive Ergonomics", "year": "2017", "authors": "J Malcolm; Charles Cook; Robert Cranmer; Andy Finan; Carol-Ann Sapeluk;  Milton"}, {"title": "", "journal": "", "year": "2015-08-15", "authors": "Microsoft Cortana"}, {"title": "What can i help you with?: infrequent users' experiences of intelligent personal assistants", "journal": "ACM", "year": "2017", "authors": "Nadia Benjamin R Cowan; David Pantidi; Kellie Coyle; Peter Morrissey; Sara Clarke; David Al-Shehri; Natasha Earley;  Bandeira"}, {"title": "The da Vinci Surgical System", "journal": "", "year": "2018-08-28", "authors": ""}, {"title": "Embarrassment in consumer purchase: The roles of social presence and purchase familiarity", "journal": "Journal of consumer research", "year": "2001", "authors": "W Darren;  Dahl; V Rajesh; Jennifer J Manchanda;  Argo"}, {"title": "Human-Computer Interface Based on Visual Lip Movement and Gesture Recognition", "journal": "IJCSA", "year": "2010", "authors": "Czyzewski "}, {"title": "Prospects for a silent speech interface using ultrasound imaging", "journal": "", "year": "2006", "authors": "Bruce Denby; Yacine Oussar; G\u00e9rard Dreyfus; Maureen Stone"}, {"title": "Silent speech interfaces", "journal": "Speech Communication", "year": "2010", "authors": "Bruce Denby; Thomas Schultz; Kiyoshi Honda; Thomas Hueber; Jim M Gilbert; Jonathan S Brumberg"}, {"title": "Speech synthesis from real time ultrasound images of the tongue", "journal": "", "year": "2004", "authors": "Bruce Denby; Maureen Stone"}, {"title": "Embarrassing interactions", "journal": "ACM", "year": "2015", "authors": "Sebastian Deterding; Andr\u00e9s Lucero; Jussi Holopainen; Chulhong Min; Adrian Cheok; Annika Waern; Stefen Walz"}, {"title": "Privacy concerns for use of voice activated personal assistant in the public space", "journal": "International Journal of Human-Computer Interaction", "year": "2015", "authors": "Easwara Aarthi; Kim-Phuong L Moorthy;  Vu"}, {"title": "Can cognitive assistants disappear?", "journal": "IEEE Pervasive Computing", "year": "2016", "authors": " Maria R Ebling"}, {"title": "Development of a (silent) speech recognition system for patients following laryngectomy", "journal": "Medical engineering & physics", "year": "2008", "authors": "J Michael;  Fagan; R Stephen; James M Ell; E Gilbert; Peter M Sarrazin;  Chapman"}, {"title": "Ultimaker Polylactic Acid Filament(PLA)", "journal": "", "year": "2018", "authors": ""}, {"title": "Dental LT Clear Resin", "journal": "", "year": "2018-08-30", "authors": " Formlabs"}, {"title": "SilentVoice: Unnoticeable Voice Input by Ingressive Speech", "journal": "ACM", "year": "2018", "authors": "Masaaki Fukumoto"}, {"title": "Notes on the concept of a social convention", "journal": "New Literary History", "year": "1983", "authors": "Margaret Gilbert"}, {"title": "Embarrassment and social organization", "journal": "American Journal of sociology", "year": "1956", "authors": "Erving Gofman"}, {"title": "Analysis of phonetic similarity in a silent speech interface based on permanent magnetic articulography", "journal": "", "year": "2014", "authors": "A Jose;  Gonzalez; A Lam; Jie Cheah;  Bai; R Stephen; James M Ell;  Gilbert; K Roger; Phil D Moore;  Green"}, {"title": "Evaluation of a silent speech interface based on magnetic sensing and deep learning for a phonetically rich vocabulary", "journal": "", "year": "2017", "authors": " Ja Gonzalez Lopez; A Lam; Phil D Cheah;  Green; M James;  Gilbert; R Stephen;  Ell; K Roger; Ed Moore;  Holdsworth"}, {"title": "A hands-free interface for controlling virtual electric-powered wheelchairs", "journal": "International Journal of Advanced Robotic Systems", "year": "2016", "authors": "Tauseef Gulrez; Alessandro Tognetti; Woon Jong Yoon; Manolya Kavakli; John-John Cabibihan"}, {"title": "Improved Hands-Free Text Entry System", "journal": "", "year": "2018", "authors": "Gaurav Gupta"}, {"title": "A novel handy probe for tongue pressure measurement", "journal": "International Journal of Prosthodontics", "year": "2002", "authors": "Ryo Hayashi; Kazuhiro Tsuga; Ryuji Hosokawa; Mitsuyoshi Yoshida; Yuuji Sato; Yasumasa Akagawa"}, {"title": "Insertable digital devices: voluntarily under the skin", "journal": "ACM", "year": "2016", "authors": "J Kayla; Frank Hefernan; Lauren M Vetere; Bryan Britton; Thecla Semaan;  Schiphorst"}, {"title": "Hands-free gesture control with a capacitive textile neckband", "journal": "ACM", "year": "2014", "authors": "Marco Hirsch; Jingyuan Cheng; Attila Reiss; Mathias Sundholm; Paul Lukowicz; Oliver Amft"}, {"title": "Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips", "journal": "Speech Communication", "year": "2010", "authors": "Thomas Hueber; Elie-Laurent Benaroya; G\u00e9rard Chollet; Bruce Denby; G\u00e9rard Dreyfus; Maureen Stone"}, {"title": "Continuous-speech phone recognition from ultrasound and optical images of tongue and lips", "journal": "", "year": "2007", "authors": "Thomas Hueber; G\u00e9rard Chollet; Bruce Denby"}, {"title": "Phone recognition from ultrasound and optical video sequences for a silent speech interface", "journal": "", "year": "2008", "authors": "Thomas Hueber; G\u00e9rard Chollet; Bruce Denby; G\u00e9rard Dreyfus; Maureen Stone"}, {"title": "Acquisition of ultrasound, video and acoustic speech data for a silent-speech interface application", "journal": "", "year": "2008", "authors": "Thomas Hueber; G\u00e9rard Chollet; Bruce Denby; Maureen Stone"}, {"title": "A magnetoinductive sensor based wireless tongue-computer interface", "journal": "IEEE transactions on neural systems and rehabilitation engineering", "year": "2008", "authors": "Xueliang Huo; Jia Wang; Maysam Ghovanloo"}, {"title": "", "journal": "", "year": "2011-08-22", "authors": " Integramouse+"}, {"title": "", "journal": "", "year": "2010-08-15", "authors": "Siri Apple Ios"}, {"title": "", "journal": "", "year": "2018-08-28", "authors": " Jouse3"}, {"title": "AlterEgo: A Personalized Wearable Silent Speech Interface", "journal": "ACM", "year": "2018", "authors": "Arnav Kapur; Shreyas Kapur; Pattie Maes"}, {"title": "Embarrassment: its distinct form and appeasement functions", "journal": "Psychological bulletin", "year": "1997", "authors": "Dacher Keltner; Brenda Buswell"}, {"title": "Feasibility of Hands-Free Smart-Device Interface for Quadriplegic Patients", "journal": "International Information Institute (Tokyo). Information", "year": "2016", "authors": "Hyeonseok Kim; Heon Jeong; Jaehyo Kim"}, {"title": "The tongue enables computer and wheelchair control for people with spinal cord injury", "journal": "Science translational medicine", "year": "2013", "authors": "Jeonghee Kim; Hangue Park; Joy Bruce; Erica Sutton; Diane Rowles; Deborah Pucci; Jaimee Holbrook; Julia Minocha; Beatrice Nardone; Dennis West"}, {"title": "Input device for disabled persons using expiration and tooth-touch sound signals", "journal": "", "year": "2010", "authors": "Koichi Kuzume"}, {"title": "Silent speech interface design methodology and case study", "journal": "Chinese Journal of Electronics", "year": "2016", "authors": "Wenshi Li"}, {"title": "Tongue-n-cheek: non-contact tongue gesture recognition", "journal": "ACM", "year": "2015", "authors": "Zheng Li; Ryan Robucci; Nilanjan Banerjee; Chintan Patel"}, {"title": "EarPut: Augmenting Earworn Devices for Ear-based Interaction", "journal": "ACM", "year": "2014", "authors": "Roman Lissermann; Jochen Huber; Aristotelis Hadjakos; Suranga Nanayakkara; Max M\u00fchlh\u00e4user"}, {"title": "Bluetooth low-energy transceiver nRF51822", "journal": "", "year": "", "authors": ""}, {"title": "Designing, playing, and performing with a vision-based mouth interface", "journal": "", "year": "2003", "authors": "J Michael; Michael Lyons; Nobuji Haehnel;  Tetsutani"}, {"title": "InEar BioFeedController: a headset for handsfree and eyes-free with mobile devices", "journal": "ACM", "year": "2013", "authors": "J C Denys;  Matthies"}, {"title": "Botential: Localizing on-body gestures by measuring electrical signatures on the human skin", "journal": "ACM", "year": "2015", "authors": "J C Denys;  Matthies; T Simon; Bodo Perrault; Shengdong Urban;  Zhao"}, {"title": "Earfeldsensing: a novel in-ear electric feld sensing to enrich wearable gesture input through facial expressions", "journal": "ACM", "year": "2017", "authors": "J C Denys; Bernhard A Matthies; Bodo Strecker;  Urban"}, {"title": "Refexive Interaction -Extending Peripheral Interaction by Augmenting Humans", "journal": "", "year": "2017", "authors": "J C Denys;  Matthies"}, {"title": "SensIR: Hand Gestures with a Wearable Bracelet using Infrared Transmission and Refection", "journal": "ACM", "year": "2017", "authors": "Jess Mcintosh; Asier Marzo; Mike "}, {"title": "On the nature of embarrassabllity: Shyness, social evaluation, and social skill", "journal": "Journal of personality", "year": "1995", "authors": "S Rowland;  Miller"}, {"title": "Teethclick: Input with teeth clacks", "journal": "", "year": "2006", "authors": ""}, {"title": "Would you do that?: understanding social acceptance of gestural interfaces", "journal": "ACM", "year": "2010", "authors": "S Calkin; Jason Montero;  Alexander; T Mark; Sriram Marshall;  Subramanian"}, {"title": "Voice activated personal assistant: Privacy concerns in the public space", "journal": "", "year": "2013", "authors": "Moorthy Aarthi Easwara"}, {"title": "Comparative study on diferent adaptation approaches concerning a sip and puf controller for a powered wheelchair", "journal": "IEEE", "year": "2013", "authors": "Imad Mougharbel; Racha El-Hajj; Houda Ghamlouch; Eric Monacelli"}, {"title": "Sip n-Puf Origin Instruments", "journal": "", "year": "2018-08-24", "authors": ""}, {"title": "Development and functional demonstration of a wireless intraoral inductive tongue computer interface for severely disabled persons", "journal": "Disability and Rehabilitation: Assistive Technology", "year": "2017", "authors": "Lotte Ns Andreasen Struijk; Eugen R Lontis; Michael Gaihede; A Hector; Morten Enemark Caltenco; Henrik Lund; Bo Schioeler;  Bentsen"}, {"title": "Continuous activity recognition in a maintenance scenario: combining motion sensors and ultrasonic hands tracking. analysis and applications 15", "journal": "", "year": "2012", "authors": "Georg Ogris; Paul Lukowicz; Thomas Stiefmeier; Gerhard Tr\u00f6ster"}, {"title": "Unlimited Electric Gum: A Piezo-based Electric Taste Apparatus Activated by Chewing", "journal": "ACM", "year": "2018", "authors": "Naoshi Ooba; Kazuma Aoyama; Hiromi Nakamura;  Miyashita"}, {"title": "An arch-shaped intraoral tongue drive system with built-in tongue-computer interfacing SoC", "journal": "Sensors", "year": "2014", "authors": "Hangue Park; Maysam Ghovanloo"}, {"title": "First Click: When new tech is just too embarrassing to use", "journal": "", "year": "2016-08-28", "authors": "Thomas Ricker"}, {"title": "Quality of life in patients with locked-in syndrome: Evolution over a 6-year period", "journal": "Orphanet journal of rare diseases", "year": "2015", "authors": "Marie-Christine Rousseau; Karine Baumstarck"}, {"title": "The tongue ear interface: a wearable system for speech recognition", "journal": "ACM", "year": "2014", "authors": "Himanshu Sahni; Abdelkareem Bedri; Gabriel Reyes; Pavleen Thukral; Zehua Guo; Thad Starner; Maysam Ghovanloo"}, {"title": "An isometric tongue pointing device", "journal": "ACM", "year": "1997", "authors": "Chris Salem; Shumin Zhai"}, {"title": "Optically sensing tongue gestures for computer input", "journal": "ACM", "year": "2009", "authors": "Daniel Scott Saponas;  Kelly; A Babak; Desney S Parviz;  Tan"}, {"title": "A tongue input device for creating conversations", "journal": "ACM", "year": "2011", "authors": "Ronit Slyper; Jill Lehman; Jodi Forlizzi; Jessica Hodgins"}, {"title": "An inductive tongue computer interface for control of computers and assistive devices", "journal": "IEEE Transactions on biomedical Engineering", "year": "2006", "authors": "Lotte Ns Andreasen Struijk"}, {"title": "Wireless intraoral tongue control of an assistive robotic arm for individuals with tetraplegia", "journal": "Journal of neuroengineering and rehabilitation", "year": "2017", "authors": "Lotte Ns Andreasen Struijk; Line Lindhardt Egsgaard; Romulus Lontis; Michael Gaihede; Bo Bentsen"}, {"title": "Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands", "journal": "UIST ACM", "year": "2018", "authors": "Ke Sun; Chun Yu; Lanand Shi Yuanchun; Weinanand Shi;  Liu"}, {"title": "Widraw: Enabling hands-free drawing in the air on commodity wif devices", "journal": "ACM", "year": "2015", "authors": "Li Sun; Souvik Sen; Dimitrios Koutsonikolas; Kyu-Han Kim"}, {"title": "Activity recognition an accelerometer embedded mobile phone with varying positions and orientations", "journal": "Springer", "year": "2010", "authors": "Lin Sun; Daqing Zhang; Bin Li; Bin Guo; Shijian Li"}, {"title": "The future of wireless capsule endoscopy", "journal": "World journal of gastroenterology: WJG", "year": "2008-05-04", "authors": "Paul Swain"}, {"title": "An oral tactile interface for blind navigation", "journal": "IEEE Transactions on Neural Systems and Rehabilitation Engineering", "year": "2006", "authors": "Hui Tang; J David;  Beebe"}, {"title": "Driving while interacting with Google Glass: Investigating the combined efect of head-up display and hands-free input on driving safety and multitask performance", "journal": "Human factors", "year": "2017", "authors": "G Kathryn; Elayaraj Tippey; Thomas K Sivaraj;  Ferris"}, {"title": "Monitoring the state of the occlusion-gnathosonics can be reliable", "journal": "Journal of oral rehabilitation", "year": "1998", "authors": " Kw Tyson"}, {"title": "Array-based Electromyographic Silent Speech Interface", "journal": "", "year": "2013", "authors": "Michael Wand; Christopher Schulte; Matthias Janke; Tanja Schultz"}, {"title": "Preliminary test of a real-time, interactive silent speech interface based on electromagnetic articulograph", "journal": "", "year": "2014", "authors": "Jun Wang; Ashok Samal; Jordan R Green"}, {"title": "Clinical applications of gnathosonics", "journal": "Journal of Prosthetic Dentistry", "year": "1966", "authors": "M David;  Watt"}, {"title": "Gnathosonics -a study of sounds produced by the masticatory mechanism", "journal": "Journal of Prosthetic Dentistry", "year": "1966", "authors": "M David;  Watt"}, {"title": "A novel hand gesture input device based on inertial sensing technique", "journal": "IEEE", "year": "2004", "authors": "Jing Yang; Eun-Seok Choi; Wook Chang; Won-Chul Bang; Sung-Jung Cho; Jong-Koo Oh; Joon-Kee Cho; Dong-Yoon Kim"}, {"title": "IMU-based ambulatory walking speed estimation in constrained treadmill and overground walking", "journal": "Computer methods in biomechanics and biomedical engineering", "year": "2012", "authors": "Shuozhi Yang; Qingguo Li"}, {"title": "Non-intrusive tongue machine interface", "journal": "ACM", "year": "2014", "authors": "Qiao Zhang; Shyamnath Gollakota; Ben Taskar; Raj Pn Rao"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: ChewIt prototype -basic hardware is integrated with fexible custom-made PCB, placed inside the 3Dprinted casing, developed from a polylactic acid flament.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Confusion matrix for the 9 gestures (+ 1 default class: \"No Gesture\") for a C4.5 Decision Tree. The numbers are in percentages. The overall F1 Score is 0.861.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4:  We the object in one dimension in the following order of width, length, and thickness.", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: The shape was derived based on previous fndings. (Top row: top view; bottom row: side view)", "figure_data": ""}, {"figure_label": "78", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 7 :Figure 8 :78Figure7: Impact of volume on maneuverability (1: very easy, 5: very difcult) for 6 volumes: A 0.25Kmm 3 , B: 0.5Kmm 3 , C: 0.75Kmm 3 , D: 1Kmm 3 , E: 2Kmm 3 and F: 3Kmm3 ", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Classifer Performances (Recall Rate).", "figure_data": "Classifer SVMRFBNIbKDTRecall96.77% 97.1% 95.48% 89.35% 86.13%"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Detection of the presence of an object in the mouth by a spectator.", "figure_data": "Response1. Talking 2. Smiling 3. Rotating HeadLarge object42%29.6%37.6%Small object20.4%22%23.7%No object14.9%22%17.2%Unsure22.7%26.4%21.5%Correct62.4%51.6%61.3%Incorrect37.6%48.4%38.7%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Fourteen gestures have been defned. Peripheral Bite is performed on the object's peripheral surface. Incisor and Molar bites are performed at the center of the device using the Incisors or the Molar. Rotations around the axis are shown in Figure 1d. Natural Look: . The subset deduced for this parameter involves ten gestures: Molar Bite (M= 4.31; SD= 1.33) , Translation: Left-Right (M= 4.15; SD= 0.69), Peripheral Bite (M= 4.15; SD= 0.8), Location: Side to Side (M= 4.08; SD= 0.95), Rolling (M= 4.08; SD= 1.12), Location: to Side (M= 4; SD= 1,08), Translation: Front-Back (M= 3.77; SD= 1.09), Location: Bottom to Top (M= 3.62; SD= 1.04), Incisor Bite (M= 3.54; SD= 1.33) and Swipes (M= 3.46; SD= 1.33).", "figure_data": "12345MolarRollingPeripheralBiteIncisor BiteSwipesTranslation:Left-RightLocationMiddle-SideLocation:Side to SideTranslation:Location:Bottom to TopComplexYawningto FrontComfort Natural Ease of UseFigure Average User rating for each gestures in terms ofEase of Use, Natural Look, Comfort, Unobtrusiveness.Unobtrusiveness: The subset deduced for this parameter in-volves three gestures: Incisor Bite (M= 4.38; SD= 0.96), MolarBite (M= 4.15; SD= 0.9) and Rolling (M= 4.15; SD= 0.8)."}], "doi": "10.1145/3290605.3300556"}