
 Hello, today I am introducing  our system, Gesture Knitter, which is a hand gesture design tool geared towards mixed reality applications. We will first introduce the
 problem and the specifications that drove our system design. Then, we will detail the motivation and implementation of our approach, and conclude with an overview of the evaluation.
 The emergence of head-mounted  mixed reality requires the need for expressive gesture-based interaction. Expressive interactions are especially prevalent in a wide variety of applications such as in education, 3D drawing and art, and gaming. However, there has been a lack

 of focus on protocols explicitly aimed at designing hand gestures.
 Current approaches involve deep learning techniques for directly developing recognizers, which is data intensive.
 In contrast, we aim to design a system enabling the design of expressive hand gestures, encoding them into recognizers,
 while being low data-intensive to enable rapid prototyping. We are motivated by a similar  system, Gesture Script, which focuses on interactively designing  2D unistroke gestures with rendering scripts. We take a similar  approach for hand gesture design.
 To meet our specifications, we take  a software engineering approach to promote reuse. Specifically, we  created a visual declarative script, allowing designers to create  complex expressive gestures by concatenating simpler gestures.
 Next, we implemented a backend process that converts the script into a recognizer by using Hidden Markov Models.
 We also have a lightweight method of generating synthetic samples to aid in training the recognizers from a Bayesian preference gallery approach.
 For our script, we first  divided hand gestures into two separate components - the  gross component which is the motion of the center of mass of the  hand, and the fine component
 which consists of the movement  of the fingers. For example, gross gestures include forward  and right as shown in this figure and fine gestures include thumbs up and pointing. We term these gestures as primitive gestures.
 Our visual declarative script  concatenates these primitive gestures node by node to create a complex  gesture. We enable designers to place the nodes sequentially as well as loop a particular node as shown. For example, this script shows  the movement of two hands with circles and fists then  forward with the palm forward.
 We then take the visual  declarative script and encode the complex gesture recognizer. Our  method uses Hidden Markov Models, which enable the concatenation  of the recognizers of individual primitive gestures to form the  complex gesture recognizer. This approach allows the reuse  of component recognizers in forming expressive complex gestures.
 Our approach so far relies on the  assumption that we have recognizers available for the gross and  fine primitives to build the complex gesture recognizers.  Although the user can provide hand demonstrations, this approach  can be laborious and data intensive. To alleviate this, we implemented  a synthetic sample generator using a preference gallery which  can aid in the training process. The user can select which  of the two options generated to put into the training dataset which adds sufficient perturbation.
 We also included additional tools to  aid the designer in the design process. Our decoding mechanism allows  the input of a complex gesture demonstration and automatically  generates the visual declarative script.
 In addition, we provide a  discernability tool to notify designers if two gestures declared are  difficult to discern from each other.
 We evaluated Gesture Knitter’s  potential with an evaluation of the recognition rates using data we  collected from participants using the HoloLens 2. Here,  evaluated are the train-on-all, cross validation between participants,  and synthetic data accuracy rates. These all show high feasibility in accuracy.
 The developer study also showed Gesture  Knitter was easy to use and intuitive.
 Gesture Knitter therefore allows  designers to expressively design novel and expressive hand gestures  while having low data overhead.
 Our approach is practical and  shows promise in deploying the recognizers generated  to real-world applications.

