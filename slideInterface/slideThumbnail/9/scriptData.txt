
 Hi, I’m Navid and I’m happy to present our paper, Latte: Use-Case and Assistive-Service Driven Automated Accessibility Testing Framework for Android.
 15% of the world population have some forms of disability. Software should be accessible,
 especially right now when apps are essential parts of our daily lives
 Users with disabilities use assistive services to interact with software
 For example, blind users use screen reader such as TalkBack in Android.
 Screen reader announces the textual description of elements in the screen and let users to navigate through element by especial gestures Also, users with motor impairments uses special keyboards like this one through a service
 called SwitchAccess in Android Unfortunately, Recent studies show accessibility
 issues are prevalent in mobile apps Therefore, it’s necessary for developers
 to evaluate the accessibility of their apps. Testing can be done either manually or automatically
 In manual testing, given an app, a user preferably with disability,
 performs some use cases using assistive service and report existing issues This technique is accurate since the use cases are evaluated using the actual way disabled users interact with a device. However, manual testing is time consuming,
 impractical, and usually developers do not have access to users with variety of disabilities
 The other way of testing is through automated tools. The existing automated testing technique, such as Accessibility Scanner, scans a screenshot and perform some checks
 on individual GUI elements according to guidelines. Automated techniques are fast and can be performed
 after each update of the app. However, developers either not utilize or
 even ignore the results, one study showed that half of GUI templates in official Android IDE have accessibility issues. And another study showed that more than 50%
 of the actual accessibility problems could not be detected by accessibility guidelines. The question is that is it possible to have a testing technique that could benefit from
 both side? We believe our approach, Latte, can be the first step toward this ideal case.
 Latte is automated, And focuses on main functionalities by reusing
 GUI test cases. Mobile developers usually write GUI test cases to test main functionalities of their app. Also, Latte reflects the way users with disabilities
 interact with apps, using assistive services. Here is an overview of Latte’s process which
 consists of three parts First, Latte generates a use-case specification
 from the input test case. Use-case specification is a representation of the use case with accessibility-related information. Then Latte executes the use-case specification
 with an assistive service like TalkBack or SwitchAccess. Note that this is the main part that our approach differs from other technique as Latte actually interact with the device using an assistive service, just like users with disability do Finally, Latte produces a report that contains the accessibility failures and warnings that
 it finds during the execution Accessibility Warnings are steps in a use
 case that are accessible; however, it takes an exorbitant amount of time. Moreover, we report additional information such as number of actions that it takes to complete a use case and a video recording of the use case execution to make developers familiar with the struggles of users with disabilities.
 Now let’s see how Latte works through an example. This is the launcher screen of an app. Let say its developers want to test an important
 use case in this app like Registration. Here is a GUI test case that executes this functionality.
 Since the test case is passed, it seems this use case is working correctly; however, only
 for users without disabilities. Then the developers test the app using Accessibility Scanner. It reports a several issues with elements
 such as low contrast and small touch target size which do not impact a blind user. Now, let’s evaluate the app by Latte. Since the first step of the test case is clicking
 on create account, Latte tries to click on it using TalkBack.
 When TalkBack is enabled, the background image is focused

 Latte tries to reach to the front layout but it’s impossible Because as Latte navigates the app, the background images are revolving and creates an infinite
 loop This scenario is the same for SwitchAccess.
 Now let’s see the effectiveness of Latte for real-world apps in our empirical evaluation.
 We implemented a prototype of this work for TalkBack and SwitchAccess in Android
 We used 20 Android apps for the empirical evaluation. For TalkBack, Latte found 39 failures where 21 of them could not be detected by Accessibility

 Scanner And For SwitchAccess, Latte found 11 failures where none of them could be detected by Accessibility Scanner.
 We manually examined the failures and warnings detected by Latte and categorize them; so, developers can be aware of them. Due to the limited time, I encourage you
 to visit our website to watch video demonstrations, read our paper, or access the entire source code of Latte. Thanks for listening.
