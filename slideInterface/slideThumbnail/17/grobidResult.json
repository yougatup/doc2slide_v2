{"authors": "Brianna Richardson; Jean Garcia-Gathright; Samuel F Way; Jennifer Thom; Henriette 2021 Cramer", "pub_date": "", "title": "Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits", "abstract": "In order to support fairness-forward thinking by machine learning (ML) practitioners, fairness researchers have created toolkits that aim to transform state-of-the-art research contributions into easily-accessible APIs. Despite these eforts, recent research indicates a disconnect between the needs of practitioners and the tools ofered by fairness research. By engaging 20 ML practitioners in a simulated scenario in which they utilize fairness toolkits to make critical decisions, this work aims to utilize practitioner feedback to inform recommendations for the design and creation of fair ML toolkits. Through the use of survey and interview data, our results indicate that though fair ML toolkits are incredibly impactful on users' decision-making, there is much to be desired in the design and demonstration of fairness results. To support the future development and evaluation of toolkits, this work ofers a rubric that can be used to identify critical components of Fair ML toolkits.\u2022 Computing methodologies \u2192 Machine learning; \u2022 Humancentered computing \u2192 Empirical studies in HCI.", "sections": [{"heading": "INTRODUCTION", "text": "As new applications of machine learning (ML) emerge across industries, stakeholders and researchers alike aim to reduce the negative infuence of unanticipated biases in these algorithms. News coverage has focused on several algorithms for the role they play in furthering social inequities. For instance, the ML recognition systems in autonomous vehicles have been less efective in recognizing darker skin [53]; search engine results have reinforced representation bias [42]; and even professional networking sites were discovered to recommend \"male-sounding\" variations of names when \"female-sounding\" names were searched [17]. Institutions have been quick in formulating their own guidelines for confronting algorithmic bias [28]. These guidelines focus on themes like fairness, transparency, accountability, and so on [28]. Within institutions, teams are also focusing on how to transform literature into actionable steps that can be taken by product teams [14,22,27,34,35].\nAdding to these studies, the last half decade has seen a proliferation of algorithmic bias research [15]. New research communities, such as the Fairness, Accountability, and Transparency (FAccT) conference have emerged with goals of confronting algorithmic bias across industries [3]. In order to support the inclusion of fairnessforward thinking in the machine learning pipeline, applications are being created that allow non-research-based practitioners to employ state-of-the-art fairness considerations in their own work.\nA few examples of such toolkits include IBM's AI Fairness 360 [9], Google's Fairness Indicators [2], Microsoft's Fairlearn [39], and UChicago's Aequitas [47].\nDespite these advancements in tooling, however, recent work has revealed a disconnect between the tools created by fairness experts and the needs of practitioners [27,34,35,37,50]. While several tools are available to practitioners, a lack of communication has prevented their application in high-stakes product teams. In order to encourage adoption, participation and buy-in with respect to fair ML tool development and usability must be considered.\nThrough analysis of prior work and practical testing of tools, we present a set of standards for evaluation of such tools in the future. By giving 20 practitioners the frst-time experience of engaging with fair ML tools, this study measured the impact of these tools, collected practitioner perceptions towards them, and utilized practitioner feedback to inform recommendations for the design and creation of fair ML toolkits. Participants saw one of two fair ML to previous literature on tooling, were used to inform a rubric that can be used as both a guide and evaluation tool for practitioners, fairness design experts, and organizations to use in the creation and selection of fair ML in industry settings.\nThe contributions of this study are three-fold:\n\u2022 Conduct a comprehensive literature review on user needs in FAccT tooling \u2022 Collect user perceptions towards these tools and understand the role and the capabilities of fairness in user ML responsibilities \u2022 Measure the level of impact of fair ML tools \u2022 Propose a rubric that can be used as an evaluation tool and a guide for the integration of fairness tools into the workplace", "n_publication_ref": 21, "n_figure_ref": 0}, {"heading": "BACKGROUND & RELATED WORK 2.1 Fair ML", "text": "Fairness in ML is defned by both mathematical and social notions of fairness and considers the distributions of both the harms and the opportunities provided by ML [15]. Fairness is defned in a multitude of ways by researchers [8,38,52], making the decision of which fairness methodology to adopt and which corresponding metrics to quantify highly contextual.\nIn order to assist practitioners in the fairness consideration process, several institutions have created toolkits that are applicable to a diverse range of models and datasets [2,9,20,39,47,49]. Each tool difers in the means and the methods provided for enforcing fairness, the visual demonstrations of statistics, and the support provided to new fairness users, among other diferences. Some tools allow for easy intersectional analysis, while others require prior data manipulation to see subgroup performance. Some tools provide interactive or colorful depictions of fairness results, while others leave the task of creating visualizations up to the user. Some tools are designed to integrate with pre-existing machine learning tools, while others are stand-alone, and the extent of any provided background information, demos, and tutorials difers substantially between toolkits.\nWhile each tool has explicit advantages and disadvantages, to our knowledge, this is the frst study engaging practitioners with fairness toolkits to evaluate the efectiveness of the tools, providing a greater understanding of the barriers to implement fair ML in practice.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "The Needs of Practitioners", "text": "Several previous works have outlined the needs of ML practitioners as they relate to FAccT technologies [27,34,35,37,50]. Veale et al. [51] conducted exploratory interviews with ML practitioners across several countries and industries who were applying ML to a diverse range of practices. This work demonstrated the critical need for conversations that elicit the fairness needs of experts. Furthermore, it highlighted a disconnect between institutions and practitioners when it came to fairness [51]. Holstein et al. [27] defned critical design needs of practitioners via interviews and online surveys. Through engaging with practitioners, these researchers defned critical gaps between conversations had in industry and through fairness research literature. Madaio worked alongside practitioners to co-design a checklist of needs for the construction of organizational infrastructures of fairness [37]. In interviews with practitioners, Law et. al. [34] showed participants two bias detection prototypes with difering design strategies to understand how they impact users' insights. Their results suggest that information load and comprehensive axes are critical considerations for optimal toolkit design [34].\nSeveral works have focused on identifying the needs of the practitioners by testing the efectiveness of FAccT technologies. Outside of fairness, but in the realm of interpretability, Kaur et al. studied the efectiveness of interpretability tools [31]. Through the use of contextual inquiry and online surveys, these researchers found that many practitioners struggled with interpretability tools and often over-trusted or misused these tools [31]. Lakkaraju & Bastani [33] tested the limits of certain explainable AI features by testing its efectiveness against black box and adversarial attacks. They found that manipulating favorable features in explanations could impact user trust by nearly 10-fold. This emphasized a need for careful consideration of how interpretations are presented to users [33]. Ribeiro et al. [46] tested the efectiveness of their XAI, LIME, by measuring the level of insight users were able to collect from engaging with the tool.\nWhile previous studies engaged practitioners through needfnding interviews, this study allowed practitioners to actively engage with mature, publicly-released fairness toolkits.", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "METHODS", "text": "To collect practitioners' feedback on Fair ML toolkits, we conducted semi-structured, one-on-one interviews with participants who had experience assessing machine learning models. Users interacted with a Qualtrics [44] survey, a Google Colaboratory notebook [23], and Google Meet conferencing during the extent of the study.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Participants", "text": "We recruited 20 participants across four research and industry institutions in July and August of 2020. All participants were individuals who had responsibilities assessing machine learning models. Participants were involved with a diverse array of technologies, including: Audio & Voice, Recommendation Systems, Bio-engineering, ML infrastructure, Ethical AI, and Natural Language Processing. 11 participants considered their work area Engineering/Product-oriented, 8 participants considered their work area Exploratory/Researchoriented, and 1 participant considered their work both research and product-oriented. Additional information about the participants can be referenced in Table 1.\nSix pilot interviews were conducted prior to ensure that the level of difculty of the tasks was appropriate considering the diverse roles being recruited for this study. Pilot interviews were also used to confrm that users would have enough time (60 minutes) to perform all tasks in the study.\nParticipants were recruited through internal communication channels and word-of-mouth. Invitations to participate included eligibility requirements and an invitation to learn about new and emerging fair ML toolkits. Participants for this study were, with relation to fair ML tools, mostly non-experts with high levels of interest. This collection of non-experts allowed us to gauge the the challenge presented by the novelty of the tool for frst-time users.  1: Participant Demographics. Participants self-reported work area and role along with the Participant ID that will be used for the extent of the paper.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Work Area", "text": "These participants represent a pivotal, yet highly understudied, group: those without experience with ML-fairness, but who are interested in using fair toolkits to address potential negative impacts in their own ML-work. These participants are a key demographic in fair ML UX research: the practitioners who will be the frst willing to engage with and advocate for fair ML tooling in their individual domains.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Fairness Toolkits", "text": "A between-subjects approach was used for this study where each participant saw one of two toolkits: either Google's Fairness Indicators toolkit [2] or UChicago's Aequitas toolkit [47] in a Google Colaboratory notebook. To select these tools used in this study, pilot tests were used to test several available tools with visual components. Of the tools that worked in the Google Colaboratory environment, we found that these two selected tools were the most representative, self-explanatory, and required the least guidance from researchers. Participants were shown the name of the toolkit, but not the institution where the toolkit came from. While no participant had experience with the toolkit that they worked with, one participant, however, did verbally notice that the Fairness Indicators toolkit was part of the TensorFlow pipeline [1] from appearance alone.\nFairness Indicators and Aequitas were the chosen toolkits for this study also for their ability to function in the study environment (Google Colaboratory) and their choice of visual aids which are infuential in the fairness consideration process [9]. Fairness Indicators [2] functions as an interactive widget, where users can provide their model(s), their performance and fairness metrics of choice, and the attributes on which slicing and evaluating will take place. Users also have the choice of doing model comparisons, intersectional analysis, and performance testing across thresholds. Fairness Indicators allow users to compare at most two models and allows users complete control of the attribute slices they choose to focus on. As an interactive widget, users have the opportunity to manipulate the metrics that they see, the thresholds that they can compare, and the slices that they'd like to focus on.\nAequitas [47] also serves as a visual bias detecting toolkit. Aequitas can be accessed via three diferent platforms: the command line, a python package, or through their web interface. Through this toolkit, users have the option to evaluate and compare models, calculate group biases, and highlight disparities. Aequitas' python toolkit has two types of visualizations: grouped bar charts and treemaps. Users also have the option of color coding these visualizations green or red based on fairness disparity thresholds that the user can assign. Similar to Fairness Indicators, comparisons can be made across models. While these two toolkits are diverse in the types of visualizations ofered and the features available, they are also very representative of the options in the fair ML space.\nA randomizer was used to make sure an even distribution of participants saw each of the toolkits.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Data & Models Viewed by the Participants", "text": "Fairness tools were used to visualize and compare fairness results from three diferent models built from the same data. The dataset used for this study was UCI's 1994 Adult Census data [32]. This publicly-available dataset is commonly used in algorithmic bias research [4,9,20,31,36] due to its biased sampling of classes. Once the data was retrieved from the UCI page [19], gender, race, and the predictive class were separated for subsequent analysis and a few irrelevant attributes were removed. The attributes left for model generation were age, work class, education level, marital status, occupation, investment capital loss and gain, hours worked per week, and native country. These attributes were used to predict whether or not an individual makes over $50,000 a year. This dataset was chosen due to its inherent biases that -if left ignored -could be refected in models trained with it.\nThree separate models were built: a Logistic Regression (LR) Model, a Random Forest (RF) Model, and a Neural Network (NN) Model. These models were selected to provide a diverse subset of supervised learning methods and counteract any preference bias from participants. Two-thirds of the data was used for training the models while the last third was used for model evaluation. To generate fairness diferences between models for participants to discover, we post-processed the models in the following ways: for Linear Regression and Random Forest, post-processing fairness modifcations and balancing of training data was done on the predictors. Microsoft's FairLearn ThresholdOptimizer [39] was used to adjust these predictors to satisfy certain parity constraints. For Logistic Regression, the model was optimized to satisfy Equalized Odds, which requires that a fair classifer predict positive and negative class across groups with the same likelihood [25]. For Random Forest, the model was optimized to satisfy Demographic Parity, which states the proportion of people in each demographic group classifed in the positive class should be equivalent [30]. The Neural Network did not receive any post-processing modifcations. Each of the models were evaluated using the evaluation test set.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Interview Protocol", "text": "Each study took approximately one hour. Studies were conducted virtually via video conferencing. Meetings were video recorded with the participant's permission and later transcribed. Participants began with a brief Qualtrics pre-survey that allowed them to share a few details about their roles and the type of work they focus on. From there, participants began the simulated experience. They were told they would serve as the decision-maker for their team and told that a client was requesting a predictor that could determine whether or not an individual made over $50K. They were introduced to the data and shown performance output for the evaluation test set. A depiction of the results shown to the participants can be seen in Figure 1. Using the information given thus far, participants were asked their willingness to deploy each of the models to the client. Next, the participants followed a link from the survey into one of the two Colaboratory notebooks, one for each Fairness toolkit. They were told that this notebook consists of analysis done by one of their team members and they should use these results to make a fnal decision on whether or not they would like to deploy these models. Each notebook is broken down into two parts. The frst part, the Data Exploration Phase, allows a more in-depth picture of the data and the models, while the second part of the notebook, the Fairness Exploration phase, utilizes its respective fairness toolkit to complete fairness analysis. For each part of the notebook, users were given three questions that they must answer in the survey. These questions served multiple purposes: to compare insight and confdence when users query common data science tools like confusion matrices versus fairness toolkits and to give participants an opportunity to dig deep into the data, models, and visualizations. For each question, users provided their answer, their confdence in their answer, and can detail what type of support would strengthen their confdence. Once the six questions were complete, participants were then asked to verbalize the strengths and weaknesses they noticed for each model. From there, they were asked again their willingness to deploy each of the models. Then they completed some post-survey questions about their prior level of exposure, interests, and opinions on their respective fairness toolkit. Additional time left at the end of any session was used by the researcher to query participants about their experience, their opinions on the toolkit, and decisions they made during the study. Figure 2 provides an image depiction of the interview protocol procedures.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "RESULTS & EVALUATION RUBRIC", "text": "In combination with the plethora of literature research studying the key features of fairness and the design needs of practitioners, the aim of the study was to support the creation of a rubric that could be used by fairness experts, practitioners, and organizational institutions alike to design, build, and evaluate the efectiveness of fair ML toolkits. Using the results from the user study and literature analysis, we created a list of needs for future fair ML assessment tools. Here we present our results along with the recommended criteria in the evaluation rubric. The rubric is composed of two parts: the frst part of the rubric is about enabling analysis on ML and the second part emphasizes criteria that assists with improving the tooling. The criteria and short descriptions can be seen in Table 2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Criteria for Supporting Fairness Analysis", "text": "Criteria on fairness receives much of its support from the abundance of algorithmic bias-based literature from the last decade. Much research has been done on how bias can infuence models, which formulas can detect bias, and what methods can be used to mitigate bias [7,11,21,29,45,50,54]. It is critical that frst and foremost, fairness toolkits are equipped with the basic functionalities emphasized by researchers when it comes to fairness. Fairness experts should work to include as many diverse options for the following criteria, while organizational units and practitioners should make sure that the options provided satisfy their needs.\n4.1.1 Applicable to a diverse range of predictive tasks, data types, and models. The toolkits used in this study focus on binary/multiclass problems. Many prominent toolkits are similar in this respect. However, this is not representative of the models being used in practice. Practitioners that focus on ranking, recommendation tasks, or speech synthesis found that these tools would not be easily applicable, if at all. Furthermore, P18 mentioned that group fairness was not as important as individual fairness for their work. While there exists fairness frameworks for non-traditional, non-classifcation problems [11,45,54], the future of fairness most defnitely requires toolkits versatile to a diverse range of machine learning tasks. The types and applications of machine learning are extensive and always growing [29]. While supervised learning is the most exemplifed in fairness literature, it is not at all generalizable to the types of models that exists that require fairness consideration. Whether it be through expanding toolkit functionality or exemplifying this extensive functionality through demos, fairness experts must demonstrate to practitioners that their toolkits are diverse in the types of models, data and predictive tasks they are given. Fair-aware methodologies for ranking, clustering, and embedding [11,45,54] should also be included in these toolkits. Furthermore, there should be fairness analysis options for individuals without access to sensitive features, which was a sentiment from the practitioners in this study, as well as [27]. As new methodologies for bias detection and mitigation are proposed, fair ML toolkits should work to include a diverse set of research that covers the most diverse set of existing technologies.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Detecting & Mitigating bias.", "text": "Fair ML tools focus on both the detection and mitigation of bias. While these two steps are separate, it is important that users are supported for both options [27]. Some participants within this study expressed that the process of detecting bias was complicated and that detection introduces more questions than it provides answers. This can be frustrating for practitioners as it forces them to search for their own solutions blindly. An optimal tool would, at the very least, provide recommendations for how users can mitigate the biases in their models. Furthermore, tools should be able to detect various forms of bias. from many fairness toolkits, proxy/association bias to identify proxy Mehrabi et al. and Olteanu et al. defne a plethora of biases that sensitive attributes is mostly missing from fairness toolkits. An can exist in the data and models [38,43]. Of particular importance, optimal fairness framework would provide users explicit feedback representation bias, sampling bias, and proxy/association bias are on diferent forms of biases. all biases that participants discussed wanting to pay particular 4.1.3 Intervening at diferent stages of the ML life-cycle. The efects attention to. While sampling and representation bias can be deduced of algorithmic bias can occur at almost any point in the ML lifecycle,  [7,21,50]. Considering that some practitioners have more fexibility at diferent stages of their ML pipelines, it is important that options be made available across the board.\nUsers in this study, as seen in previous work [27], said that intervention in the data collection and pre-processing phase was the most important to them. However, few toolkits interrogate the methods used during this phase. At the very least, recommendations should be made on the impact of pre-processing techniques, like binning and feature engineering, on fairness. Furthermore, more options should be available to allow the users to investigate the distribution of their data and easily identify sampling issues.\nCurrently, some toolkits do provide options for during-training and post-training modifcations [9,39], which are also pivotal features to make available to students. Fairness experts should ensure that diverse, customizable options are provided and practitioners and institutions should ensure that their selected toolkit provides the interventions critical for their work.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "4.1.4", "text": "Fairness & Performance Criteria Agnostic. Despite the comparatively small body of literature centered around algorithmic bias, translations on fairness are plentiful. Narayanan identifed 21 defnitions for fairness [40], and with the introduction of fairness into new applications of ML, this number will continue to grow. Several authors have identifed general fairness categories within fairness research [8,38,52]. Users must identify the fairness defnitions that best satisfy their interpretations of fairness given their context.\nVerma & Rubin classifed fairness defnitions into fve categories: those that focus on predicted outcome, those that focus on predicted and actual outcome, those based on predicted probability and actual outcome, those based on similarity measures, and those based on causal reasoning [52]. Currently, most fairness metrics provided by toolkits focus on the defnitions that are based on predicted and actual outcome. These metrics for some applications, however, can be counter-productive [13]. Furthermore, this widely limits the varying fairness defnitions that practitioners could focus on satisfying. While an optimal tool would allow for custom fairness metrics, it would also provide by default a representative subset of metrics.\nSimilarly to the fairness metrics criteria, performance metrics should be customizable and diverse. A useful feature, as suggested by P2, would allow users to do fairness-performance trade-of analysis between models.\n4.1.5 Provides intersectional analysis. Several participants suggested intersectional analysis, which is a feature easily accessible in Fairness Indicators and completely feasible in Aequitas. Intersectional analysis across attributes is a highly under-emphasized practice [16,26]. Fairness toolkits should encourage and have easily accessible subgroup analyses. Furthermore, fairness experts should be diligent about the means for which subgroup performance is displayed to the user. The number of groups multiply and, therefore, the information load required is substantially higher.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Criteria for Usable Fairness Tooling", "text": "While the fairness criteria was mostly defned by fairness experts, it is critical that practitioner feedback also be incorporated into the development and evaluation of fairness toolkits. While user suggestions are still critical in the former section, this section relies heavily on user feedback to support the usability and efectiveness of the toolkit. As previous work [27,34,35,37,50] suggests, practitioner feedback is of the utmost importance for successful and meaningful fairness impact.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Contextualizes", "text": "Fairness. One of the frst tasks users have, when engaging with fair ML tooling, is their decision on an acceptable defnition of fairness, which we have found to be incredibly difcult and highly contextual. When asked to re-assess their willingness to deploy, many participants vocalized the overwhelming procedure of assessing fairness. Comments like, \"This is a lot to decompose\" (P3), \"There was more information than I thought\" (P4), and \"There's so many stats here\" (P6) confrmed that for both tools, users were overwhelmed by the number of fairness metrics and scores presented to them. When discussing the fairness metrics presented, P2 said, \"It was hard to know like, 'okay, what one should I be looking at?' Obviously, it's good to have them all there but it was a little hard to like parse out like, 'oh, what exactly would be the ones to isolate to know what would be the best [in terms of] fairness'. \"\nFor both Fairness Indicators and Aequitas, fairness charts are displayed across a wide variety of fairness metrics. Fairness Indicators as a widget allows for immediate control of which fairness charts can be visualized, giving users more control of the immediate information load. For Aequitas, users have control of which metrics are printed, but they must re-run scripts to change the visualizations that they see.\nWhile the number of metrics available present an overwhelming task, the fairness metrics themselves also contribute to this information overload. Metrics available in these tools require frequent interpretation in the evaluation process. Participants unknowingly had been asked to juggle the mathematical defnition of metrics they rarely worked with, the contextual scenario for interpreting these metrics, and the potential harms that could results from a high, low, or uneven metric score. In the end, this task carried too much weight, so participants decided to go one of two directions: concluding on unfairness by group disparities across all metrics or by looking at one or two fairness metrics. P4 verbalized this same process of transitioning procedures during the analysis, \"I looked at all fairness metrics together. Spent a lot of time looking at frst one, but I felt more confdence when I looked across the board. \"\nCurrently, the overload itself deterred participants from wanting to engage with these tools. P6 said, \"It's unlikely I would use this tool, To be honest, it's a lot of work to use it. \"\nThere is space for much work to be done on fairness tools in reducing the information overload for users. Providing contextualization within the fairness output could be promising. Furthermore, allowing users to share and store these 'harm scenarios' frees some of the mental capacity needed to actually weigh scenarios and make decisions.\nThe importance of contextualization can be seen in anthropology research [5], user experience research [48], and algorithmic bias research [41]. In this situation, contextualizing fairness most similar aligns with work done in information science research, where if given additional information about the task, systems can automate contextualizing components [10]. Simple support in the form of contextualization could be incredibly benefcial to fair ML toolkits.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "4.2.2", "text": "Provides a global and local perspective of fairness and diversifies explanation styles. Individuals enjoyed the visual demonstration of scores, which P4 described as \"self-explanatory. \" One participant (P6) did mention that it would be helpful if visual components were accompanied by separate tables. While Aequitas ofers this option, external tables cannot be extracted for the Fairness Indicators widget. Another participant (P2) suggested a diferent type of visualization where users could compare metrics against each other. For example, P2 said, \"I think it could be interesting to look at the accuracy Fairness trade-of. So that you just have a better sense of like, what are you giving up in accuracy as the model is enforcing these Fairness metrics?\" P2 also noted that it could be benefcial if fairness defnitions/equations were incorporated into the fairness widget for easy access.\nThe interactive component for Fairness Indicators was helpful to many participants as it allowed them to explore and compare metrics. A few participants (P1, P2, P6, P9, P16) who saw this tool wanted functionalities that would allow for more easy comparisons between metrics. P6 suggested for the placement of selected metrics, \"Like having them in one graph would make it exponentially easier to look through, kind of like being able to discern better between the diferent slices and models. \"\nIn the study, the Fairness Indicators widget was also accompanied by Google's TFX [1] statistical visualization widget, which allows users to visualize distributions of the data across attributes. Many participants referenced this widget when trying to understand group fairness scores. On the other hand, Aequitas participants often mentioned the need for looking at the data more to better understand what the tool was outputting.\nAs Dodge et al. found in their work, diferent explanation styles can be seen as inherently more fair than other styles [18]. Furthermore, Arya et al. emphasized the importance of diverse explanation styles [6]. It is critical that alternative depictions of fairness scores be presented. Many existing tools currently focus on a more global outlook of fairness, identifying group patterns or trends. Local depictions of metrics can be helpful in exemplifying global trends [18,46]. These results are also emphasized through the study. Participants exhibited signs of information overload and mistrust that could be assisted by the support of diverse explanation types for fairness outcomes. [27,31,33]. The impact of fair ML toolkits in this study is evident when comparing willingness to deploy models before and after fairness analysis as was seen in Figure 4. Logistic Regression received extensive support because the fair ML toolkits deemed it less 'unfair'. Nonetheless, it still performed, with respect to performance metrics, as poorly as it had at the start of the study. This displays that over-reliance is most defnitely a concern with these technologies and, therefore, they should be accompanied by preambles that detail the limitations and dissuade over-reliance, as was shown in [37].", "n_publication_ref": 9, "n_figure_ref": 1}, {"heading": "Provides explicit interpretation of limitations. Previous work has been extensive when it comes to over-reliance and over-trust of FAccT technologies", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Provides fairness recommendations.", "text": "For Aequitas, participants were shown fairness plots color-coded green and red where colors represented whether or not, respectively, group scores satisfed fairness determination for that metric. Many participants enjoyed this color coding as it provided recommendations for metrics or groups that might be of concern. P3, P11, and P20 all mentioned that it would be nice if equations for how those values are calculated were included in the visuals. Relatively, P16, who saw the Fairness Indicators tool, suggested that their tool include green and red color-coding for recommendations on fairness. Participants from previous work [27] also emphasized a need for support beyond fairness detection. Recommendations can come in many forms. While mitigation techniques are very helpful, text-based recommendations for handling diferent types of bias is also preferential to bias detection alone.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Well-Supported by Demos & Tutorials.", "text": "Within the post-survey, participants were asked about their prior level of exposure, interests, and opinions on their respective fairness toolkit. The majority of participants self-reported low or moderate levels of familiarity with fair ML and high interests toward using fair AI. Participants also had the opportunity to share their feedback on methodologies for learning how to use a fairness toolkit and what fairness interventions are best for them and their type of work. Of particular importance, Figure 3 depicts the diference of opinion in preferred fairness learning methodologies across product-focused practitioners and research-focused practitioners. This delineation of need when it comes to learning how to use fairness methodologies should be of utmost importance to both fairness experts and institutions wishing to normalize fairness procedures.\nOne important consideration is how fairness is demonstrated to practitioners. For this study, the task was binary. Similarly, demos for many fairness toolkits are simplifed problems. These demos seem to be infuential to practitioners when it comes to assessing the capabilities of fairness toolkits. For the purpose of this study, P19 had seen the Aequitas tool, but also shared that they had attended a demo of the Fairness Indicators tool. Of interest, P19 recalled about Fairness Indicators, \"I don't know how relevant it is to our domain if it is like very specifc to words. \"\nFairness Indicators often uses the Jigsaw's Unintended Bias in Toxicity dataset [12] as its interactive case study. This presumably is the same demo that P19 saw and attributed to Fairness Indicators capabilities. This same pattern emerged as people were exposed to fair ML toolkits for the frst time using the Adult Census data. P7 said about their fairness toolkit, \"I feel like these are more for comparing prototypes. It seems like its for simpler models.\" As was depicted in Figure 3, practitioners rank highly demos and case studies as a means of learning how to use fair ML toolkits. It seems logical that they would also use these methods to understand the functionalities and capabilities of these tools as well. Therefore, fairness experts must be intentional in the amount of time and efort they place on these supportive documentation. Additionally, a diverse set of documentation must be provided to supplement the diverse needs of diferent practitioners (reference Figure 3.\nFurthermore, participants in this study had the opportunity to rank learning methods. These results, similar to the results of Holstein et al. [27] emphasize that users require domain-specifc guides exemplifying fairness analysis across applications. Participants struggled to consider how fairness might work in their feld, and supported through extensive documentation, tutorials, and demos is incredibly benefcial to practitioners. Furthermore, open lines of communication between fair ML toolkits developers and practitioners is helpful, as well.", "n_publication_ref": 2, "n_figure_ref": 3}, {"heading": "Incorporates Components from other FAccT technologies.", "text": "For improving the tool, participants often suggested features that can be seen in other FAccT toolkits. P2 and P7 both mentioned a similar feature they thought would be benefcial. \"I would like to see a tool that could simulate how you could improve a model using fairness considerations\" (P7). Currently, Google's What-If widget [24] provides this exact functionality. While this study focused on bias detection, suggestions for bias mitigation are defnitely useful features for any fairness toolkit.\nOne major suggestion from users demonstrates features that you might see in an explainability toolkit. Several participants (P7, P8, P10, P11, P19) suggested that additional feature analysis be a part of the fairness consideration process. Since sensitive attributes can be refected in other attributes, participants wanted to see what Figure 3: Ranking of fair ML Learning tools between product-focused practitioners and research-focused practitioners depicts that there are delineations between the needs of these groups even when it comes to learning about fair ML toolkits. Black diamonds demonstrate outlier points and green triangles mark the mean for each group.\nfeatures are guiding performance. A tool like LIME [46] would integrate well for this type of feature.\nA small group of participants verbalized feelings of mistrust towards these tools. P10 even stated, \"I'm not super like convinced by these plots to be fair. Like I feel it raises-I mean it's maybe interesting you know just to kind of get a feel for you know how things are stacking up against each other. But I feel it raises more questions than it really answers but maybe that's the [point]. \"\nFor fairness experts to garner the trust they need from practitioners, as much support should be provided to the toolkit. As suggested above, support in the form of help icons with defnitions and equations and explanations could provide support to practitioners to better understand the information being shown. Furthermore, the implementation of explainability or interpretability could allow users to become more trusting of the results. Dodge et al. [18] found that intersecting fairness with explainability impacted user's perceptions on fairness outcomes, with some explanation types being more infuential than others. 4.2.7 General ease of use guidelines. Users were asked 6 questions that gauged their understanding of the data, the models, and the fairness toolkits. For these questions, Wilcoxon test comparisons across the phases and Mann-Whitney test comparisons across the two diferent fairness toolkits were analyzed. There were no statistically signifcant diferences between outcomes with respect to correctness, confdence, and timeliness (p<0.05), however this is not conclusive in light of the limited sample size. Comparisons between toolkits can be seen in Table 3. When comparing time between the Data Exploration Phase and the Fairness Exploration Phase, there was a signifcant diference in time taken by users to answer questions (Mann-Whitney, U=21.0, p<0.01), with the latter phase taking substantially longer than the former.\nThese results depict that participants were able to successfully gain basic insight with high confdence. Understandably, when it came to engaging with new fairness visualizations and metrics for the frst time, users took more time to come to conclusions. However, with the high confdence scores across the tools, participants felt as though they were using and reading these tools successfully.\nOne participant with experience with fairness toolkits (P16), noted that having seen Fairness Indicators for the frst time as a fairness expert, they would presume that this would be a difcult analysis for those without experience. P16 noted, \"I know about these rates but I felt that there was not a lot of components for educating the user. \" Despite fair ML having been a relatively specialized set of technologies, designed and used by a somewhat specifc group of people, their broader adoption will still require an increased focus on usability of the tools that implement them. With the lack of enforcement of fairness considerations, many users are volunteering to use fair ML and could, therefore, be easily dissuaded from doing so by poor design decisions. In this study, a few participants were discouraged from using fair ML when features did not work as expected or interfaces were too difcult to use. Some participants even discussed preferring to do the analysis on their own over using readily avail-counteract any bias they might encounter. Furthermore, fair ML able tools because of the difculty in navigating or understanding toolkits should be infuential on practitioners and stakeholders. P7 output from the tools. mentioned that their respective tool would be useful for convincing stakeholders of fairness considerations,", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Fairness Indicators Aequitas", "text": "Average Correctness (0-1) Average Confdence (1-7) Average Time Taken (in seconds) 0.85 5.93 297.00 s 0.93 5.90 309.10 s Table 3: Between the two toolkits that participants saw, there was no signifcant diference for the fairness phase between the correctness in answers, the confdence in answers, and the time taken to use each tool.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Influential on Model Processing.", "text": "Participants were asked both before and after visualizing fairness analysis about their willingness to deploy each of the three models. Figure 4 depicts the change in opinion for each users across each model. Likelihood to deploy was measured on a scale from -3 to +3, with negative -3 being extremely unlikely to deploy and +3 being extremely likely to deploy. When Wilcoxon signed-rank tests were done compare willingness to deploy for each model before and after fairness analysis, outcome was signifcant for both LR (t=9.0, p<0.01) and NN (t=12.5, p<0.01). The average change in willingness for LR, RF, and NN models were 1.20, 0.55, and -1.75, respectively.\nParticipants were generally vocal about the insight brought by using the fairness toolkits and the fairness analysis process. P1 noticed that their willingness to deploy was heavily impacted by the fairness analysis.\n\"[I]f I was going to be looking at this as somebody who's shipping things out, it would defnitely make me think twice, because originally I talked about my neural net having potential to be shipped out, but it's obviously not as great for some groups versus others. \" (P1) Several other participants also took notice of how the tool so clearly changed their perspective. Furthermore, the comparative group performance analysis was new to some. One participant (P5) verbally noted, \"These graphs facilitate my thinking about [fairness] and comparing these models. [...] I wouldn't have thought of these comparisons without seeing this graph. \" The impact of fairness analysis was evident in the change in perspective. While participants were understandably never too enthusiastic to launch models they were unfamiliar with, fairness analysis did provide additional information that was infuential on their decision to deploy. Despite how performance for the LR model was substantially lower than for the NN model, when participants served as the decision-maker in these scenarios most all of them sacrifced traditional performance for 'fairer' outcomes.\nA central goal behind any FAccT tool is to inspire change in the motivations, means, and results of machine learning algorithms. Many of the components of this portion of the Insight and Usability section work towards guaranteeing that, if necessary, practitioners \"I think this has potential as a tool that you can demonstrate to the business side [for] why fairness considerations are not degrading product performance. \" Fairness experts should focus on being interpretable and infuential for both practitioners and their lead decision-makers who may not be ML-experts. If, for any reason, change does not occur, attention must be placed on whether the tool is efectively performing its responsibilities.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "CONCLUSION", "text": "Many of the practitioners in our study vocalized that this was their frst experience considering fairness in a systematic way. For those with similarly formatted data and access to sensitive attributes, the importance and the accessibility of fairness toolkits was made apparent. However, a large number of participants (P2, P3, P4, P6, P8, P9, P11, P18, P19) voiced their concerns on whether or not fairness was applicable to their work. Participants mulled over what fairness would even mean in the context of their domain. While some participants concluded that fairness (defned via group performance) was not relevant to their work, others began to formulate what fairness might look like and consider the impact of their work from a new angle. This discussion of how to defne fairness in context, determine the impact of unfairness, and decide how to respond to it is critical in team settings and the use of fair ML tools in some capacity encourages practitioners to defne fairness in their own domain. Furthermore, the diversity of fairness issues detected and the solutions ofered by practitioners suggests that this discussion might be the most pivotal frst step in incorporating fairness. The blindspots of practitioners still exist [27] and the easiest solution is one that is through collaboration. When comments were consolidated, participants in this study were able to expose more bias and propose more solutions than currently any one toolkit can provide. Therefore, the importance of fairness education for ML practitioners and fairness discussion within ML teams cannot be over-emphasized.\nThe future of fair ML heavily depends on the participation and feedback of practitioners. This feedback informs design needs and ensures fairness tools are used and used efectively. While the fairness needs can be clearly supported by literature, the assurance that tools are usable and allow successful insight must be validated by the practitioners. By engaging with practitioners, this work was able to confrm the importance and impact of fair ML toolkits, elicit the fairness background of potential users, identify methods best suited for fairness learning, and identify need areas in fair ML design. Through feedback from practitioners and support from previous literature, this work introduced a rubric that summarizes critical components necessary for any complete fair ML toolkit.\nFrom this research, we have developed suggestions for institutions, fairness experts, and practitioners. First, institutions should be actively involved in the fairness process. They should incentivize fairness analysis, remain knowledgeable on fairness research, Figure 4: Participants' signifcant change in willingness to deploy before and after fairness analysis depicts the infuential impact that fair ML toolkits has on decision-making. In particular, the Neural Network, which had no fairness processing done on it was the most favored before fairness analysis and the least favored after analysis. Empty circles represent participants starting willingness, while flled circles represent their fnal willingness to deploy. Green is used to depict participants with an increased willingness to deploy, red depicts a decrease, and black demonstrates no change. and provide infrastructure that supports organizational fairness guidelines. Furthermore, they should provide their practitioners with a diverse set of fairness supports that allows for successful learning and application of fair AI. Institutions should ensure that practitioners have all the tools necessary, including access to fairness experts and domain-specifc guides to fairness research. This rubric can be used by the actors within institutions in selecting the toolkit(s) that best ft the needs of their practitioners.\nSecond, fairness experts could fll the existing gap in the development of a more complete 'One Stop Shop' toolkit for fairness. This could be best demonstrated as an open-source fairness toolkit where the community of fairness experts could incorporate their contributions while also providing practitioners an easy means of critiquing. Fairness experts should diversify their toolkits to consider the wide variety of tasks that exists across the ML space. In addition, critical fairness components like recommendations for mitigation and providing fairness interventions for early ML stages are currently missing from many toolkits. Furthermore, several features that could guide the fairness decision-making process and reduce information overload for practitioners are missing.\nThird, practitioners should also be active participants in implementing fairness into their work spaces. They should be diligent in learning and selecting the tool that best fts their work. Practitioners interested in fair ML should advocate for its implementation in their teams and projects. This support can be incentived by toolkit creators and institutions. Furthermore, the fairness research community is still fairly new and feedback and support is encouraged for most all technologies. This open line of communication should be made apparent by toolkit creators and used often between practitioners and fairness experts.\nThis work exemplifes what we believe could be the beginning of a new era of fair ML. As can be seen in the design of high-stakes products, fair ML should undergo iterative design procedures that actively engage practitioners and stakeholders. Future work will focus on a wider array of practitioners, especially those with medial to low interest in fairness. Furthermore, the results of this work should be used to inform a more complete set of fair ML that can be incorporated into high-stake product and research teams.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Limitations.", "text": "Limitations of the study include the sample size, participation bias, and available tooling. Future work can focus on a larger subset of practitioners from a more diverse background with respect to institution afliation and interests in fair ML. Furthermore, without social distancing restrictions, diferent techniques, such as participatory design or in-person group discussions about fairness, could be used to further unpack how practitioners would use tools in diferent environments. Furthermore, this study did not do a comparative analysis of tool features, so future work would focus on deciphering the strengths and weaknesses of toolkit features. Finally, the current study methodology did not focus on isolating difering translations of fairness. Future work would interrogate how tooling impacts an individual's defnition of fairness in context.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "Firstly, we want to send a huge thanks to our participants for taking the time to participate in this study. We had such an enthusiastic group of participants who were incredibly excited about the future of this work. Furthermore, we want to send thanks to all the individuals who helped recruit and spread the word. Lastly, we want to thank a few of our awesome team members who helped extensively along the way, including: Nediyana Daskalova, Ziang Xiao, and Lex Beattie.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "TensorFlow Extended (TFX) | ML Production Pipelines", "journal": "", "year": "2020", "authors": ""}, {"title": "Tensorfow's Fairness Evaluation and Visualization Toolkit", "journal": "", "year": "2020", "authors": ""}, {"title": "", "journal": "ACM. 2020. ACM FAccT", "year": "", "authors": ""}, {"title": "Black Box Fairness Testing of Machine Learning Models", "journal": "", "year": "2019", "authors": "Aniya Aggarwal; Seema Nagar; Diptikalyan Saha"}, {"title": "Anthropological refections on descriptive analysis, its limitations and implications", "journal": "Anthropology and Medicine", "year": "2014-09", "authors": "Rikke Sand Andersen; Mette Bech Ris\u00f8r"}, {"title": "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques", "journal": "", "year": "", "authors": "Vijay Arya; K E Rachel; Yu Bellamy; Amit Chen; Michael Dhurandhar;  Hind; C Samuel; Stephanie Hofman; Vera Houde; Ronny Liao; Aleksandra Luss; Sami Mojsilovi\u0107; Pablo Mourad; Ramya Pedemonte; John Raghavendra; Prasanna Richards; Karthikeyan Sattigeri; Moninder Shanmugam;  Singh; R Kush; Dennis Varshney; Yunfeng Wei;  Zhang"}, {"title": "Understand, Manage, and Prevent Algorithmic Bias: A Guide for Business Users and Data Scientists", "journal": "Apress", "year": "2019", "authors": "Tobias Baer"}, {"title": "Fairness and machine learning", "journal": "", "year": "2019", "authors": "Solon Barocas; Moritz Hardt; Arvind Narayanan"}, {"title": "AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias", "journal": "", "year": "2018-10", "authors": "K E Rachel; Kuntal Bellamy; Michael Dey;  Hind; C Samuel; Stephanie Hofman; Kalapriya Houde; Pranay Kannan; Jacquelyn Lohia; Sameep Martino; Aleksandra Mehta; Seema Mojsilovic;  Nagar; John Karthikeyan Natesan Ramamurthy; Diptikalyan Richards; Prasanna Saha; Moninder Sattigeri; Kush R Singh; Yunfeng Varshney;  Zhang"}, {"title": "Watson: Anticipating and Contextualizing Information Needs", "journal": "", "year": "1999", "authors": "Jay Budzik; Kristian Hammond"}, {"title": "Fair Clustering Through Fairlets", "journal": "", "year": "2017", "authors": "Flavio Chierichetti; Ravi Kumar; Silvio Lattanzi; Sergei Vassilvitskii"}, {"title": "Jigsaw Unintended Bias in Toxicity Classifcation", "journal": "", "year": "2019", "authors": ""}, {"title": "The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning", "journal": "", "year": "2018-07", "authors": "Sam Corbett; - Davies; Sharad Goel"}, {"title": "Translation, tracks & Data: An algorithmic bias efort in practice", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Henriette Cramer; Sravana Reddy; Romain Takeo Bouyer; Jean Garcia-Gathright; Aaron Springer"}, {"title": "The Trouble with Bias", "journal": "", "year": "2017", "authors": "Kate Crawford"}, {"title": "Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine", "journal": "", "year": "1989", "authors": "Kimberle Crenshaw"}, {"title": "How LinkedIn's search engine may refect a gender bias", "journal": "", "year": "2016", "authors": "Matt Day"}, {"title": "Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment", "journal": "", "year": "2019-01", "authors": "Jonathan Dodge; Q Vera Liao; Yunfeng Zhang; Rachel K E Bellamy; Casey Dugan"}, {"title": "UCI Machine Learning Repository", "journal": "", "year": "2017", "authors": "Dheeru Dua; Casey Graf"}, {"title": "A comparative study of fairness-enhancing interventions in machine learning", "journal": "", "year": "2018-02", "authors": "A Sorelle; Carlos Friedler; Suresh Scheidegger; Sonam Venkatasubramanian; Evan P Choudhary; Derek Hamilton;  Roth"}, {"title": "Big data preprocessing: methods and prospects", "journal": "Big Data Analytics", "year": "2016", "authors": "Salvador Garc\u00eda; Sergio Ram\u00edrez-Gallego; Juli\u00e1n Luengo; Jos\u00e9 Manuel Ben\u00edtez; Francisco Herrera"}, {"title": "Assessing and Addressing Algorithmic Bias -But Before We Get There", "journal": "", "year": "2018", "authors": "Jean Garcia-Gathright; Aaron Springer; Henriette Cramer"}, {"title": "Google Colaboratory", "journal": "", "year": "2020", "authors": " Google"}, {"title": "Google. 2020. What-If Tool", "journal": "", "year": "", "authors": ""}, {"title": "Equality of opportunity in supervised learning", "journal": "", "year": "2016", "authors": "Moritz Hardt; Eric Price; Nati Srebro"}, {"title": "Where fairness fails: data, algorithms, and the limits of antidiscrimination discourse", "journal": "Information, Communication & Society", "year": "2019", "authors": "Anna Lauren Hofmann"}, {"title": "Improving fairness in machine learning systems: What do industry practitioners need", "journal": "", "year": "2018-12", "authors": "Kenneth Holstein; Jennifer Wortman Vaughan; Hal Daum\u00e9; Miro Dud\u00edk; Hanna Wallach"}, {"title": "The global landscape of AI ethics guidelines", "journal": "Nature Machine Intelligence", "year": "2019-09", "authors": "Anna Jobin; Marcello Ienca; Efy Vayena"}, {"title": "Machine learning: Trends, perspectives, and prospects", "journal": "", "year": "2015", "authors": "M I Jordan; T M Mitchell"}, {"title": "Data preprocessing techniques for classifcation without discrimination", "journal": "Knowledge and Information Systems", "year": "2012-10", "authors": "Faisal Kamiran; Toon Calders"}, {"title": "Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning", "journal": "", "year": "2020", "authors": "Harmanpreet Kaur; Harsha Nori; Samuel Jenkins; Rich Caruana; Hanna Wallach; Jennifer Wortman Vaughan"}, {"title": "Adult Data Set", "journal": "", "year": "1996", "authors": "Ronny Kohavi; Barry Becker"}, {"title": "How do I fool you?\": Manipulating User Trust via Misleading Black Box Explanations", "journal": "", "year": "2019-11", "authors": "Himabindu Lakkaraju; Osbert Bastani"}, {"title": "The Impact of Presentation Style on Human-In-The-Loop Detection of Algorithmic Bias", "journal": "", "year": "", "authors": "Po-Ming Law; Sana Malik; Fan Du; Moumita Sinha"}, {"title": "Designing Tools for Semi-Automated Detection of Machine Learning Biases: An Interview Study", "journal": "", "year": "2020", "authors": "Po-Ming Law; Sana Malik; Fan Du; Moumita Sinha"}, {"title": "Bias Mitigation Post-processing for Individual and Group Fairness", "journal": "", "year": "2018-12", "authors": "K Pranay;  Lohia; Manish Karthikeyan Natesan Ramamurthy; Diptikalyan Bhide; Kush R Saha; Ruchir Varshney;  Puri"}, {"title": "Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI", "journal": "", "year": "2020", "authors": "A Michael; Luke Madaio; Jennifer Wortman Stark; Hanna Vaughan;  Wallach"}, {"title": "A Survey on Bias and Fairness in Machine Learning", "journal": "", "year": "2019", "authors": "Ninareh Mehrabi; Fred Morstatter; Nripsuta Saxena; Kristina Lerman; Aram Galstyan"}, {"title": "", "journal": "", "year": "2020", "authors": " Microsoft"}, {"title": "21 fairness defnitions and their politics", "journal": "", "year": "2018", "authors": "Arvind Narayanan"}, {"title": "When eliminating bias isn't fair: Algorithmic reductionism and procedural justice in human resource decisions", "journal": "Organizational Behavior and Human Decision Processes", "year": "2020-09", "authors": "David T Newman; Nathanael J Fast; Derek J Harmon"}, {"title": "Algorithms of Oppression: How Search Engines Reinforce Racism", "journal": "NYU Press", "year": "2018", "authors": "Noble Safya Umoja"}, {"title": "Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries", "journal": "Frontiers in Big Data", "year": "2019-07", "authors": "Alexandra Olteanu; Carlos Castillo; Fernando Diaz; Emre K\u0131c\u0131man"}, {"title": "", "journal": "", "year": "2020", "authors": " Qualtrics"}, {"title": "FairWalk: Towards fair graph embedding", "journal": "", "year": "2019", "authors": "Tahleen Rahman; Bartlomiej Surma; Michael Backes; Yang Zhang"}, {"title": "Explaining the Predictions of Any Classifer", "journal": "", "year": "", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Aequitas: A Bias and Fairness Audit Toolkit", "journal": "", "year": "2018-11", "authors": "Pedro Saleiro; Benedict Kuester; Loren Hinkson; Jesse London; Abby Stevens; Ari Anisfeld; Kit T Rodolfa; Rayid Ghani"}, {"title": "An architecture for contextualized learning experiences", "journal": "", "year": "2006", "authors": "Marcus Specht; Andreas Lorenz; Andreas Zimmermann"}, {"title": "Addressing bias in large-scale AI applications: The LinkedIn Fairness Toolkit", "journal": "", "year": "2020", "authors": "Sriram Vasudevan; Cyrus Diciccio; Kinjal Basu"}, {"title": "Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data", "journal": "Big Data & Society", "year": "2017", "authors": "Michael Veale; Reuben Binns"}, {"title": "Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making", "journal": "", "year": "2018-02", "authors": "Michael Veale; Max Van Kleek; Reuben Binns"}, {"title": "Fairness Defnitions Explained", "journal": "IEEE/ACM International Workshop on Software Fairness", "year": "2018", "authors": "Sahil Verma; Julia Rubin"}, {"title": "Predictive Inequity in Object Detection", "journal": "", "year": "2019-02", "authors": "Benjamin Wilson; Judy Hofman; Jamie Morgenstern"}, {"title": "Mohamed Megahed, and Ricardo Baeza-Yates", "journal": "Association for Computing Machinery", "year": "2017", "authors": "Meike Zehlike; Francesco Bonchi; Carlos Castillo; Sara Hajian"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Participants were initially shown this performance output between the three models, as well as, a ROC curve that can be seen in the Appendix.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Interview Protocol Methods. Participants underwent several stages of surveying, exploration, and query responding. The above chart depicts the fow of events each participant underwent in the hour-long session. A random and equal distribution of participants interacted with the Aequitas tool and with the Fairness Indicators tool.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Rubric. Considerations based on fairness, usability, and insight criteria that can be used to both select and build the Provides users with a comprehensive perspective on its limitations", "figure_data": "Provides fairness recommendationsProvides optional recommendations for acceptable levels of deviationfrom fairness criteriaWell-Supported by Demos & TutorialsUsers should be able to apply, with little struggle, these tools in theircontextIncorporates Components from other FAccT technologiesUtilizes components from other Explainable, Interpretable AI to sup-port fairness analysisGeneral ease of use guidelinesUsers do not struggle with interpreting, understanding, and manipu-lating toolInfuential on subsequent model processingSuccessfully demonstrates trends that are infuential in users' nextstepsfrom the decision of what data to collect to build the model to thelocations and communities that are impacted by the fnished anddeployed model"}], "doi": "10.1145/3411764.3445604"}