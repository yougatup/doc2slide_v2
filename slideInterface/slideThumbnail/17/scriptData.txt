
 Good morning, my name is Brianna Richardson and I will be presenting on behalf of my team at Spotify on our recent work in creating a practitioner-oriented rubric for creating and evaluating fair machine learning toolkits.
 The motivations for our work were three-fold: Firstly, There currently exist real and consequential harms that emerge from the unintended biases that exists in our machine learning algorithms. To combat these harms, there exists a diverse array of tools and resources that have been made available to machine learning practitioners in the Fair AI research space. Nonetheless, there is an evident lack of application of these resources in practice.
 Our objective for this project was to identify the features that would bridge the gap and make Fair AI tools work in practice. Therefore, we decided to build a rubric that could be used as both a guide and an evaluation tool for the creation of Fair AI tooling. To build this rubric, we took the recommendations of fairness experts in literature and practitioners in the workplace.
 In addition to a complete literature review on the solution space of fair AI, we wanted to also collect feedback from machine learning practitioners. So we conducted a study with 20 participants, all of whom were individuals who were affiliated to machine learning projects in some capacity. Our participant pool was mixed with both product and research oriented practitioners from a diverse array of domains. The study was done virtually using the video conferencing platform, Google Meet, and took about an hour to complete.
 After collecting demographic information, interviewers introduced participants to the study task. Participants were told that they would be the project manager for a new product that predicts whether or not an individual makes over 50k.
 They were told that their team had produced three different models and that they would be using a fairness toolkit to evaluate the three models.
 Participants had the opportunity to decide their willingness to deploy each of the models before and after viewing fairness analysis. Each participant was shown one of two fairness toolkits: UChicago’s Aequitas or Google’s Fairness Indicators.
 To start, participants were shown common performance metrics for each model in the form of confusion matrices. Using this information alone, participants shared their willingness to deploy each of these models. Next, participants were shown completed analyses

 in Google Colab. The fairness portion of this analysis can be seen in the videos shown here. On the left, you can see what the fairness analysis done in Aequitas and on the right, you can see the fairness analysis done in Google’s Fairness Indicators. After spending time with the analyses sections, participants were then asked again about their willingness
 to deploy each model. Our results were extensive, just to name a few: we found that the impact of fairness tooling was clear: fairness toolkits were significantly influential on user’s willingness to deploy the models.
 Furthermore, there was a clear need for diverse resources to assist in the use of toolkits. Practitioners stated a need for fairness collaborations and colleagues, access to source code for fairness tools, company-led training courses, etc.
 Our main contribution was a comprehensive rubric that lists the fairness and usability needs as requested by practitioners when it comes to fairness toolkits. This rubric can be used to identify critical components for any Fair ML toolkit.
 Two of these rubric components can be seen here. Firstly, many of the interviewed practitioners found the tools to be inapplicable to their own machine learning algorithms. Either the task of classification or the use of tabulated data were irrelevant for them. Since many tools are very limited in the predictive tasks or the allowable data types, there is a dire need for diversification of these toolkits. Another major complaint was the lack of guidance or recommendations for when and how outcomes were unfair. Many practitioners voiced that the experience of deciphering unfairness was exhaustive and requested as much support as possible within the toolkit for determining where and how bias may sneak into their models. To see more about these rubric components in more detail, please check out our paper in the conference’s proceedings.
 The results depict a dire need for improvement when it comes to the diverse problem sets that can be handled by Fair ML, the usability of Fair ML, and the additional resources provided to support the use of the fair ML. However, the responsibility for normalizing the use of fair ML in practice does not solely lie in the hands of fairness experts. Institutions should be active supporters of fair AI pursuits. They should provide resources for educating practitioners and should define their own fairness expectations. Next steps should focus on prototyping this improved fair ML.
 Thanks for attending our talk. Please feel free to reach out to me at any point. I look forwarding to hearing your questions and comments.
