
 Our paper is Designing Ground Truth and the Social Life of Labels, from a team of researchers at IBM.
 In Machine Learning, people try to predict an outcome variable, called the Ground Truth, on the basis of multiple predictor variables, or Features. The prediction takes place in a Data Science Pipeline
 which typically involves extensive data preparation or data wrangling, as well as modeling and quality assurance.
 In Human Centered Data Science, we want to populate that pipeline by adding the people who are doing the work, to learn about their uniquely human contributions to that work. In some cases, the outcome is not “given” in the data, and so we have to ask human domain experts to apply ground truth labels, which tell us what the human believes the outcome is. Their labels might represent a medical diagnosis, or a business characterization of a social media post, or a user’s intention when talking to a chatbot.
 We interviewed 15 people in IBM who worked in the area of ground truth labeling, And we applied grounded theory methods to make sense of what they told us.
 In the full paper, we report on seven emergent concepts that we found through our grounded analysis. For this brief presentation, we focus on the last of the seven concepts, which is how people designed ground truth labels. We identified three major patterns in the design of ground truth.
 In Principled Design, people had a plan, and they carried out their plan, and there were few surprises. In medical labeling, the labeling vocabulary may be based on “diseases, conditions, symptoms, medication...” The labelers already knew what they were looking for, and they labeled strictly according to that prescribed vocabulary. In a chatbot project, labelers knew that there were exactly 30 actions that the chatbot could take, and therefore they could apply exactly 30 labels to end-users’ input.
 In the second pattern, Iterative Design, the teams anticipated the need to revise their labels as they did the labeling work, and they included revisions as part of their process. In a different chatbot project, the team anticipated iterations: “We would cold-label [the] conversations, evaluate what we did, discuss what went well, what went poorly... And then go back and do it again.” One informant was teaching an algorithm to recognize types of structured documents. They noted that, “the tool generates examples [where] the system is uncertain [what] the labels should be... After additional labeling from the user, the system refines the rules.” Clearly, the automated tool needed human correction, and they built those human correction processes into their plan.
 The third pattern we called Improvisational Design of ground truth, in which people encountered unexpected challenges to their labeling activities. One intrinsic problem was that people didn’t agree on the labels. Another problem was complexity. In a social media labeling project, the informant told us: ”We ended up doing the simpler ones, because... this is... more explainable, less time-consuming.” Significantly, this team had to exclude certain complex data types, and thus their labeling outcome described only a subset of their data records. Extrinsic challenges often came from clients or the terms of a contract. An informant reflected on how they modified their labeling vocabulary to meet their client’s expectations. ”Client was happy... If we had more time, more data, more annotations, we could have done a lot better. We usually work on 6-week engagements.” In these examples, we see that the scope and the nature of labels in Improvisational Design may be constrained by unanticipated challenges.
 Ground truth labels may be determined in advance, or may be malleable, changeable, and – importantly – socially negotiable. People do the work of ground-truth labeling, and they bring human strengths and human weaknesses to that work. And they or their organizations become accountable for that work. Far from the conventional interpretation of the word “truth,” we learned to think of ground truth as a worthwhile but necessarily social achievement.
 Briefly, we learned some lessons for tools to support labeling or annotating data: Those tools should provide features for human consensus, social convergence, and accountability. We learned to think about data as a crafted medium of design, and we learned that the design of data depends very much on who is describing the data, and what kinds of “data vision” they bring to that task. And we realized that, after labeling is done, the ground truth labels tend to vanish into the infrastructure, and become merely “the truth” when the modelers compute predictions based on the labeled dataset.
 The design of ground truth is a human process. We will do better data science if we are aware of how humans necessarily intervene to contribute human discernment to the data that we analyze. Thank you.
