{"authors": "Michael Muller; Christine T Wolf; Josh Andres; Narendra Nath; Michael Desmond; Aabhas Sharma; Kristina Brimijoin; Evelyn Duesterwald; Casey Dugan; Zahra Ashktorab;  Qian", "pub_date": "", "title": "Designing Ground Truth and the Social Life of Labels", "abstract": "Ground-truth labeling is an important activity in machine learning. Many studies have examined how crowdworkers apply labels to records in machine learning datasets. However, there have been few studies that have examined the work of domain experts when their knowledge and expertise are needed to apply labels. We provide a grounded account of the work of labeling teams with domain experts, including the experiences of labeling, collaborative confgurations and work-practices, and quality issues. We show three major patterns in the social design of ground truth data: Principled design, Iterative design, and Improvisational design. We interpret our results through theories of from Human Centered Data Science, and particularly work on human interventions in data science work through the design and creation of data.\u2022 Human-centered computing \u2192 Collaborative and social computing; \u2022 Computing methodologies \u2192 Machine learning approaches.", "sections": [{"heading": "INTRODUCTION", "text": "There is no \"the truth,\" \"a truth\" -truth is not one thing, or even a system. It is an increasing complexity. -Adrienne Rich [84] The demand for Machine Learning (ML) models continues to grow as companies aim to beneft from cognitive services that harness large amounts of data in providing data-driven machine action [78,121]. ML models arise in all areas of our lives, from optimizing our commute paths, assisting in work decision-making practices, to recommending entertainment options and suggesting dating partners e.g., [3,78,99]. Building ML models involves data practices in a number of data phases along the data science pipeline [5,114]. These data practices require efort and care [120]. Data science involves craftful, engineering practices, and has been explored in a number of dimensions [30,69,114,116,123].\nHowever, ML models don't work without human engagement [7,43,91]. As we think about organizations' needs for models, we must also think about the people who do the work to make the models work [4,6,57,72], and to engage in critical examinations of data science work-practices and discussions [30,31,68,70,122]. In this paper, we focus on an area of human work in modeling that has received relatively little attention -the collaborative workpractices to organize the work of labeling data, or more precisely, labeling ground truth data for use in those ML models. We show the difcult and collaborative work of data science workers to manage the design of labels that work for ML projects. We examine workpractices that are involved in the social production of ground truth. While labeling may seem straightforward, a closer look reveals its situated and emergent character [41].\nIn most accounts of supervised (machine) learning, the ground truth is considered to be the \"dependent variable\" that is predicted by a collection of features (independent variables) [26,58,78]. Each record contains the features along with the value of the ground truth. The value of the ground truth in each record is referred to as a \"label\" (in machine learning projects) or an \"annotation\" (in linguistic and medical projects). We will say \"label\" for simplicity.\nGround truth is an important and usually human contribution to the dataset. It is important to understand how ground truth is added to a dataset, and therefore we need to understand how humans collectively make that contribution. We will show how humans organize their work to design and create ground truth labels. We will describe their collaborations, their concerns about quality, and their mitigations of those concerns. Through a grounded analysis of interviews with 15 data science workers, we describe three major patterns in this work:\n\u2022 The Principled design of ground truth as an unfolding of a planned process. \u2022 The Iterative design of ground truth as a series of anticipated clarifcations and redefnitions. \u2022 The Improvisational design of ground truth as a series of surprises and repairs.\nThroughout these descriptions, we highlight the diligence, responsibility, and ingenuity of the humans who do this work.", "n_publication_ref": 30, "n_figure_ref": 0}, {"heading": "RELATED WORK 2.1 The Need for Labeling in Data Science", "text": "As the use of machine learning has grown, so has the need for labeled data. The process of labeling data to train supervised machine learning algorithms 1 is often an expensive and labor-intensive process, requiring humans to analyze and annotate large amounts of data in a repetitive manner [82]. Labeling is part of the preparation of data for analysis, and includes cleaning the data, removing outliers, transforming quantitative features to strengthen statistical analyses, and making non-linear combinations of features to represent additional data-derived concepts [85]. Workers in data science have described data preparation -often called \"data wrangling\" -as laborious, taking up to 80% of the time and efort of a data science project [40,46,53,82,107]. Sutton et al. referred to the problem of extensive data-preparation work as \"death by a thousand wranglings\" [107]. We study the labeling of ground truth data because it is part of this time-consuming process. Improvements in labeling would reduce the \"thousand wranglings\" down to a more manageable workload, and would make projects easier to perform, with better use of humans' time.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Crowdsourcing.", "text": "Crowdsourcing has emerged as one way to address this challenge, by distributing labeling tasks to an army of anonymous low-paid digital workers -what Gray and Suri [43] call \"ghost work. \" Crowdsourcing is widely used in the feld of artifcial intelligence to obtain labeled data (e.g., [48,50,61,75]), and has been utilized to solve diferent kinds of problems such as authoring tools [13], evaluation of search results [17], library cataloguing and translation [23] and document relevance assessment [42].\n2.1.2 Domain Experts. However when labeling requires more specialized knowledge, or when label quality is a priority, organizations have obtained labels from domain experts -also called Subject Matter Experts (SMEs) -to provide higher-quality labels. These Subject Matter Experts must sometimes even undergo training which is both time consuming and expensive, in order to label accurately [33,91]. Some applied research projects, e.g., in the medical domain, have shown that expert discussions following labeling yield more accurate results [32]. Aslan et. al showed that a human expert labeling approach could improve the accuracy of the student engagement measurement model they develop and describe [7]. Similarly Woitek et al. showed the benefts of expert musicologist annotations of audio samples [119].\nOrganizations need to fnd ways to use SMEs' time wisely and efectively, given that SMEs have limited time, and their time is usually more costly than crowdworkers. There have been attempts to augment crowdsourced labels with expert-generated labels to increase the quality of the data. obtained [24,52,60,81,98,112]. However, it is widely agreed that SME-labeled data is the \"gold standard\" data source for high quality labeled data for specialized tasks.", "n_publication_ref": 21, "n_figure_ref": 0}, {"heading": "The Need for People in Labeling", "text": "While there may be specialist applications to do fully-automatic ground truth labeling (e.g., [1,2,71]), much of the work of labeling is done by people. And yet, there is a tradition that presents data science as a rational and \"data-driven\" way of \"discovering\" truths about patterns in the world [49,80,111] -a world that sometimes seems to be eerily unpopulated by human beings [69]. People, of course, bring their own strengths and weaknesses to the work of data science [115,121], and they may assert diverse defnitions of what \"rational\" can mean [127].", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "2.2.1", "text": "Human Centered Data Science. The emerging feld of Human Centered Data Science (HCDS) helps us to understand the values and peculiarities of humans as they pursue individual and collective goals in data science [4,6,57,72], and to engage in critical examinations of data science work-practices and discussions [30,31,68,70,122]. Aragon et al. described HCDS as \"an emerging feld at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science\" [6]. Together, these perspectives can be selectively mixed and recombined to inform both the design of tools for labelers, and also our conceptions of the activity of labeling.\nIn a discussion of HCDS, Yapchaian describes human work in data science as a series of observations and refnements [122]: Data is not always received in an organized and tidy package ready to be acted on. Data scientists, working with subject matter experts (SME), begin their efort by understanding the data available to them. (Including the asset(s) generating data.) From this initial view, they can determine what additional data and quantity of it is needed.\nBeyond recognizing a need for more data -or for more \"tidy\" data -data science workers often have to negotiate the data and the potentially multiple meanings of the data. Elaborating on the theme of \"tidy\" data -and its converse, \"messy\" data -Passi and Jackson reported,\n[W]e describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantifcation, but also on negotiation and translation. Thus, humans necessarily intervene in the acquisition and preparation of data for analysis [30,31,69,96,97].", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Design of Data.", "text": "In this paper, we will focus on such human interventions and work-practices around one particular aspect of messy and negotiable data -namely, the collaborative and critical processes around labeling ground truth data. As we will show, these work practices often involve collaborative activities such as shared assessments of available data, collective search for additional data, negotiation over meaning, and in some cases management of trust. The collective work of labeling is important to data science, because labeling is initially visible to the labeler and their administrator, and is thus a form of articulation work [92] -i.e., in the words of Hampson and Junor, \"work that is necessary for the work to proceed\" [47].\nSubsequently, labeling work becomes \"invisible to rationalized models of work\" [102], such as when a data scientist constructs a model using an entire dataset which is now treated as a form of infrastructure -including not only the dataset itself, but also the human work that was required to produce that dataset [101]. If the labeling work becomes invisible, then it is no longer available for inspection in the event of problems later in the data science cycle. As Gerson and Star observed, \"To the degree to which articulation is tacit and deleted from representations, requirements developed from those representations will be inadequate for local contingencies\" [35]. In the context of this paper, labeling can be considered, by analogy to Hampson and Junor [47], \"the invisible work that allows data science and data scientists to work. \"\nOur analysis is informed by several papers that examined the nature of \"data\" in data science work. Muller and colleagues described fve ways in which diligent professional data science workers necessarily intervene between a dataset and the modeling work that is usually the focus of data science projects [69]. In their grounded analysis, data may be discovered, captured, curated, designed, and/or created. For this paper, we focus on the fourth and ffth of those interventions, the design of data and the creation of ground truth.\nIf data are never \"raw\" [38], then it is important to understand how responsible professionals prepare their data for analysis. 2 The frequent use of phrases like \"data preparation\" and \"data wrangling\" indicate that we already know that data must be organized [69], disciplined [65], and transformed [30,31,96,97] for use in data science.\nSeparately, both Feinberg [30,31] and Seidelin [97] described the every-day data-design practices in use among data science workers. Feinberg analyzes \"data infrastructure [which] involves the selection and defnition of entities, attributes, and value parameters, plus the processes established to facilitate data creation\" ( [30]; see also [77]). Noting Redstr\u00f6m's description of design as including an implicit conception of anticipated usage [83], Feinberg claimed that a data infrastructure does not so much determine data; rather, it provides an environment in which people create data so that those data will become ft for purpose -e.g., ft for analysis, and potentially ft for a particular type of analysis (see also [109]).\nSeidelin considered data in an organizational context as a design material, where it is both acted on by a collection of humans and algorithms, and where it also acts on other parts of a data and analytic assemblage [97]. Data act by becoming part of the computational infrastructure (which includes Feinberg's concept of a data infrastructure). Once placed in that infrastructure, data develop a quality of authority and objectivity [38,44]. Data thus become powerful actors. Seidelin argues that data should be considered a potential design material for co-design activities among the organizational stakeholders who are afected by those data [96]. Tanweer extended this analysis to include collaborative preparation of data: \"Data are inextricable from the assemblages in which they are embedded\"an activity of collective sense-making [109]. Data infrastructure becomes a design material [30], and humans work diligently to design data within that environment [69].\nHowever, once data have been shaped and molded through this design process, the evidence of those processes tends to disappear. One reason for the disappearance is that data scientists usually focus on the dataset as an entirety, rather than on its components. In Star's descriptions of infrastructure, she says that we lose focus on the contents of the infrastructure while we focus on what we can do with the infrastructure as a service or a tool ( [101]; see also [31]). A second reason for the disappearance is that lack of documentation about the labeling process -which is one aspect of the general dearth of documentation in data science [77,86,124]. Of course, data science workers engage in complex and social labeling processes that produce the labels in the dataset. However, they do not leave any traces of the thoughtful human work that produces those labels. Labels take on a quality of being an infrastructural part of the data of the dataset, even though they were designed and added to that dataset through human work-processes.", "n_publication_ref": 34, "n_figure_ref": 0}, {"heading": "Concept Count", "text": "Informants In this paper, we are concerned with understanding the particular and organizationally powerful data type of ground truth labels while they are being designed (e.g., [30,97]) and created [69], and before they slip unobserved into the infrastructure [101] of the modeling work that dominates thinking in data science. While the phrase \"ground truth\" implies a kind of objectivity and authority [38], we show that ground truth labels are the designed outcomes [30,97] of both intentional and improvisatory collective design processes, embedded in both strategized and reactive work-practices. We show that the experience of labeling needs to be improved -for the sake of the labelers, and of the labels. We then describe how people collaboratively defne and apply labels. Aspects of that collaborative work reveal weaknesses in the quality of some labeling work-practices and the quality of the labels that people create through those work-practices. In response to these weaknesses, we describe how responsible data science workers mitigate some of those problems. Finally, we describe three approaches to the collective design of ground truth: Principled design; Iterative design; and Improvisational design. In the Discussion, we collect these ideas into proposed new understandings, and we carry ideas and understandings into implications for future technologies and workpractices.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "METHOD 3.1 IBM as Research Site", "text": "We conducted our research in IBM Research and customer-facing parts of IBM's consulting organizations, primarily in North American locations. IBM provides computer hardware, software, and services to global customers. Most of the interviews were conducted online; four were face-to-face. All interviews were recorded and transcribed.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Informants and Interviews", "text": "We interviewed 15 people who worked in several diferent data science roles. Because we are interested in collaborative labeling of ground truth, we restricted our analysis to people who worked in teams. 47% of the informants were women and 53% were men. Table 1 shows summary quantitative information about the interviews. Table 2 provides information about each informant.  With one exception, informants worked on teams in the IBM Research organization, or in customer-facing teams that were closely aligned to the research organization; the one exception was a data science researcher who was working on an algorithmic project that involved labels (I-02).\nBecause of the structure and business relations of IBM as a business-to-business company, we were able to interview only the researchers, team leads and managers who organized the labeling work -and not the labelers themselves. To understand labeling practices within the broader time-course of a project, we always asked informants to discuss a completed project. Labelers among IBM's customers were employees of the customers, and we could not ask for their time. In other cases, I-05 and I-06 worked with physicians as labelers, and had completed payments for their participation; the same was true for the fnancial analysts in I-08's project. I-03 reported her own team's difculty to persuade domain experts to provide detailed information. I-01 described labeling by a team of three interns; however, those interns were no longer to be interviewed. We therefore note that this paper provides a useful but partial view into the specifcally collaborative work of labeling. A more complete view will emerge over time, when it is possible to interview people in other roles, including both domain experts and clients.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Grounded Theory Research Methodology", "text": "We analyzed the interview transcripts using ground theory methodology, based on a combination of practices from Charmaz [18] and from Corbin and Strauss [22], as adapted for HCI [67]. Grounded theory is an inductive method that builds theory \"up\" from datai.e., the \"grounded\" in grounded theory is \"grounded in the data.\" Grounded theory is usually applied to qualitative data. As a deliberately and self-consciously rigorous methodology, grounded theory includes a series of analytic practices that have been developed since the foundational book by Glaser and Strauss, originally published in 1967 [39]. For example, grounded theory researchers engage in constant comparison of data with data, and of data with theory -i.e., the theory that the researchers are iteratively constructing from the data [18,22]. A second analytic practice example is theoretical sampling, in which researchers builds a temporary theory, and then collect more data to \"test\" that theory -typically at its weakest points [16,19,34]. When the iteratively-developed theory passes all of these tests, then researchers feel it is strong enough to publish. Thus, although a result developed through grounded theory has not been evaluated through statistical testing, it has undergone a series of rigorous qualitative tests which tend to revise and expand the power of the result over successive iterations within the qualitative analysis protocol.\nWorking with the transcripts (Table 1) and following grounded theory practices as adapted for HCI [67], we began with descriptive codes for items and events in the data (\"open codes\"), and then we looked for related items [22]. One researcher organized this work and conducted most of the coding. Two additional researchers reviewed those codes, discussing changes as needed until we reached consensus. 3 We then put those related items into structured collections of concepts or categories (\"axial codes\"). Through iterations of coding and re-coding (i.e., constant comparison and theoretical sampling), certain stronger and more generalizable concepts began to emerge. In this paper, those concepts are the experiences of labeling, and especially the collaborative work of teams while labeling, including multiple patterns in their design of ground truth labels. Figure 1 provides a high-level summary of the emergent codes from our analysis; these emergent codes become the basis for our Results in Section 4.\nPapers in the CHI conference tradition have successfully applied grounded theory to analyze interview data for sample sizes ranging from as few as six interviewees in two papers [8,79] to a high of 74 interviewees in a market-oriented study [105]. Our sample size of 15 informants fts comfortably within this range. We note that appropriate sample size is not really a settled matter in the broader literature on grounded theory [14,66]. In a policy editorial in Archives of Sexual behavior, Dworkin surveyed grounded theory interview papers, and reported advices of minimal sample sizes that ranged from fve to 50 informants [27]. In a fascinating paper, Baker and Edwards asked 15 well-respected experts on grounded theory research about sample sizes [9]. Most of the experts discussed the complexities of making such a determination, including the scope of the research project, the experience of the researcher, and the practical constraints of budget and time.\nMultiple of their expert informants recommended other criteria for assessing a grounded theory analysis. An important quality of rigor in grounded theory methodology is called saturation, or sometimes saturation of categories or saturation of data [45,66]. Saturation is generally understood to mean that the researcher is no longer fnding new information about each important concept [45]. However, as Morse stated, while \"saturation is the key to excellent qualitative work, \" there are nonetheless \"no published guidelines... for estimating the sample size required to reach saturation\" [66].\nStern discussed the concept of saturation in terms of being \"bored\" by interviews that added no additional analytic insight [104]. Guest et al. performed a rare, detailed analysis of saturation 3 Our use of grounded theory was interpretive, and therefore measures of inter-rater reliability were inappropriate [64].\nin terms of how many new codes were learned in each interview [45]. When the number of new codes dropped to zero, saturation had been achieved. In their study, the frst 12 of 60 informants accounted for the vast majority of codes. Using a similar saturation criterion, Majid et al. also found that saturation occurred after 12 interviews [62]. Within HCI, Muller described saturation in terms of curiosity and surprise, echoing Stern's position [104] that saturation occurs when the researcher is no longer surprised by information in each interview [67]. In this paper, we describe concepts that were reported in multiple interviews as our evidence of saturationi.e., a multi-informant form of the no-new-codes criterion of Guest et al. [45] and Majid et al. [62]. If we are hearing again about the same concept from multiple informants, then we have saturated that concept.", "n_publication_ref": 32, "n_figure_ref": 1}, {"heading": "RESULTS", "text": "We begin the Results section with some brief orientation material (Sections 4.1 and 4.2) to prepare for our detailed fndings . We then present informants' views on the technologies that their teams use (Section 4.3). We next consider the experiences of labeling, which become a motivation for improving existing technologies and workpractices (Section 4.4). We consider informants' use of collaboration practices (Section 4.5), and potential quality issues that may arise (Section 4.6). Finally, we interrogate the design of data (Section 4.7), analyzing these practices into three categories: Principled design, Iterative design, and Improvisational design. Figure 1 summarizes our interpretation, and states the major Axial codes in our grounded theory analysis.\nAs we described above, some additional codes (not shown) occurred only once, and we therefore excluded them from further analysis because there was no saturation on those codes.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Confgurations of Labeling Projects", "text": "In labeling work, the labeler classifes each record in terms of the category that it belongs to. The category is generally referred to as \"ground truth. \" The goal of most machine learning projects is to predict the ground truth on the basis of multiple predictor variables, often referred to as \"features. \" A familiar tutorial example is the Iris dataset, in which measurements of plant parts (features) are used to predict the species of each iris fower (ground truth) [74,108].\nIn the preceding paragraph, we stated that \"the labeler classifes each record...\" But who or what is the \"labeler\"? Labels may be applied to records by human labelers, or by algorithms (e.g., [21]), or by \"human-and-AI-in-the-loop\" hybrid systems [24,81,98,112,126]. In this paper, we focus on human-centered labeling practices as a path toward a hybrid future.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Confgurations of Labeling Teams", "text": "If our focus is on humans and their work-practices, then we need to know more about how they organize their work. In general, labeling teams had a hierarchical structure. Usually, there was a team-lead or a project-lead who organized the work of the labelers. In tightlystructured hierarchies, the labelers were recognized domain experts who were sometimes assigned from other teams. In one set of projects, the domain experts were external authorities, paid for a day of labeling work (e.g., physicians who worked with I-04 and I-05, and fnancial analysts who worked with I-08). In loosely-structured hierarchies, the team-lead had to recruit labelers from within the team -and sometimes encountered reluctance from team members (see Section 4.4). We ofer this overview of team confgurations as an orientation to the results that follow. Further details on team structures and their consequences may be found in Section 4.5 and especially Section 4.5.2. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Technologies of Labeling", "text": "Informants used diverse labeling tools. To avoid critical comments on specifc products, we will list them together in this paragraph. The most frequently used tool was an unmodifed spreadsheet. Some projects used more specialized tools such as CrowdFlower (previously FigureEight). 4 Projects that focused on both entities and their relationships used more complex linguistic tools such as brat. 5 . A few projects had unusual needs that led them to try multiple commercial tools, and then to build their own in-house tools (similar to e.g. [87]).\nInformants described their considerations when selecting the labeling tools according to the task they were performing, for example, I-12 commented about the usability of the tools in relation to multiple labelers, stating that \"the challenging piece here is the... usability of those tools and ability to distribute in [an] efective manner across several people, which is... time-consuming. \" I-15 commented on the challenges of using external labelers \"We defnitely needed to trust the labeler and on the external tool you cannot trust the labelers. \" This kind of difculty became part of the motivation to focus on team members and/or domain experts, who performed their labeling work under more controlled and partially-accountable conditions.\nI-08 highlighted that some commercial tools can support multiple labelers' labeling practices in a sophisticated fashion, \"You have multiple people working on it and they have like a very sophisticated interface where they can click on [the] labels they want to attribute. \" She also described some of the consensus practices used to agree on label disagreement ofered in the tool, \"And then at the end of the task, they actually have even a verifcation test set because it's multiple people that work on that to say yes or no, this labeler made sense or not. \"\nIn contrast to using commercial tools, other labelers took a less formal approach. I-05 stated \"I may start of with not really even a true annotation tool or a very, you know, just building up a system just trying it out.\" This approach was echoed by I-02 in relation to the act of labeling in one of their projects, \"We did kind of a brute force approach. Everybody on the team got their 100 or 200 examples to label and they just had to choose a class label for every single example\".\nInterestingly, labeler task creators and labelers refected on the tools and artifacts they created when assembling the labeling task and conducting labeling. I-11 stated \"We've kind of came up with a set of rules and defnitions that I think ended up in creating fowcharts at one point, to help us guide our labeling.\". This quote followed with comments about what the fowcharts enabled the labelers to do. I-12 mentioned \"We started creating spreadsheets with the label data... my colleague and I ended up getting pretty close, we got into each other's heads enough to where we were doing this fairly reliably\". Some informants also wished for a tool that could support a broad range of labeling tasks, I-01 said \"If you were building a custom labeling tool for one customer I would never say, 'Building a spreadsheet.' But, if your goal is to have not just like an [open source] labeling tool for a specifc domain but a truly general or general fexible labeling framework... I wouldn't even say the whole thing is like one smart spreadsheet but the notion that you can import and export into spreadsheets ... and then you plug your spreadsheet in and it gets sucked into this system... \"\nAs we will show, below, the labeling projects were quite diverse. Teams chose their labeling tool based on the nature of the labeling problem that they needed to solve, the characteristics of their ground truth, and conveniences or limitations in available budgets, timeframes, and tools.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "The Experiences of Labeling", "text": "Amongst informants, there was a general sentiment that labeling is a thankless and burdensome task. Labeling \"tends to alternate between mind-numbingly boring and excruciatingly painful, \" as I-01 noted. They explained that the painful part, in their project, was dealing with unclear or ambiguous instances to label: \"You've got to be kidding me. How am I supposed to label it?\" As I-ll fatly put it: \"labeling is this inherently boring task. \"\nOrienting one's self to the labeling task can be challenging, as I-12 shared:\"labeling was not something which... they did day to day. That was very new for them... \" (I-12). I-06 spoke of the \"resistance\" of team members when asked to perform labeling, \"it's not like really a fun job to do. \" I-14 said that he had to resort to \"bugging people to do it when they had time... \" I-12 reported of his team that \"in most cases they had tried to stay away\" from labeling sessions. I-1 tried incentives, such as \"we have what we call a tag party. You know, we got pizza and we got 13 people to sit in a conference room and start tagging [i.e., labeling] posts. \" However, even in this case, \"people were getting really hung up\" on labeling disagreements, which points to the situated and emergent nature of labeling data (see Section 4.7).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Contrasting Automation and Human", "text": "Expertise. Given that labeling is unpleasant and difcult work, many see it as an opportunity for automation technologies to ease the repetitive and numbing nature of labeling for humans [3]. Yet, in most cases, human knowledge remains an essential dimension in generating a high-quality, labeled training set. Cutting-edge ML paradigms, like reinforcement learning, enable models to gradually improve accuracy by measuring user behavior and interactions with model outputs. But even in this type of approach, the model needs human-labeled inputs to provide a \"warm start, \" as I-03 described. Their project, which centered around a reinforcement learning model to build a chatbot, needed a starting set of labels to provide a reasonably accurate starting place for the system's early users (e.g., routing service requests to the appropriate channel). I-03 provided this warm start by generating some labeled data, applying her own best guess as to what category each user request fell into. She was conscious that she was creating these labels in her role as data engineer, and she said, \"think of it as almost like user labeling. \" The actual user-derived labels would be incorporated into the model and refned via reinforcement learning during end-use.\nIn the healthcare domain, I-05 described the necessity of involving medical experts when labeling the contents of an electronic health record (EHR) because it \"is not always complete or contains outdated information. \" Further, an EHR contains both general and specifc information. She explained, \"So you may have a concept that shortness of breath which is also the name is dyspnea which is all the same difculty breathing just diferent way to express it. \" In a project to categorize the social infuence of outbound social media posts from IBM's marketing organization, I-1 said that \"there are attributes in a social media post that would be impossible to code for... is it entertaining?, funny?, does it relate to an audience, and the timing of the post. \" 4.4.2 Summary. Thus, the tedium and difculty of labeling call out for automated assistance. However, human knowledge remains critical to train that automated assistance. We explore further complexities of human labeling work-practices in the following sections.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Collaborations in Labeling", "text": "Collaboration followed multiple patterns among our informants. Several informants reported that they asked at least two peopleand as many as fve people -to label the same record. Except for extreme cases (e.g., 1-2 labelers in a team), these replications of labels did not appear to be related to team size, but rather to the intended degree of label quality. I-04 said that \"as a team we pick 20 patients each... and where we doubled on a patient with somebody else. \" In an image-labeling project, I-14 said that \"most of the images that have -would be labeled by at least 3 diferent people. \" I-09 reported that \"every [labeling] task has fve people. \" The general rationale for asking multiple labelers to work on the same record was accuracy (sometimes expressed as \"verifcation\"); however, informants did not report a rationale for the larger numbers of labelers.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dealing with Disagreements.", "text": "If the multiple-labelers agreed on a label, then of course this was the ideal outcome. I-01 reported that a requirement for \"consensus\" was helpful but potentially timeconsuming. If a pair of group of co-labelers disagreed, then an additional labeler might be called in as a \"tie-breaker\" (I-09, I-1).\nFor other informants, there was a hierarchy of authoritative labelers. I-09 reported that \"We can actually let everything go to the crowd and only edit. And we ask the expert to process [it]\" -i.e., an SME would examine cases of disagreements among frst-pass labelers. In several healthcare projects, the labelers were themselves SMEs (physicians). They were encouraged to resolve disagreements via \"some kind of consensus voting\" (I-04). However, if discussion did not lead to consensus, then I-05, a project team member with medical training, would apply her combination of healthcare expertise and data science expertise to resolve the issue:\nWe'll do an hour of annotation. Then we'll regroup. Maybe it's lunchtime. And while they do lunch ... I'm like changing things to try to ft in what they're saying and capture the nuances they're pointing out... Failures in consensus on labels for a chatbot, required that I-01 examine the labels from each of 3-5 labelers, as recorded in a spreadsheet. Both I-01 and I-11 complained that comparing multiple labels in a single row of a spreadsheet could be problematic -\"horizontal scrolling is awful\" (I-11).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Division-of-Labor.", "text": "Informants who organized the work of labelers often used diverse forms of division-of-labor. One type of division of labor was mentioned in the preceding sub-section: I-04, I-05, and I-09 described the management of disagreements in terms of a hierarchy of labeler-expertise (e.g., SMEs). Less formally, I-1 worked with a team of three interns. For the most part, the interns resolved their own labeling disputes, but occasionally \"they'd like raise something to me, what's the tie-breaker, what's your thought on this?\"\nAnother form of division-of-labor occurred during initial training. I-06 told us that their team mostly worked by themselves, but in the beginning especially we kind of worked together just trying to fgure out what things are like. But yes, general rule of thumb it was like by ourselves. I-02 also reported that labelers worked in isolation, \"we were kind of trying to do the simplest thing where none of our labelers could interact with any other labelers. \" I-02's motivation was primarily to divide necessary work among people who were reluctant to do the work: \"We did kind of a brute force approach. Everybody on the team got their 100 or 00 examples to label. \" However, this principle could be applied diferently for some teams when labelers disagreed. Echoing the expertise-based distinctions from the previous subsection, I-11 said\nWe also considered various methods of... sharing the label example from ones who were high-quality labelers... For example, if you label something and it was an outlier compared to your peer group... highlighting that and saying, hey, four out of fve labelers actually chose this... You may want to consider -reconsider your choice. I-01 applied a diferent form of division-of-labor among a team of chatbot developers. In many chatbot architectures, the front-end dialog is used to route the user's request to an appropriate backend service (called a \"user intent\") [29]. Members of I-01's team had organizational relationships with teams that built some of the back-end services -\"a sense of ownership,\" according to I-01. For this team, the division-of-labor called for each labeler to look for user requests that would match their back-end service-of-interest:", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Some of the companies have -individual people [who]", "text": "were responsible for diferent subsets of the classes and the model.... \"I'm responsible for the legal questions and you're responsible for the technical questions... \" Finally, we observe that there may be multiple stakeholder groups with an interest in the labels. While these groups may not have agreed on an explicit division-of-labor, they nonetheless must manage their multiple sets of labels as part of their multiple stakes. In customer facing work, I-08 reported that \"We also have labels from their side that we use for the extrapolation of labels... And then of course the labels that we created or taxonomies, then we would present that to them. \" I-08 elaborated:\n...The labels we attribute to [their data] -of course we check them with them. We have regular [meetings] where we go over the results and say, \"Hey, we found this, this, and this. Does this makes sense?\" And then they can say yes [or] no... so there's a... running back and forth there.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Summary.", "text": "Collaborations took diverse forms. Some teams isolated each labeler from other labelers, resulting in a real but minimalist dyadic collaboration between team-lead or researcher, and each labeler. A common theme was to collect multiple label-values for each record, to increase accuracy or verifcation for the label or labels that would ultimately be assigned to that record, and then become the basis of modeling. Informants and their teams structured their collaborations to detect and resolve potential disagreements, employing strategies that ranged from tie-breaking to consensus. Often these strategies made use of diferences in expertise among the labelers, assigning greater responsibility and infuence to labelers with greater expertise (e.g., SMEs). However, other organizing strategies were also observed, such as taking advantage of organizational relationships to assign each labeler to fnd records that matched their organizational stakes, and to assign the same labels to all of those records. Finally, some teams consulted with their clients over labeling decisions. As we will see in the next section, various collaboration and division-of-labor strategies had implications for the quality of the assigned labels.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Examining the Quality of Labels", "text": "Informants were candid about the compromises that they had to engage in, to make the labeling process \"work\" in the context of their projects. We say \"to make the labeling process 'work' \" in the sense of articulation theory, which is often characterized as the \"work to make work work\" [92]. In this section, we describe how the informants worked to make the work of labeling successful in practical circumstances. 4.6.1 Reconsidering the Qality of Labels. Informants acknowledged that they were working in a research engineering environment, and that engineering is often a matter of trade-ofs. The quality of labels had to be considered in relation to resource limitations, such as available staf and available time. Working on a chatbot project, I-03 acknowledged that her labels were \"not a ground truth [in the sense of] which one is the absolute best,\" but rather a series of approximations that would work enough for a \"warm start\" of her chatbot service, which would then improve the quality of its labels through reinforcement learning. I-06, who worked on a diferent chatbot project, stated the situation more bluntly: \"Not super scientifc, but it works. \"\nThese issues were not limited to chatbot projects. In a healthcare project, I-04 acknowledged that \"there's lots of confusability, \" Working on a diferent aspect of the same project, I-05 spoke of the need for administrative control in the labeling project: \"everyone has an idea of what a problem is but we need to tell them what [the] problem is... because everyone has a diferent idea.\" I-06 agreed about the ambiguities of the general situation of the labeling task: \"Depending on who you [are] or depending on the time of the day, or the week, you could probably label it two diferent things. \" In the words of I-05, there was sometimes a need to \"kind of clean the ground truth. \"\nI-04 told us that some of the difculties with label quality occurred \"when an expert... may not have been available... \" and noted that in some situations, \"medical expertise was less important as compared to just word knowledge or linguistic input. \" I-03 also reported difculty to access domain experts and their knowledge: \"that's very rare that we work with them [domain experts]. It's kind of just like a conversation. They give us their skills and we build stuf on top of them typically.\" In some projects, domain experts provided documentation for the labeling team, but declined to label directly. I-08 said that \"where we have documents, we have no help from their side so it was done in-house -yes. \" 4.6.2 Situated Data Science Projects. The practical circumstances of contextualized project work imposed several types of constraints. Working on a chatbot project, I-03 described a set of client cases that raised statistical distribution issues: \"One of your [labels]... covers a bunch of diferent topics. So we want to split this into several [labels]. \" In this project, it was not possible to change the statistical distribution by obtaining more data, so an existing label had to be split into additional new labels in order to improve the downstream modeling performance. I-06's team also \"limited the possible labels\" based on pragmatic constraints. I-08 described a customer-facing project in which the business goal was to demonstrate the value of IBM's approach to the customer, within the period of a limited customer-engagement: \"You try to do your best, but it's sometimes complicated because of the [time limits]. \" Therefore, \"It's really more of... how to adapt or attack a business problem or how to get value out of it... by using machine learning as efectively as possible. \"\nIn the health sciences domain, I-05 acknowledged that the scoping of the items to be labeled could be limited by resources and timeframes, resulting in limitations on what her domain experts should label:\nWe said clearly... we don't want some of the... chronic problems, you want acute problems are happening right now. You don't want anything that is resolved. But then there's the question 'okay what if the problem is resolved but there are some residuals?' ... Or another thing we tell them we want recurrent problems. 4.6.3 Mitigating Qality Issues. Earlier, we quoted I-05's call for administrative control over the labeling process. Informants exercised control in three principal ways. The frst strategy was to use multiple labelers on each record, as described in detail in Section 4.5, further strengthened through hierarchies of expertise and authority as described in Section 4.5.2. We describe the two other strategies in this section: applying tacit knowledge of data and humans; and the use of guidelines.\nTacit Knowledge. Informants' second type of administrative control was less formal, and sometimes under-acknowledged as a form of quality-control. Informants often had intimate knowledge of multiple formal aspects of their projects, and this knowledge allowed them to apply their expertise in diverse ways. As Sch\u00f6n has argued articulately, designers often enter into \"a refective conversation with [their] materials\" ( [93]; see also [10]). Muller et al. showed that this concept could be applied to the analysis of data science activities [69], through the data science worker's intimate knowledge of the data and the tools that manipulate the data.\nInformants' accounts revealed this kind of intimate and sometimes actionable knowledge of their data. While working on a sampling algorithm, I-02 remembered, \"One thing that we realized was that when we as humans looked at the experts some conficts just looked bad to us in ways that none of our automatic metrics were capturing. \" Later in describing this aspect of his work, I-02 added, \"There's something fshy going on... When I see this example I don't really know how to fx it, but I do agree that there's something wrong going on.\" When I-02 talks about \"some conficts just looked bad\" and \"there's something fshy, \" these are examples of using intimate knowledge of the data to perceive problems that are not detectable by formal algorithms. I-05 spoke of using her medical knowledge of physicians' note-writing practices to detect issues, \"as a physician reading the note, you know, their thoughts are organized in a certain way. \" In all of these cases, informants used their tacit knowledge to address quality issues.\nInformants also used a diferent kind of tacit knowledge -their knowledge of humans as labelers. I-02 considered human limitations and algorithmic limitations in selecting which records should be labeled by human labelers: So if you give somebody an example to label that's incredibly hard to label and they take 20 minutes to fgure out what the right label is, that might not be as good as if you were able to give them 50 diferent examples at the same time, and learn even more from the diferent examples, because they were easier to label, even though that one example is the most valuable single example in your data set.\nI-05 also had to navigate limitations of human cognitive load vs. the needs of modelers, \"It was always kind of a back and forth between... what the researchers and engineers want versus what the human is capable of doing... \"\nIn these examples, we can see informants as knowledgeable and intuitive practitioners, seeing into their data and their colleagues. We described their knowledge as \"tacit,\" which implies that the informants may not always be aware that they are using these kinds of insights. We hope that these observations may encourage researchers to explore tools to encourage and capture refective practice (e.g., [56].\nGuidelines. Informants exercised a third type of administrative control through multiple forms of guidance and documentation for labelers. Fort has made a strong argument for extensive \"Annotation Guides\" -i.e., documents that explain the labeling process in detail [33]. Informants did not typically provide the rich detail recommended by Fort. However, they often provided as much guidance as initially appeared to be needed for labelers. I-09 stated the goal as follows: \"When we're perform[ing] labeling tasks for the product[-use] that is a very precise defnition... So labeling for everything is actually consistent. \" For complex labeling requirements, I-12 spoke of a \"codebook\" and I-11 made reference to \"fowcharts. \" Guidelines may or may not require a formal document. Working on an image-labeling project to model trafc congestion, I-14 described a set of lightweight guidelines in the form of onscreen reference photographs: \"We had 6 or 8 diferent pictures on the main screen showing what we would consider to be congested. \"\nGuidelines can help to reduce the need for double-labeling, thus saving time and costs. I-04 reported that she would \"try to defne the annotation rules before starting the actual annotation process... Once the rules have been defned and something has been made more (deterministic) [for] an expert... it's typically not double annotated. \"\nHowever, some of the projects found their initial guidelines to be insufcient. I-05 began with the intention of the guidelines -\"usually it should be if your guidelines are good or the way the test is set up it should be very clearly right or wrong\" -and then described potential problems: \"But there is always a gray area... \" The problems could become an occasion for further refnement of the labeling vocabulary and hence the guidelines: \"rather than forcing the gray area into a right or wrong a lot of times what we do is we try to qualify it.\" Guidelines that are prepared in advance may require revision when used \"in the feld, \" according to I-04, who added \"but you arrive at that point often it's confusing and it... leads to a lot of disagreement. \" What is required for adequate guidelines? 4.6.4 Summary. Informants sometimes struggled to make difcult trade-ofs between the pragmatic needs of their projects, and the formal needs of high-quality labels for subsequent analysis. They described problems of labeler confusion, and many issues of resource constraints. Informants also reported on their eforts to improve or restore label quality, including the use of their own well-informed tacit knowledge, and diverse forms of guidelines for labelers and other members of the team.\nIn work mentioned earlier, Fort advocated a process that could take as long as six months to develop a set of extensive guidelines [33]. However, several of the informants described projects whose entire duration was less than three months. We assert that there is evidence both of the value of guidelines, and also of the practical diffculties of creating those guidelines [55,77,86,124]. We hope that researchers will work on future print-and/or-online tools that can provide more robust support to labelers, and we believe that these tools may also provide labor-saving and consistency-enhancing recommendations based on team needs and individual preferences (see Section 4.3).", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Designing Ground Truth", "text": "The principal fndings of our paper are our account in this subsection of how the work-practices in the design of ground truth labels. In grounded theory terms, this subsection constitutes our Selective Code, and corresponds to the lower portion of Figure 1. We describe three appraoches to the design of ground truth: Principled design; Iterative design; and Improvisational design.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Principled Design of Ground", "text": "Truth. The strategies in this subsection refect both a solid planning process and, importantly, an absence of substantive surprises. Several projects began with a wellplanned procedure for defning labels and applying those labels to data records. In one of the healthcare projects, I-04 discussed a preselected medical vocabulary for labels: \"In a medical context it might be problems like diseases, conditions, symptoms, medication... So these would be diferent labels that would be assigned to the same [textpassage] and it would persist. \" Her research partner, I-05, reported similar to \"build a lexicon\" for a diferent study, \"We had tasks where you were identifying a single concept for example medication, or a lab test, or even a lab test and its corresponding value and, you know, billing together. \" In a chatbot project, I-06 described a plan to constrain the labeling vocabulary to \"30 altogether topics that were very limited, so we didn't choose anything outside that 30. \"\nOf course, ambiguities frequently arise in labeling edge cases. Projects taking a \"principled design\" approach were also distinguished by a plan to resolve ambiguities. I-05 provided an example of resolving ambiguous terms online through the Unifed Medical Language System (UMLS), an online medical database, where \"by linking.... [to] UMLS which has all the various medical language terms and is linked to, you know, it provide[s] additional information. \"\nAnother strategy for \"principled design\" was described in Section 4.5.2, in which I-01's chatbot team divided their labor by assigning each team member to look for user-requests that corresponded to that team member's respective back-end service.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Iterative Design of Ground Truth.", "text": "However, what happens when plans do not go \"as planned?\" Informants on a second group of projects knew that they could not make a principled plan for all of their labeling protocols and stafng. They accepted this uncertainty, and built their plans around disciplined, iterative approaches to resolving the uncertainty.\nOne strategy was to build iterations into the labeling plan. In I-05's healthcare project, her team intentionally developed certain labels through a series of refnements: \"I may start of with not really even a true annotation tool or a very, you know, just building up a system just trying it out. \" In I-04's related healthcare projects, there was a series of steps in defning and refning labels, \"the input from the expert annotator is only at certain points and done with certain... resources that might help this process, but not in terms of actually creating that entire question answering dataset. \"\nA related theme came from I-11's and I-12's work on a project to estimate conversational quality in chatbot sessions. They knew that they would need to work to fnd a solid set of metrics for conversational quality. I-11 described their iterative approach: \"We'd grab like 20 conversations, we would cold-label [the] conversations, evaluate what we did, discuss what went well, what went poorly... and see how we were doing, And then go back and do it again. \" I-12 concurred: \"There's a lot of discussion, lot of iteration on the rules of the code sets. \"\nBeyond planning in general for iterations, some informants described iterating on combinations of human activity and computer activity. In I-10's project on structured documents, \"the tool learns an initial set of rules and then it continuously refnes this by asking the user for concrete examples. So, the tool generates examples that the system is uncertain the labels should be... And then, after additional labeling from the user the system refnes the rules. \"\nSimilarly, I-06 worked with a more product-oriented chatbot tool, and said, \"we were trying... to gather the utterances, [because the tool] requires some utterances to be trained so that they can respond to, and they can fgure out what topic it's talking about and then they can start the dialogue from there. \"\nIn both of these projects, the teams' development plans provided for step-by-step development of the competence of the chatbot system through iterations of human and computer label-development.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Improvisational Design of Ground", "text": "Truth. For some teams, there were many surprises, which took them away from any possibility of a planned development of labels. Other teams seemed to approach the problem of ground truth labeling as an area of open explorations into an unknown domain. In these cases, ground truth emerged from a series of improvisations.\nInformants acknowledged that limited time, resources, and labelerknowledge could impact the labels that were assigned. I-1 was concerned that his very busy team of reluctant labelers had \"too little time commitment, too little thought commitment, \" and he eventually re-assigned the work of labeling to three interns, who could be assigned to create labels as a full-time activity. Earlier in Section 4.6.1, we quoted I-03 describing her chatbot project as \" It's not a ground truth [in the sense of] which one is the absolute best.\" I-06 reached a similar conclusion in her chatbot project, \"we don't have to be like extensive but we have to be representative enough so that it will fgure out that these are going to be labeled into one and the others are going to be labeled into the other. \" I-02 described her labelers as \"learning as they go. \"\nWhen teams work intensively together, they may be able to apply good labels. I-ll reported that \"we got into each other's heads enough to where we were doing this fairly reliably. \" However, more diverse teams may encounter unexpected difculties. I-1 remembered, \"So strategists, creatives, not predominantly data analysts or subject matter experts or anything of that nature... What we ran into immediately was some of the categories were subjective in nature, like what constitutes the exit post versus a promotional post. And people were getting really hung up on those defnitions. And so each person... was coding things diferently. \" We previously noted I-06's complaint in Section 4.6.1, \"depending on who you're doing or depending on the time of the day, or the week, you could probably label it two diferent things.\" Even successful labeling eforts could encounter early problems. I-05 described her early labeling work as \"mostly we had a huge dump of an HTML with all the text -all the notes for a patient that... can be sorted in diferent ways, can be fltered in diferent ways, and you just have to navigate your way through.\" Teams had to stop, reconsider, negotiate, and put considerable work into aligning their labels with one another. This mutual re-alignment was of course possible, but it added time and efort to what was already an efortful task.\nOther problems emerged from pragmatic limitations and relationships. We learned about two categories of pragmatic challenges, including issues intrinsic to the project, and challenges that primarily addressed issues extrinsic to the project, including concerns that arose from relationships with clients.\nIntrinsic Challenges. Some projects faced \"massive amount of topics... I don't think we tried much to actually harmonize them or evaluate how diferent they were\" (I-06). Facing a similarly formidable number of topics, I-07 said \"we ended up doing the simpler ones, because... this is more straightforward, more explainable, less time-consuming. \" Like I-07, I-05 also had to \"change the name of the label going forward if the label is confusing to [SMEs]. \" Sometimes it was necessary, according to I-05, to \"make up our labels... because... there is no single word that represents what we mean. \"\nThe requirements of projects seem to have powerful efects on the names and defnitions of ground truth labels.\nExtrinsic Challenges. When teams faced limited time or resources, they often had to make compromises on how they managed their labels. I-08 and his team worked on quick cycles to show value to customers. They could not label extensively, but rather \"for most of the cases I guess we are working with single label just because of the nature of the POC [proof of concept].\" I-08 and I-13 reported that customer-reactions drove part of their labeling protocols and outcomes. I-08: \"we try... to help them adopt our line of product and show them the value in that by sharing them with the very specifc use cases how we actually create value.\" and I-13: \"we managed to implement both methods and, yes, okay results. Client was happy. Obviously, if we had more time, more data, more annotations, we could have done a lot better. we usually work on six-week engagements. \"\nIn the preceding examples, the relationship with the client led to restrictions in the types of labels and the thoroughness of labeling. I-07 worked with an inverse of this challenge while using labels on outbound social media posts to control the relative volumes of posts. I-07 distinguished between frequent posts that made assertions vs. rare posts that showed products in action, \"there's very few of those 'show' posts, but we wanted to highlight our -showing a product in action versus telling people. \"\nOf course, relationships are important, and it makes sense to adjust labels and labeling protocols. to support those relationships. From the perspective of \"ground truth labels, \" we see again that both labels and processes are contingent on both intrinsic and extrinsic relationship opportunities. Ground truth appears to be grounded in data, and also in networks of social and business ties.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Summary of Results", "text": "These accounts imply that label names and meanings are not fxed in advance, but are malleable, changeable, and negotiable depending on who is applying them. Labels become accountable beyond simple defnitions -accountable to pragmatic limitations, to persons, and to corporate entities. Labels thus perform multiple types of work for us. Labels describe the world. Labels mediate defnitions of the world. Labels arise from the needs of relationship partners. \"Ground truth\" begins to look less like a formal or \"objective\" truth, and more like a worthwhile social accomplishment.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION", "text": "We began this paper with Rich's observation that \"truth\" is or becomes an \"increasing complexity\" [84]. Informants have helped us to see how, when we examine the simple concept of \"ground truth, \" we discover the complexity of their collective practices, and the strategies of their thought processes. These complexities lead to implications which may be of interest to", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Implications for Data-Labeling Tools and Work-Practices in Context/HCDS", "text": "Through discussions with informants, we propose potential improvements to the tools of labeling, and also the work-practices that can be supported through those tools.\n5.1.1 Smarter Labeling UIs. Future labeling UIs could include a number of features that could be responsive to the labeler, and that could provide assistance. For example, I-02 noted \"there's something fshy going on\" in her data. Over time, an intelligent labeling tool could learn the defnition of \"fshy,\" and could help labelers to set \"fshy thresholds\" that could determine which records were automatically brought to their attention as potential problems.\nWe may learn that entire data science teams can agree on their local defnition of \"fshy;\" or we may learn that data science workers in diferent roles focus on diferent data attributes, and thus have their own distinct and role-specifc defnitions of \"fshy.\" It seems likely that a domain expert might apply a diferent set of criteria for \"fshiness\" as contrasted with a statistician. How can humans teach their skills to an AI agent, so that the agent can bring role-specifc \"fshy\" candidates to the human for inspection? Does the AI agent need to have a model of the roles of the humans, so that each \"fshy\" instance can be brought to an appropriate human for discernment?\nIn addition to providing insights into \"fshiness, \" I-02 also mentioned the problem of a labeler's cognitive exhaustion. An intelligent labeling UI could contain a private user model for each labeler, possibly including the labeler's history with other labeling tasks. The UI could help the labeler to track their own personal performance, recommending breaks or other refreshing activities when there was evidence of exhaustion. A more sophisticated possibility is that the labeling UI could actively adjust the degree of difculty of the next item-to-be-labeled, based on an analysis of the labeler's cognitive state and the labeler's prior history with similar items.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Social Labeling UIs.", "text": "A socially-intelligent labeling tool could provide awareness services related to other labelers -something that is seldom done in crowdmarkets other than in the work of Von Ahn [113]. These services could include an indication of the individual labeler's comparative performance, which could be anonymized so as to show that labeler's performance in relation to unnamed other labelers, following principles of social translucence (e.g., [28,37,63]). This kind of design would allow self-calibration in relation to peers, or in more dramatic applications, it could become a means of gamifcation.\nA second type of social awareness could be implemented for cases in which two or more labelers work on the same record, so as to achieve higher quality through comparisons of their labels. In this case, early detection of conficts in labels could be moved immediately to a queue of items to be resolved between or among the labelers who had contributed labels to the same record [20,51,90,117,118]. With good user modeling, it may be possible to predict conficts before they actually occur, and to help labelers consult with one another to prevent or \"pre-resolve\" conficts [25].", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Labeling Provenance.", "text": "We have shown the evolving complexities of ground-truth labeling. However, these complexities leave few marks on the data [122]. Labels seldom contain their own histories or provenances. 6 Further, we know that data science workers tend not to document their work as they are doing it [55,86], and that measurement plans (which might shape or the labeling process) often go unrecorded [77] -despite Fort's advice to develop and continually use a detailed set of labeling guidelines [33]. As Kery et al. showed, some of the obstacles to documentation appear to be based in the tools themselves [55], while other obstacles appear to be due to the exploratory work-practices that characterize important parts of data science practice [86].\nNot all of these provenance problems can be solved through technology, but some features could surely help. Once applied, labels appear in a dataset without label-metadata. Similarly to Kery et al.'s Verdant versioning-support tool [54], we propose that the preservation of labeling histories could help to reconstruct how a particular label was applied to a particular record. Simple metadata could include a timestamp and a link to the labeling vocabulary that was available at the time of labeling -i.e., the labeling-structuralcontext that governed the labeling activity. An equally valuable type of metadata would be the identity of the labeler, who might be able to provide insights about how they assigned that label -i.e., the labeling-social-context.\nDisagreements among labelers could also be recorded with their resolutions. These metadata could also be used analytically when needed, to examine other labels that were assigned during the same timeframe, or that were assigned by the same labeler. These and other label-metadata would help to reveal what might be called informally \"the truth beneath the ground truth\" 7 -i.e., the located accountabilities (e.g., [73,106]) around the creation and refnement of labels. In much contemporary labeling practice, these accountabilities would involve humans as actors. With enhanced \"human-and-AI-in-the-loop\" algorithms [24,125] these accountabilities could also involve computational actors (e.g., [25,95]).", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Implications for HCI Data-Craft Research: Data as Design Medium", "text": "In most data science projects, one or more people create groundtruth labels through a process that subsequently fades into invisibility. Because the term of art implies these labels are neutral descriptions of reality -i.e., as \"ground truth\" -we tend to forget that there was a series of human actions that led to creating what now appears self-evidently as \"truth.\" The labels stand as rationalized data attributes, while the informal articulation work that was done to produce them, becomes less and less visible [47,92,100,101]. Earlier, we introduced Feinberg's work on the design of data ( [30]; see also the work of Seidelin [96,97] and Tanweer [109]). Muller et al. showed that their concepts of design-of-data could be applied to the analysis of data science activities as a kind of craft work, in which the data science worker uses tools analogous to weavers' looms and potters' wheels to engage with and shape the data [69]. If, as Feinberg, Seidelin, and Tanweer argue, data can be designed, then the analysis of Muller et al. suggests that we should pay attention to the tools through which \"data designers\" (i.e., data science workers) pursue their under-acknowledged crafting of data. In this regard, we may want to revisit Sch\u00f6n's work on \"designing as refective conversation with the materials of a design situation\" [93] -or, in the succinct summary of Bean and Rosner, \"materials matter\" [12].\nFeinberg also provided a useful commentary in her paper on \"material vision, \" in which she pointed out that we can choose how we approach data and related concepts ( [31]; see also [72,103]). She described how we change the conceptual signifcance of an item by changing the characteristics that we attribute to it. Applying her ideas to the labeling process, we propose that:\n\u2022 For a labeler, each record presents a new challenge. The labeler takes a record-centric view, and solves each labeling challenge. \u2022 Subsequently, for a modeler, each dataset presents a new challenge. The modeler takes a dataset-centric view, and solves each modeling challenge.\nApplying the concepts of Feinberg's analysis, the labeler chooses to focus on labels as the materials that they work with (see also [96,97]), while the modeler chooses to focus on datasets as the materials that they work with. In the transition of the work from labeler to modeler, focus is a key choice. 8 As the focus of the data science team shifts from one activity to the next, the perceptual status of the labels changes from foreground to background, from structure to infrastructure [31], from visible to invisible [100]. Correspondingly, the articulation practices that we have described here, also fade from foreground to background [102]. As the modeler continues with their work -and as the client waits impatiently for the results of the model -it is easy to forget that someone sweated the labels.", "n_publication_ref": 20, "n_figure_ref": 0}, {"heading": "Implications for STS: Labeling Work as Infrastructure", "text": "The shift in perspective that Feinberg described, may also be analyzed in Star's infrastructure approach as a shift in analytic focus (e.g., [101]), which in our case translates into a shift in both data granularity and work practices required to work at that level of granularity. As Star notes, when a concept enters an infrastructure, it can easily become a \"boring thing\" [100]. For a team of data scientists who are intent on producing a functioning model or pipeline, ground truth labels and the work-practices that produce them, tend to disappear into the data infrastructure [30] -along with other \"boring things\" that are necessary, but unexciting. We recall I-01 and I-11, who reported that labeling was \"inherently boring\" (I-11) and indeed \"mind-numbingly boring\" (I-01).\nIf we want to bring attention and accountability to the processes resulting in ground truth, we need to fnd ways of again shifting focus, so that labels become a worthy topic of analysis and of human efort. Bowker described a similar analytic strategy as infrastructural inversion [15]. Our paper has attempted a similar shift in analytic focus, showing how collaboration and division-of-labor make important contributions to the development of ground truth, raising questions of data and process quality, and focusing on three distinct classes of processes in the design of ground truth labels (Section 4.7). Based on the difcult and thoughtful work reported by informants, we assert not only that labels are \"interesting things, \" but that the people and practices that give rise to labels are also interesting sites of our scholarly attention.\nIn the context of machine learning pipelines, labeling work tends to occur as a series of acts of layering, in which each layer renders the layer below it invisible. Star and Strauss eloquently made the general case for invisibility in their paper \"Layers of silence, arenas of voice\" [102], and we apply the silences part of their argument here, as a series of consequences of shifts in our focus when we work in data science. The original work of discovering or capturing a dataset [69] tends to become invisible as we strive to make the data ft for purpose -i.e., ready for analysis. We do this through acts of designing the data [30], applying ground truth labels to the records (this paper), and curating records in the dataset [65]. Each of these necessary actions adds another layer of value \"on top\" of the original dataset. However each layer reduces the visibility of the original data, and crucially, each layer reduces the visibility of the activities of the layer beneath it. If problems arise subsequently in the analysis, the multiple invisibilities imposed by these layers may interfere with our ability to repair problems that we inadvertently introduced in the earlier steps of our analysis, as we argued above in Section 5.1.3 on \"Labeling Provenance. \"", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Limitations", "text": "This paper describes research in a single company in a single national culture. Of necessity in interview studies, we considered fewer than 20 projects, and we considered those projects at a moment-in-time in the year 2020, during a period of rapid development of data science concepts, work-practices, and technologies.\nWe also note that the projects had more of a research emphasis than a development emphasis. Nonetheless, even in the research projects, we found that pragmatic considerations imposed constraints on how ground truth was created and refned. We hypothesize that pragmatic constraints may be even more impactful in a development, product, or service context. We note that informants who were working closer to a \"client-facing\" project, had a tendency to report less complete labels and less complete labeling strategies. In a small-n interview study, we do not want to claim that this was a causal relationship. We mention it as a possibility for future research about how organizational settings may afect ground truth practices.\nFinally, we revisit the issue of the partial view. In Section 3.2, we noted that, like other industry research settings, IBM's structure and business model prevented us from interviewing the labelers themselves. Thus, this paper provides only one view into labeling phenomena and practices. In the future, we hope to fnd ways to gain direct access to the labelers, for a more complete view.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CONCLUSION", "text": "In this paper, we explored the collaborative work-practices of teams that engage in the important work of ground-truth labeling for machine learning. We believe our principal contributions are as follows;\n\u2022 Experiences of ground-truth labeling \u2022 Collaborative practices in ground-truth labeling, especially focusing on division-of-labor during the creation of labels and the administration of diverse labeling processes and teams \u2022 Quality issues that arise during labeling, and mitigations through collaboration, division-of-labor, and the administrative control through fxed labeling vocabularies and documentation \u2022 Three practices in the collaborative design of ground truth \u2022 Integration with work in Human Centered Data Science, focusing on human practices with data \u2022 Proposals for technology developments that may aid labelers and their organizations\nWe shape our data science analyses, and the impact of our analyses on the world, through our design of data, including ground-truth labels. We hope that this paper will help us to understand our shared human accountability in the increasing complexities that we create.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Werner Geyer and Darrell Reimer for advice and recommendations to fnd labeling informants.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Novel index for objective evaluation of road detection algorithms", "journal": "IEEE", "year": "2008", "authors": "Jos\u00e9 Manuel \u00c1lvarez; Antonio Lopez"}, {"title": "Ground truth verifcation tool (GTVT) for video surveillance systems", "journal": "IEEE", "year": "2009", "authors": "Amol Ambardekar; Mircea Nicolescu; Sergiu Dascalu"}, {"title": "Power to the people: The role of humans in interactive machine learning", "journal": "", "year": "2014", "authors": "Saleema Amershi; Maya Cakmak; William Bradley Knox; Todd Kulesza"}, {"title": "Keeping the human in the data scientist: Shaping human-centered data science education", "journal": "Proceedings of the Association for Information Science and Technology", "year": "2019", "authors": "Theresa Dirndorfer ; Anderson ; Nicola Parker"}, {"title": "Scenario-based XAI for Humanitarian Aid Forecasting", "journal": "", "year": "2020", "authors": "Josh Andres; Christine T Wolf; Sergio Cabrero Barros; Erick Oduor; Rahul Nair; Alexander Kjaerum; Anders Bech Tharsgaard; Bo Schwartz Madsen"}, {"title": "Developing a research agenda for human-centered data science", "journal": "ACM", "year": "2016", "authors": "Cecilia Aragon; Clayton Hutto; Andy Echenique; Brittany Fiore-Gartland; Yun Huang; Jinyoung Kim; Gina Nef; Wanli Xing; Joseph Bayer"}, {"title": "Human expert labeling process (HELP): towards a reliable higher-order user state labeling process and tool to assess student engagement", "journal": "", "year": "2017", "authors": " Sinem Aslan; Eda Sinem Emine Mete; Ece Okur; Nese Oktay;  Alyuz; David Utku Ergin Genc; Asli Arslan Stanhill;  Esme"}, {"title": "Understanding the Impact of TVIs on Technology Use and Selection by Children with Visual Impairments", "journal": "", "year": "2019", "authors": "M Catherine; Lauren R Baker; Richard E Milne;  Ladner"}, {"title": "How many qualitative interviews is enough? Expert voices and early career refections on sampling and cases in qualitative research", "journal": "", "year": "2012", "authors": "Sarah Elsie Baker; Rosalind Edwards"}, {"title": "Crafting quality in design: integrity, creativity, and public sensibility", "journal": "", "year": "2012", "authors": "Shaowen Bardzell; K Daniela; Jefrey Rosner;  Bardzell"}, {"title": "Steps to an ecology of mind: Collected essays in anthropology, psychiatry, evolution, and epistemology", "journal": "University of Chicago Press", "year": "2000", "authors": "Gregory Bateson"}, {"title": "Old hat: craft versus design? interactions", "journal": "", "year": "2012", "authors": "Jonathan Bean; Daniela Rosner"}, {"title": "Soylent: a word processor with a crowd inside", "journal": "", "year": "2010", "authors": "Greg Michael S Bernstein;  Little; C Robert; Bj\u00f6rn Miller;  Hartmann; S Mark; David R Ackerman; David Karger; Katrina Crowell;  Panovich"}, {"title": "From the life-history approach to the transformation of sociological practice", "journal": "", "year": "1981", "authors": "Daniel Bertaux"}, {"title": "Science on the run: Information management and industrial geophysics at Schlumberger, 1920-1940", "journal": "MIT press", "year": "1994", "authors": "C Geofrey; C Bowker;  Geofrey;  Bernard Carlson"}, {"title": "Demystifying theoretical sampling in grounded theory research", "journal": "Grounded Theory Review", "year": "2009", "authors": "Jenna Breckenridge; Derek Jones"}, {"title": "Crowdsourcing for search evaluation", "journal": "ACM", "year": "2011", "authors": "R Vitor; Matthew Carvalho; Emine Lease;  Yilmaz"}, {"title": "Constructing grounded theory. sage", "journal": "", "year": "2014", "authors": "Kathy Charmaz"}, {"title": "Grounded theory and credibility", "journal": "Qualitative research", "year": "2011", "authors": "Kathy Charmaz; Antony Bryant"}, {"title": "Crowd disagreement about medical images is informative", "journal": "Springer", "year": "2018", "authors": "Veronika Cheplygina; P W Josien;  Pluim"}, {"title": "Non-user generated annotation on user shared images for connection discovery", "journal": "IEEE", "year": "2015", "authors": "Ming Cheung; James She; Xiaopeng Li"}, {"title": "Basics of qualitative research: Techniques and procedures for developing grounded theory", "journal": "Sage publications", "year": "2014", "authors": "Juliet Corbin; Anselm Strauss"}, {"title": "Towards crowdsourcing translation tasks in library cataloguing, a pilot study", "journal": "IEEE", "year": "2010", "authors": "Jonathan Corney; Andrew Lynn; Carmen Torres; Paola Di Maio; William Regli; Graeme Forbes; Lynne Tobin"}, {"title": "Automatic labeling of unlabeled text data", "journal": "US Patent", "year": "2004", "authors": "J Frederick; David E Damerau;  Johnson; C Martin;  Buskirk"}, {"title": "", "journal": "AI=Assisted Data Labeling. Demo at NeurIPS", "year": "2020", "authors": "Michael Desmond; Kristina Brimijoin; Evelyn Duesterwald; Narendra Nath Joshi; Michael Muller; Zahra Ashktorab; Aabhas Sharma; Casey Dugan; Qian Pan"}, {"title": "KNIME for open-source bioimage analysis: a tutorial", "journal": "Springer", "year": "2016", "authors": "Christian Dietz;  Michael R Berthold"}, {"title": "Sample size policy for qualitative studies using in-depth interviews", "journal": "", "year": "2012", "authors": " Shari L Dworkin"}, {"title": "Social translucence: using minimalist visualisations of social activity to support collective interaction. In Designing information spaces: The social navigation approach", "journal": "Springer", "year": "2003", "authors": "Thomas Erickson; Wendy A Kellogg"}, {"title": "Sounding board: A user-centric and content-driven social chatbot", "journal": "", "year": "2018", "authors": "Hao Fang; Hao Cheng; Maarten Sap; Elizabeth Clark; Ari Holtzman; Yejin Choi; A Noah; Mari Smith;  Ostendorf"}, {"title": "A design perspective on data", "journal": "", "year": "2017", "authors": "Melanie Feinberg"}, {"title": "Material Vision", "journal": "", "year": "2017", "authors": "Melanie Feinberg"}, {"title": "An Annotation Tool for Dermoscopic Image Segmentation", "journal": "Association for Computing Machinery", "year": "2012", "authors": "P M Ferreira; T Mendon\u00e7a; J Rozeira; P Rocha"}, {"title": "Collaborative Annotation for Reliable Natural Language Processing: Technical and Sociological Aspects", "journal": "John Wiley & Sons", "year": "2016", "authors": "Kar\u00ebn Fort"}, {"title": "Using a grounded theory approach to study online collaboration behaviors", "journal": "European Journal of Information Systems", "year": "2013", "authors": "Susan Gasson; Jim Waters"}, {"title": "Analyzing due process in the workplace", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "1986", "authors": "M Elihu; Susan Leigh Gerson;  Star"}, {"title": "Provenience and Provenance Intersecting with International Law in the Market for Antiquities", "journal": "NCJ Int'l L", "year": "2020", "authors": "Patty Gerstenblith"}, {"title": "Designing social translucence over social networks", "journal": "", "year": "2012", "authors": "Eric Gilbert"}, {"title": "Raw data is an oxymoron", "journal": "MIT press", "year": "2013", "authors": "Lisa Gitelman"}, {"title": "Discovery of grounded theory: Strategies for qualitative research", "journal": "", "year": "2017", "authors": "G Barney; Anselm L Glaser;  Strauss"}, {"title": "", "journal": "", "year": "2017", "authors": "Michele Goetz"}, {"title": "Ways Data Preparation Tools Help You Get Ahead Of Big Data", "journal": "", "year": "", "authors": ""}, {"title": "Practices of color classifcation", "journal": "", "year": "2000", "authors": "Charles Goodwin"}, {"title": "Crowdsourcing document relevance assessment with mechanical turk", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Catherine Grady; Matthew Lease"}, {"title": "Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass", "journal": "Eamon Dolan Books", "year": "2019", "authors": "L Mary; Siddharth Gray;  Suri"}, {"title": "Data science as political action: grounding data science in a politics of justice", "journal": "", "year": "2018", "authors": "Ben Green"}, {"title": "How many interviews are enough? An experiment with data saturation and variability", "journal": "Field methods", "year": "2006", "authors": "Greg Guest; Arwen Bunce; Laura Johnson "}, {"title": "Proactive wrangling: Mixed-initiative end-user programming of data transformation scripts", "journal": "", "year": "2011", "authors": "J Philip; Sean Guo;  Kandel; M Joseph; Jefrey Hellerstein;  Heer"}, {"title": "Invisible work, invisible skills: interactive customer service as articulation work", "journal": "Work and Employment", "year": "2005", "authors": "Ian Hampson; Anne Junor"}, {"title": "Combining crowdsourcing and google street view to identify street-level accessibility problems", "journal": "", "year": "2013", "authors": "Kotaro Hara; Vicki Le; Jon Froehlich"}, {"title": "The fourth paradigm: data-intensive scientifc discovery", "journal": "", "year": "2009", "authors": "Tony Hey; Stewart Tansley; Kristin Tolle"}, {"title": "Crowdsourcing scoring of immunohistochemistry images: Evaluating performance of the crowd and an automated computational method", "journal": "Scientifc reports", "year": "2017", "authors": "Humayun Irshad; Eun-Yeong Oh; Daniel Schmolze; M Liza; Laura Quintana;  Collins; M Rulla; Andrew H Tamimi;  Beck"}, {"title": "Fast and Automatic Visual Label Confict Resolution. Demo at NeurIPS", "journal": "", "year": "2020", "authors": "Aabhas Narendra Nath Joshi; Michael Sharma; Qian Muller; Michael Pan; Kristina Desmond; Zahra Brimijoin; Evelyn Ashktorab; Casey Duesterwald;  Dugan"}, {"title": "Learning from crowds and experts", "journal": "", "year": "2012", "authors": "Hiroshi Kajino; Yuta Tsuboi; Issei Sato; Hisashi Kashima"}, {"title": "Wrangler: Interactive visual specifcation of data transformation scripts", "journal": "", "year": "2011", "authors": "Sean Kandel; Andreas Paepcke; Joseph Hellerstein; Jefrey Heer"}, {"title": "Towards Efective Foraging by Data Scientists to Find Past Analysis Choices", "journal": "", "year": "2019", "authors": "Mary Beth Kery; Bonnie E John; O' Patrick; Amber Flaherty; Brad A Horvath;  Myers"}, {"title": "The story in the notebook: Exploratory data science using a literate programming tool", "journal": "", "year": "2018", "authors": "Mary Beth Kery; Marissa Radensky; Mahima Arya; Bonnie E John; Brad A Myers"}, {"title": "Refection companion: a conversational system for engaging users in refection on physical activity", "journal": "", "year": "2018", "authors": "Rafal Kocielnik; Lillian Xiao; Daniel Avrahami; Gary Hsieh"}, {"title": "Mapping Out Human-Centered Data Science: Methods, Approaches, and Best Practices", "journal": "", "year": "2020", "authors": "Marina Kogan; Aaron Halfaker; Shion Guha; Cecilia Aragon; Michael Muller; Stuart Geiger"}, {"title": "Ground truth data, content, metrics, and analysis", "journal": "Springer", "year": "2016", "authors": "Scott Krig"}, {"title": "Progress and its problems: Towards a theory of scientifc growth", "journal": "Univ of California Press", "year": "1978", "authors": "Larry Laudan"}, {"title": "Pseudo-label: The simple and efcient semi-supervised learning method for deep neural networks", "journal": "", "year": "2013", "authors": "Dong-Hyun Lee"}, {"title": "Identifying medical terms in patient-authored text: a crowdsourcing-based approach", "journal": "Journal of the american medical informatics association", "year": "2013", "authors": "Diana Lynn Maclean; Jefrey Heer"}, {"title": "Achieving data saturation: evidence from a qualitative study of job satisfaction", "journal": "Social and Management Research Journal", "year": "2018", "authors": ""}, {"title": "Building for social translucence: a domain analysis and prototype system", "journal": "", "year": "2012", "authors": "Stephanie David W Mcdonald; Mark Gokhman;  Zachry"}, {"title": "Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice", "journal": "Proceedings of the ACM on Human-Computer Interaction", "year": "2019", "authors": "Nora Mcdonald; Sarita Schoenebeck; Andrea Forte"}, {"title": "Crafting the image in surgical telemedicine", "journal": "", "year": "2016", "authors": "M Helena; Ahmed Mentis; Pierre Rahim;  Theodore"}, {"title": "The signifcance of saturation", "journal": "", "year": "1995", "authors": "Janice M Morse"}, {"title": "Curiosity, creativity, and surprise as analytic tools: Grounded theory method", "journal": "Springer", "year": "2014", "authors": "Michael Muller"}, {"title": "Human-Centered Study of Data Science Work Practices", "journal": "ACM", "year": "2019", "authors": "Michael Muller; Melanie Feinberg; Timothy George; J Steven; Bonnie E Jackson; Mary Beth John; Samir Kery;  Passi"}, {"title": "How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation", "journal": "", "year": "2019", "authors": "Michael Muller; Ingrid Lange; Dakuo Wang; David Piorkowski; Jason Tsay; Vera Liao; Casey Dugan; Thomas Erickson"}, {"title": "Critique and contribute: A practice-based framework for improving critical data studies and data science", "journal": "Big data", "year": "2017", "authors": "Gina Nef; Anissa Tanweer; Brittany Fiore-Gartland; Laura Osburn"}, {"title": "Synthetic sequences and groundtruth fow feld generation for algorithm validation", "journal": "Multimedia Tools and Applications", "year": "2015", "authors": "Naveen Onkarappa; D Angel;  Sappa"}, {"title": "Data vision: Learning to see through algorithmic abstraction", "journal": "", "year": "2017", "authors": "Samir Passi; Steven Jackson"}, {"title": "Trust in Data Science: Collaboration, Translation, and Accountability in Corporate Data Science Projects", "journal": "Proceedings of the ACM on Human-Computer Interaction", "year": "2018", "authors": "Samir Passi; J Steven;  Jackson"}, {"title": "Comparison of various classifcation algorithms on iris datasets using WEKA", "journal": "Int. J. Adv. Eng. Res. Dev.(IJAERD)", "year": "2014", "authors": "Kanu Patel; Jay Vala; Jaymit Pandya"}, {"title": "What is a question? Crowdsourcing tweet categorization", "journal": "", "year": "2011", "authors": "A Sharoda; Lichan Paul; Ed H Hong;  Chi"}, {"title": "", "journal": "", "year": "", "authors": "Saumen Jo\u00e3o Felipe Pimentel; Timothy Dey; Khalid Mcphillips; David Belhajjame; Leonardo Koop; Vanessa Murta; Bertram Braganholo;  Lud\u00e4scher"}, {"title": "International Provenance and Annotation Workshop", "journal": "Springer", "year": "", "authors": "& Yin;  Yang"}, {"title": "The politics of measurement and action", "journal": "", "year": "2015", "authors": "H Kathleen; Max Pine;  Liboiron"}, {"title": "The use of machine learning algorithms in recommender systems: A systematic review", "journal": "Expert Systems with Applications", "year": "2018", "authors": "Ivens Portugal; Paulo Alencar; Donald Cowan"}, {"title": "Understanding Older Adults' Participation in Design Workshops", "journal": "", "year": "2020", "authors": "Alisha Pradhan; Ben Jelen; Katie A Siek; Joel Chan; Amanda Lazar"}, {"title": "Informatics for materials science and engineering: datadriven discovery for accelerated experimentation and application", "journal": "Butterworth-Heinemann", "year": "2013", "authors": "Krishna Rajan"}, {"title": "Snorkel: Rapid training data creation with weak supervision", "journal": "The VLDB Journal", "year": "2020", "authors": "Alexander Ratner; H Stephen; Henry Bach; Jason Ehrenberg; Sen Fries; Christopher Wu;  R\u00e9"}, {"title": "Principles of data wrangling: Practical techniques for data preparation", "journal": "Reilly Media, Inc", "year": "2017", "authors": "Tye Rattenbury; M Joseph; Jefrey Hellerstein;  Heer"}, {"title": "RE: Defnitions of use", "journal": "Design studies", "year": "2008", "authors": "Johan Redstr\u00f6m"}, {"title": "On lies, secrets, and silence: Selected prose 1966-1978", "journal": "WW Norton & Company", "year": "1995", "authors": "Adrienne Rich"}, {"title": "A survey on data collection for machine learning: a big data-ai integration perspective", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2019", "authors": "Yuji Roh; Geon Heo; Steven Euijong Whang"}, {"title": "Exploration and explanation in computational notebooks", "journal": "", "year": "2018", "authors": "Adam Rule; Aur\u00e9lien Tabard; James D Hollan"}, {"title": "Project sidewalk: A web-based crowdsourcing tool for collecting sidewalk accessibility data at scale", "journal": "", "year": "2019", "authors": "Manaswi Saha; Michael Saugstad; Aileen Hanuma Teja Maddali; Ryan Zeng; Steven Holland; Aditya Bower; Sage Dash; Anthony Chen; Kotaro Li;  Hara"}, {"title": "ProvBook: Provenance-based Semantic Enrichment of Interactive Notebooks for Reproducibility", "journal": "P&D/Industry/BlueSky", "year": "2018", "authors": "Sheeba Samuel; Birgitta K\u00f6nig-Ries"}, {"title": "ReproduceMeGit: A Visualization Tool for Analyzing Reproducibility of Jupyter Notebooks", "journal": "", "year": "2020", "authors": "Sheeba Samuel; Birgitta K\u00f6nig-Ries"}, {"title": "Understanding expert disagreement in medical data analysis through structured adjudication", "journal": "", "year": "2019", "authors": "Mike Schaekermann; Graeme Beaton; Minahz Habib; L I M Andrew; Kate Larson; L A W Edith"}, {"title": "Expert Discussions Improve Comprehension of Difcult Cases in Medical Image Assessment", "journal": "", "year": "2020", "authors": "Mike Schaekermann; Carrie J Cai; Abigail E Huang; Rory Sayres"}, {"title": "Remarks on the complexity of cooperative work", "journal": "", "year": "2002", "authors": "Kjeld Schmidt"}, {"title": "Designing as refective conversation with the materials of a design situation", "journal": "Knowledge-based systems", "year": "1992", "authors": "A Donald;  Sch\u00f6n"}, {"title": "Philipp Schorch. 2020. Sensitive Heritage: Ethnographic Museums, Provenance Research, and the Potentialities of Restitutions. Museum and Society", "journal": "", "year": "2020", "authors": ""}, {"title": "Machines as teammates: A research agenda on AI in team collaboration", "journal": "", "year": "2020", "authors": "Isabella Seeber; Eva Bittner; O Robert; Triparna Briggs; Gert-Jan De De Vreede; Aaron Vreede; Ronald Elkins; Alexander B Maier; Sarah Merz; Nils Oeste-Rei\u00df;  Randrup"}, {"title": "Co-designing data experiments", "journal": "", "year": "", "authors": "Cathrine Seidelin; Yvonne Dittrich; Eric Gr\u00f6nvall"}, {"title": "Data Work in a Knowledge-Broker Organisation: How Cross-Organisational Data Maintenance Shapes Human Data Interactions", "journal": "", "year": "2018", "authors": "Cathrine Seidelin; Yvonne Dittrich; Erik Gr\u00f6nvall"}, {"title": "", "journal": "", "year": "", "authors": "& Bcs Learning;  Development Ltd; Gbr Swindon; Article 14"}, {"title": "Active learning literature survey", "journal": "", "year": "2009", "authors": "Burr Settles"}, {"title": "Use of deep learning in modern recommendation system: A summary of recent works", "journal": "", "year": "2017", "authors": "Ayush Singhal; Pradeep Sinha; Rakesh Pant"}, {"title": "The ethnography of infrastructure", "journal": "American behavioral scientist", "year": "1999", "authors": "Susan Leigh Star"}, {"title": "Steps toward an ecology of infrastructure: Design and access for large information spaces", "journal": "Information systems research", "year": "1996", "authors": "Susan Leigh Star; Karen Ruhleder"}, {"title": "Layers of silence, arenas of voice: The ecology of visible and invisible work", "journal": "Computer supported cooperative work", "year": "1999", "authors": "Susan Leigh Star; Anselm Strauss"}, {"title": "Anticipation work: Cultivating vision in collective practice", "journal": "", "year": "2015", "authors": "B Stephanie; Steven J Steinhardt;  Jackson"}, {"title": "Properties for growing grounded theory", "journal": "", "year": "2007", "authors": "P N Stern"}, {"title": "A public ideation of shape-changing applications", "journal": "", "year": "2015", "authors": "Miriam Sturdee; John Hardy; Nick Dunn; Jason Alexander"}, {"title": "Located accountabilities in technology production. Scandinavian journal of information systems", "journal": "", "year": "2002", "authors": "Lucy Suchman"}, {"title": "Data dif: Interpretable, executable summaries of changes in distributions for data wrangling", "journal": "", "year": "2018", "authors": "Charles Sutton; Timothy Hobson; James Geddes; Rich Caruana"}, {"title": "An approach for iris plant classifcation using neural network", "journal": "International Journal on Soft Computing", "year": "2012", "authors": "Madhusmita Swain; Sanjit Kumar Dash; Sweta Dash; Ayeskanta Mohapatra"}, {"title": "Data science of the social: How the practice is responding to ethical crisis and spreading across sectors", "journal": "", "year": "2018", "authors": "Anissa Tanweer"}, {"title": "Provenance as a Knowledge Organization Principle", "journal": "KO KNOWLEDGE ORGANIZATION", "year": "2020", "authors": "Natalia Tognoli; Jos\u00e9 Augusto Chaves Guimar\u00e3es"}, {"title": "Data scientist: The engineer of the future", "journal": "Springer", "year": "2014", "authors": "M P Wil; Aalst Van Der"}, {"title": "A survey on semi-supervised learning", "journal": "Machine Learning", "year": "2020", "authors": " Jesper E Van Engelen; H Holger;  Hoos"}, {"title": "Human computation", "journal": "", "year": "2008", "authors": " Luis Von Ahn"}, {"title": "Human-AI Collaboration in Data Science", "journal": "Proceedings of the ACM on Human-Computer Interaction", "year": "2019-11", "authors": "Dakuo Wang; Justin D Weisz; Michael Muller; Parikshit Ram; Werner Geyer; Casey Dugan; Yla Tausczik; Horst Samulowitz; Alexander Gray"}, {"title": "Human-AI Collaboration in Data Science: Exploring Data Scientists' Perceptions of Automated AI", "journal": "Proceedings of the ACM on Human-Computer Interaction", "year": "2019", "authors": "Dakuo Wang; Justin D Weisz; Michael Muller; Parikshit Ram; Werner Geyer; Casey Dugan; Yla Tausczik; Horst Samulowitz; Alexander Gray"}, {"title": "AutoAIViz: opening the blackbox of automated artifcial intelligence with conditional parallel coordinates", "journal": "", "year": "2020", "authors": "Daniel Karl; I Weidele; Justin D Weisz; Erick Oduor; Michael Muller; Josh Andres; Alexander Gray; Dakuo Wang"}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "journal": "", "year": "2009", "authors": "Jacob Whitehill; Paul Ruvolo; Tingfan Wu; Jacob Bergsma; Javier Movellan"}, {"title": "Mechanisms for data quality and validation in citizen science", "journal": "", "year": "2011", "authors": "Andrea Wiggins; Greg Newman; D Robert; Kevin Stevenson;  Crowston ; Tobin"}, {"title": "IEEE Seventh International Conference on e-Science Workshops", "journal": "IEEE", "year": "", "authors": ""}, {"title": "A Novel Tool for Capturing Conceptualized Audio Annotations (AM '10)", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Peter Woitek; Paul Br\u00e4uer; Holger Grossmann"}, {"title": "Conceptualizing Care in the Everyday Work Practices of Machine Learning Developers", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Christine T Wolf"}, {"title": "AI Models and Their Worlds: Investigating Data-Driven, AI/ML Ecosystems Through a Work Practices Lens", "journal": "Springer", "year": "2020", "authors": "T Christine;  Wolf"}, {"title": "Human-Centered Data Science: A New Paradigm for Industrial IoT", "journal": "Wiley Online Library", "year": "2018", "authors": "Matthew Yapchain"}, {"title": "How do Data Science Workers Collaborate? Roles, Workfows, and Tools", "journal": "", "year": "2020", "authors": "X Amy; Michael Zhang; Dakuo Muller;  Wang"}, {"title": "How do Data Science Workers Collaborate? Roles, Workfows, and Tools", "journal": "", "year": "2020", "authors": "Amy X Zhang; Michael Muller; Dakuo Wang"}, {"title": "Active image labeling and its application to facial action labeling", "journal": "Springer", "year": "2008", "authors": "Lei Zhang; Yan Tong; Qiang Ji"}, {"title": "Learning from labeled and unlabeled data with label propagation", "journal": "", "year": "2002", "authors": "Xiaojin Zhu; Zoubin Ghahramani"}, {"title": "Rational choice and the diversity of choices", "journal": "The Journal of Socio-Economics", "year": "1998", "authors": "Laszlo Zsolnai"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Trovato and Tobin, et al.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Major codes and interpretation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Quantitative Information about Interviews", "figure_data": "15Interview duration (minutes)30-75Transcript pages433Transcript words99,124Extracted quotations240Words in extracted quotations 12,584Open codes (total)1592Open codes (unique)619major Axial codes252.2.3 Summary and a Look Forward."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Informants' Domains and Roles", "figure_data": "ID DomainRoleMethodI-01 ChatbotProject leadOnlineI-02 Labeling algorithmTeam leadFace-to-faceI-03 ChatbotResearch engineerFace-to-faceI-04 HealthcareResearcher/NLPFace-to-faceI-05 HealthcarePhysician researcher OnlineI-06 ChatbotProject leadOnlineI-07 Socisl media marketing Team leadOnlineI-08 Financial/NLPData scientistOnlineI-09 Formatted texts/PDFsManagerOnlineI-10 Structured textsResearcherOnlineI-11 Conversational qualityResearcherFace-to-faceI-12 Conversational qualityResearcherOnlineI-13 Insurance industry/NLP Team leadOnlineI-14 ImagesProject leadOnlineI-15 Multiple projectsManagerOnline"}], "doi": "10.1145/3411764.3445402"}