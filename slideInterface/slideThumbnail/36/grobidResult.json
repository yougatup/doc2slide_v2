{"authors": "Krishna Subramanian; Jan Borchers; Johannes Maas; James Hollan", "pub_date": "", "title": "From Detectables to Inspectables: Understanding Qalitative Analysis of Audiovisual Data", "abstract": "Figure 1: Qualitative researchers work with audiovisual recordings to develop insights. The fgure shows a video recording of a study participant using her computer. As a researcher, we want to understand why the participant gets distracted during work. To do this, we look for audiovisual segments where the participant is talking to a colleague-these are the inspectables (marked with yellow brackets). To fnd these, we need to look for instances where the participant looks away from the computer to face their colleague, which are the detectables (marked with green crosses).", "sections": [{"heading": "INTRODUCTION", "text": "Qualitative research methods are often employed when data cannot be quantifed. They are commonly used in human-computer interaction (HCI) [1] and social science [18,35] research. In HCI, qualitative methods are becoming increasingly prevalent. While 44% of papers from CHI 2014 used qualitative analysis methods [5], a survey of CHI 2019 papers [34] shows that this has increased to 62%. With qualitative research paradigms and software tools being employed more frequently, we need to understand how these research practices and accompanying tools are being used.\nCoding (also called annotation) is an important component of qualitative analysis. Researchers use keywords to categorize data and subsequently use these categories to detect patterns and anomalies. Researchers often code textual transcripts, e.g., of interviews, instead of the audiovisual recordings themselves. An important reason for this is that text is easier to search and navigate than audiovisual recordings [20,25,32]. However, audiovisual recordings contain additional rich information about behavior and user interaction not available in text transcripts [12]. Further, creating transcripts is tedious, and researchers often avoid doing it themselves, e.g., by paying student assistants or professionals to create them [28]. Because of these reasons, researchers often need to work directly with the audiovisual recordings. Despite the increasing frequency and importance of qualitative analyses of text-based transcripts and audiovisual recordings, there is a lack of research aimed at understanding researchers' needs and workfows.\nTo address this, we conducted interviews and a survey to help understand researchers' workfows when qualitatively analyzing audiovisual recordings. Specifcally, we addressed the following questions: How common is direct analysis of audiovisual recordings? Are transcripts involved when audiovisual recordings are available? How do researchers analyze audiovisual recordings? What do they look for? Where do they look?\nAs we will detail below, we found that researchers employ a predominantly bottom-up coding approach, in which they determine 'what to look for' only after watching the recordings, often multiple times. Signifcant time and efort during analysis is dedicated to locating and analyzing the segments in the recordings that researchers deem interesting. To locate these interesting segments, researchers search for visual and auditory cues, such as a button click or a particular sound, in the recordings that signal the presence of an interesting segment. Towards the end of the analysis, researchers seek information in the recordings, such as interesting user quotations and screenshots, with the purpose of reporting them in a publication or report. We discuss these and other fndings, and present some design recommendations to improve researchers' analysis workfows.\nThe novel contributions of this paper are:\n\u2022 Results from interviews and a survey that help to provide an understanding of the prevalence of direct audiovisual analysis in qualitative HCI research, and researchers' corresponding transcription, coding, and navigation practices.\nOur contribution here includes the vocabulary and concepts of inspectables, detectables, and reportables which help understand and structure discussions about qualitative audiovisual analysis. \u2022 A discussion of how automation could help researchers deal with the important but laborious task of locating interesting audiovisual segments in recordings, and design recommendations based on our fndings.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "BACKGROUND AND RELATED WORK", "text": "In this section, we present some background information about qualitative analysis approaches and researchers' analysis workfows. We then provide an overview of previous research-based and commercial tools that support qualitative analysis of audiovisual data, before discussing existing techniques for audiovisual navigation, summary, and visualization.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Approaches to qualitative analysis", "text": "Common qualitative research methods include grounded-theory based qualitative coding [36] and afnity diagramming [2]. In both methods, researchers identify interesting text or audiovisual snippets, categorize them, and use these categories to detect patterns or trends. While these methods dictate the researchers' overall approach to analysis, individual research felds tend to follow more specifc analysis methods. For example, in interaction analysis in HCI, the focus is to understand how humans interact with other humans, objects, and interfaces [22].", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Researchers' analysis workfow", "text": "Previous research has investigated how researchers perform qualitative analysis. Melgar et al. proposed a generic process model that describes the various steps in media analysis by researchers [30]; this work is grounded in earlier research on media analysis workfows. The resulting model includes four phases that help us understand researchers' workfows: exploration (background research, develop research intent, gather data), assembly (select corpus, collect more data if needed, select part of the corpus for analysis), analysis (pre-analyze corpus, conduct exploratory data analysis via visualizations, annotate data), and presentation (organize data and collect evidence for presentation). Although this model provides an overview of various tasks in media analysis by researchers, it does not investigate the mechanics of interaction employed in these tasks.\nOur paper is motivated in particular by Marathe and Toyama's CHI '18 paper that sought to understand how qualitative researchers interact with current software tools [27]. They found that the granularity and type of codes vary across diferent disciplines, and investigated opportunities to automate certain tasks in qualitative analysis, e.g., with codebook development and management. Their paper focused on text-based qualitative analysis. We investigate direct analysis of audiovisual recordings.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Software support for qualitative analysis of audiovisual data", "text": "Software applications used for qualitative analysis are referred to as Qualitative Data Analysis (QDA) tools. QDA tools beneft researchers with organization of media fles and powerful features to manage codebooks. While there is concern that QDA tools impact the analysis method [13], the core of this concern has largely been refuted [3,38,41].\nTo understand how efective QDA tools are in supporting qualitative analysis, a comprehensive study of QDA tools was conducted as a part of the KWALON experiment [12]. A multimedia data corpus was analyzed by diferent teams utilizing various mainstream QDA tools. The results showed that due to the wide variety of tools, researchers would have to assess whether a specifc tool will suit their workfow [31].\nThe most widely used commercial QDA tools are ATLAS.ti, 1 NVIVO, 2 MAXQDA, 3 Transana, 4 and HyperRESEARCH. 5 Transana is the only listed tool that prioritizes audiovisual data over text [12]. There are some research-based QDA tools that support audiovisual analysis, such as ELAN (EUDICO Linguistic Annotator) [40], INTERACT, 6 and Observer. 7 Some research tools tackle specifc problems in audiovisual analysis. ChronoViz allows users to view and annotate diferent types of time-coded data streams, such as videos, sensor data, and feld notes, to support more comprehensive analysis [14]. MiMeG 8 (Mixed Media Grid) and DRS [4] help with collaboration and organization of multimedia fles respectively. The goal of our research is to understand how HCI researchers use these commercial and researchbased tools, in order to identify potential avenues for improvement.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Techniques to navigate, summarize, and visualize audiovisual content", "text": "The fnding that navigating time-coded media streams is more tedious than navigating textual data is well established in research [20,25,32]. This has spurred several techniques that aim to improve video navigation, e.g., by allowing multiple perspectives of videos [10], media navigation at diferent levels of granularity [6,9,33], and video navigation through direct manipulation of objects in the video [11,17,23].\nThere have also been eforts to summarize audiovisual content in order to reduce viewing or listening time. For audio, this includes eliminating silence or speeding up clips [37] and summarizing spoken text [15]. For video, techniques include providing video thumbnails [24], video skims [8], and keyframe summaries", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "HOW RESEARCHERS ANALYZE AUDIOVISUAL RECORDINGS", "text": "In this section, we present fndings from interviews with researchers who analyze audiovisual recordings using qualitative research methods. We conducted a survey to validate our interview fndings, the results of which are discussed in the next section.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data collection and method", "text": "To recruit participants for our interviews, we reached out to experienced HCI researchers who had conducted audiovisual analyses as well as qualitative researchers we knew personally or through connections. We also recruited participants through mailing lists and by emailing psychology researchers at our university. We recruited ten participants (IP1-IP10, see Table 1) for the interview. The frst three interviews (IP1-IP3) were exploratory, designed to give us a general understanding of their workfows and where the most acute problems lie. These exploratory interviews helped us identify research questions 9 that guided the next fve interviews (IP4-IP8). In all interviews, after inquiring about each participant's background and expertise, we had them discuss their intentions during their most recent analysis and walk us through their data collection and analysis. We encouraged participants to keep their recent analysis fles and analysis tool ready so that they could walk us through their work. This simultaneously provided us with better understanding of their analysis and gave participants the opportunity to refect on their analysis process. Then we probed for specifc things our participants looked for and discussed how they looked for each. Analyzing data from these eight participants revealed patterns that were strong enough to warrant triangulation via a survey (see Section 4). After the survey, since we wanted to gather specifc examples of what researchers look for in audiovisual recordings, we recruited two more participants (IP9 and IP10) for these focused interviews.\nOur participants included two Master's students, six PhD students, and two professors; they had varying amounts of experience with qualitative analyses of audiovisual recordings. All participants conducted research in the feld of HCI, except for IP6 who worked in psychology.\nDue to COVID-19, all interviews were conducted remotely using Jitsi 10 and Skype 11 . We recorded the session audio with the participants' consent, and took notes to inform our analysis.\nThe second author transcribed the interviews, and identifed interesting text snippets and insights from the transcripts and study e Exploratory with POIs: exploratory, but with points of interest they were looking for; rough scheme: started with a rough scheme that then evolved; tabular notes: noted observations in a table; clips: extracted clips for detailed inspection.\nTable 1: Information about interview participants, including their research intent, analysis approach in a recent analysis, and analysis tools.\nnotes. Some interviews were not conducted in English; the corresponding verbatim quotes reported in this paper are translations. The frst and second authors organized the snippets and insights in the style of an afnity diagram 12 [19], in which the categories were developed from the data.\nWe now present the main fndings from our analysis of the interviews organized into three subsections. In Section 3.2, we discuss participants' overall research intent, and how this afected whether they analyze mainly transcripts or audiovisual recordings. In Section 3.3 and Section 3.4, we discuss what information participants looked for in audiovisual recordings, the navigation methods they used to look for this information, and introduce inspectables, detectables, and reportables. We summarize the key fndings in Section 3.5.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "How researchers prepare to work with audiovisual recordings", "text": "3.2.1 Exploratory workflow and opportunistic navigation. Our participants reported beginning their analysis with broad research questions. Several research questions were theory-building, e.g., understanding how users perform a complex task using various programming IDEs, understanding how users react to sensory interventions at regular intervals, or investigating how users use certain tools for learning. Other research questions were about validating a prototype or software application, e.g., by understanding what insights users of a tool would develop, by checking if users could understand the interaction ofered by the tool, or by determining the efect a prototype has on learning outcomes. These research questions served as a starting point for the qualitative analysis. Participants reported to know what events to look for only at an abstract level; none reported using a predefned codebook or annotation scheme before watching or listening to the recordings. This is similar to the open coding phase in text-based qualitative analysis, in which researchers determine an initial set of codes only after getting acquainted with the data. To get acquainted with the recordings, participants reported employing an opportunistic style of navigation, such as playing videos at a faster speed (IP2, IP5, and IP7), listening to the discourse in the background (IP5), and scrubbing the video to identify salient video segments (IP7). Also, several participants (IP1, IP2, IP3, IP6, and IP8) explicitly mentioned that they approached the analysis with an open mind, trying to \"pursue what the data says\" (IP6), which indicates the prevalence of an inductive analysis approach. The evolution of their codebooks or annotation schemes continues throughout the analysis (IP1, IP4, and IP6), sometimes over multiple iterations (IP6).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "What gets annotated-transcripts vs. audiovisual recordings.", "text": "Our participants' research questions dictated their transcription practice as well as what gets annotated. Transcripts are textual descriptions that capture information in audiovisual recordings.\nWhile certain kinds of information, such as the verbal discourse and observable user actions, require little to no interpretation, others, such as body language and other non-verbal cues, require more nuanced interpretation and are difcult to describe adequately in transcripts. We found that participants either annotated the audiovisual recordings directly, or they analyzed mainly textual transcripts, using the audiovisual recordings for clarifcation.\nIP1 and IP8 did most of the analysis working directly with the audiovisual recordings. IP8 extracted about 20 video clips from their recordings, so that these clips could be played in a loop via a conventional video player for a more focused analysis. Many participants (IP2, IP3, IP4, IP5, and IP7) performed what we classify as partial transcription of the recordings, and mainly used these transcripts for analysis. However, they also relied upon recordings during the analysis: IP2 and IP3 viewed certain video segments after transcription to observe details, such as a user's emotions, that were missing from the transcript (\"[The] participant did not say anything while they [were] struggling. [. . . ] if we watch the video, we will be able to note down that [. . . ] the participant [. . . ] expressed some frustrations.\" -IP2). IP4, IP5, and IP7 did not transcribe the verbal discourse in a conventional sense, but took notes on specifc events while watching the videos that were used in the analysis. IP6 prepared full transcripts and used them exclusively during the analysis.\nTranscripts are not always limited to verbal information. For example, IP3 included what component of the interface the user was using into the transcript for context (\"[. . . ] we had to see, OK, so he is moving the slider, and then write in brackets behind that that it was the [bottom] slider. \" -IP3).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "What researchers look for in audiovisual recordings", "text": "Our fndings indicate that most participants either worked with audiovisual recordings directly or used them as a supplement when analyzing partial transcripts, e.g., to watch salient events in detail.\nIn this section, we present fndings about participants' annotation practice, which typically begins after getting sufciently acquainted with the videos to determine an initial set of codes or events to look for.\nThere are broadly two types of information participants looked for in the recordings, which we coin inspectables and reportables.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Inspectables.", "text": "During analysis, one main task was to identify content in the recordings or transcripts that needed to be further inspected and refected upon. We call the audiovisual segments that correspond to this content inspectables. The goal is to fnd these inspectables in the recordings, and analyze them further to derive useful insights. Below are a few examples of inspectables (in italics) that are inspired by examples from our interviews.\n\u2022 To determine whether users understand how a slider interaction works, the researcher wants to inspect the audiovisual segments after the user interacts with the slider. \u2022 To determine how users converse with a voice assistant, the researcher wants to inspect the audiovisual segments in which the user responds to the voice assistant.\n\u2022 To determine reasons why a user paused a video, the researcher wants to inspect the audiovisual segments before and after the video is paused in the recording. To analyze inspectables, researchers need to fnd them in the recordings, which can be time-consuming. We discuss how our participants located these inspectables in Section 3.4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Reportables.", "text": "Towards the end of the analysis, some participants looked for information in the recordings or transcripts, which they reported in their publication. We call such content reportables. From our interviews, we identifed several instances of reportables, such as quotes from participants (IP1, IP2, IP3, and IP5), images or screenshots of the interface or user behavior (IP1, IP5, and IP8), and video clips for creating a supplementary videos accompanying a written publication (IP5). For IP1, IP5, and IP7, identifying reportables was a separate task, disjoint from the main coding task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "How researchers navigate recordings to seek information", "text": "We discussed earlier that to develop an initial set of codes and determine what events to look for, participants used an opportunistic approach. Participants also used the three following targeted information-seeking approaches, in which they used specifc features to locate the inspectables and reportables in recordings. The frst two approaches result in time-based navigation, whereas the last approach requires the user to look for audiovisual cues in the recordings to locate inspectables.\n3.4.1 Time codes for backtracking. Many participants recorded time codes 13 alongside their feld notes or during transcription. These time codes allow participants to backtrack to the relevant audiovisual segment in the recording. For example, IP2, IP3, and IP5 recorded time codes of relevant audiovisual segments alongside the user quotations they transcribed in case they needed clarifcation. Some participants captured time codes by annotating the relevant audiovisual segments before analysis, e.g., during transcription. Although time codes reduce the time required to locate interesting audiovisual segments, they need to be manually recorded either during the study itself or during transcription. Due to the exploratory nature of qualitative research, researchers cannot reliably determine which audiovisual segments are interesting before they begin the analysis. Therefore, backtracking via time codes is not a readily available navigation method in most qualitative research.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "3.4.2", "text": "Using information about the study structure. Information about the study structure, such as timeline of tasks and experimental conditions, can help researchers locate inspectables. IP5 provided a good example of using a study's structure to help narrow down possible locations where inspectables might occur (\"[The event] happens fve times, every three minutes, [for] 15 seconds, right? [I] have basically looked at minute 3-and looked, uh, about half a minute before, what they are talking about, and then a minute later, has something else [i.e. interesting] happened?\" -IP5).\n3.4.3 Detectables. Some participants provided examples of using visual or auditory cues in the recording to locate inspectables. The inspectables represented events that were too hard to fnd in the recording, but were usually signaled by the presence of another cue that they could more easily detect. We call these cues detectables.\nTo help understand how inspectables and detectables work together, consider the following hypothetical analysis. Imagine that we would like to understand how people use Wikipedia. 14 We want to investigate why users navigate within and across a wiki page. We perform a screen capture of participants using Wikipedia. When watching the recordings, we observe that participants often go back to previous pages, and we become interested in understanding the various reasons why users backtrack.\nTo make progress with this research question, we want to locate and analyze all video segments before users backtrack-these are our inspectables. We can locate them using certain cues, e.g., we notice that participants always press the browser's back button to backtrack. 15 Thus, the back button getting pressed is a detectable that acts as an indicator for the inspectable. We can navigate the recording looking specifcally for back button presses, e.g., by quickly going through the video to identify instances where the back button is clicked. Note that we were not aware of this detectable before collecting our data, and thus could not have generated logs to allow for an easier detection of button clicks. While in the given example the detectable was objective and concrete, we also consider more abstract, subjective cues detectables. For example, if you want to investigate the instances where participants became angry, a suitable detectable might involving analyzing participants' facial expressions. One reason for distinguishing detectables and inspectables is that detectables lend themselves more readily to automatic detection-a topic we will return to in Section 5.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Key takeaways", "text": "In summary, the key fndings from our interviews are:\n\u2022 Audiovisual recordings contain rich information not available in text. Most participants either coded the recordings directly, or used them in addition to transcripts. \u2022 Most participants do not know what to look for before the analysis begins. \u2022 After familiarizing themselves with the recordings, participants' wanted to locate and analyze interesting audiovisual segments called inspectables. \u2022 To fnd inspectables, which can be tedious, participants used visual and auditory cues called detectables. \u2022 Towards the end of the analysis, participants also sought information like screenshots and quotes to report in their work. We call such information reportables.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Method", "text": "To get responses from a representative sample of HCI researchers, we sent the survey to frst authors of CHI '19 papers that reported qualitative analysis of audiovisual recordings. 17 We excluded two authors who had given feedback on a pilot of our survey to avoid any conficts or biases. We also sent the survey to a local HCI research lab's mailing list and a Slack 18 group for HCI researchers. The survey was closed after two weeks. 66 respondents took the survey, and the survey took about ten minutes to complete.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Details of survey respondents", "text": "Although we had no mechanism to detect whether the answer came from a CHI '19 author or from our other postings, we suspect that the majority of respondents are CHI authors, as many of them had responded to our email confrming their participation. Respondents from ten diferent nationalities took the survey, with most from the US (54%) or the UK (19%), and were either PhD students (65%), academic researchers (12%), e.g., postdoctoral researchers and professors, or Master's students (12%). Only 23% of the respondents had conducted fewer than three qualitative analyses with audiovisual data, while 36% had fnished 3-5, 23% between 6 and 20, and 18% had conducted over 20 analyses.\nWe present selected results from the survey that informed our understanding of how researchers analyze audiovisual recordings.\nRespondents were asked to answer the survey questions in the context of their most recent qualitative analyses of audiovisual data. If they wanted to contribute multiple analyses, they were instructed to take the survey multiple times.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "What gets analyzed? Audio vs. video", "text": "24% of respondents used only audio recordings, 61% used both audio and video recordings, and only 3% used solely video recordings (i.e., without audio). Analyses that used only audio tend to follow a similar workfow, e.g., all such analyses utilized a full transcription of the recordings. Analyses that involved both audio and video, on the other hand, did not exhibit a unifed workfow. We found two sub-groups of analyses that involved both audio and video:\n\u2022 Analyses that focused primarily on the audio, in which the video acted as a supplement, e.g., a think-aloud study with a screen recording. Here, the analysis was done primarily on transcripts, and the recordings were used only to clarify ambiguous statements, e.g., \"this button is broken. \" \u2022 Analyses that focused on rich behavior in both audio and video, e.g., a longitudinal study about drivers' GPS usage. Since such studies are interested in nuanced behavior that is difcult to capture in transcripts, researchers need to analyze the recordings directly.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Types of qualitative research", "text": "From our interviews, we identifed diferent types of qualitative analyses. Respondents were asked which of the following descriptions better ft their research questions:\n\u2022 \"Theory building. For example, modeling people's behavior when using a piece of technology, or understanding the meaning of objects in people's lives.\" \u2022 \"Validation of an artifact or hypothesis. For example, testing a software prototype, or testing retention of information with a new learning method.\" \u2022 \"Measurement. For example, the time spent in diferent locations, or the time spent talking about diferent topics.\"\nRespondents could select multiple answers and give an open-ended text comment. 63% of analyses were categorized as theory building, and 29% as validation. Analyses involving measurement were rare (3%), as expected for qualitative analysis.\nMost validation studies had both audio and video recordings available (17 out of 19 responses). On the other hand, 67% of analyses involving only audio were theory building. This is to be expected since most feld researchers use interviews, focus groups, and diaries for data collection, and the largely verbal content is typically audio-recorded and transcribed to allow for text-based analysis.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "How exploratory are audiovisual analyses?", "text": "In our interviews, we found that most researchers adopt a rather exploratory style of analysis, where they identify what is interesting only after watching the recordings. With our survey, we wanted to see if this fnding holds true for a larger sample and, more importantly, assess the degree of exploration in typical analyses. To do this, we asked respondents to rate the two following statements on a fve-point Likert scale:\n(1) \"Before starting the analysis, I knew clearly where in the recordings to look and what concretely to look for.\" (2) \"Only after I watched (some or all of) the recordings did I fnd concrete events or aspects to focus the analysis on.\"\nWe designed the statements to be contrasting-if a researcher knew where and what to look for (1), they would not need to watch the recordings before fnding concrete aspects to focus on (2). Most respondents disagreed with the frst statement (51% rated it 1 or 2, 29% rated it 4 or 5), while most agreed with the second statement (63% rated it 4 or 5, 18% rated it 1 or 2), mostly confrming that the statements are contrasting.\nThe frst question, which we anticipated to provoke disagreement, had 29% agree (rated it 4 or 5), whereas the second statement does not refect this group-only 15% disagreed (rated it 1 or 2). Upon further inspection, out of the 14 respondents who rated the frst statement with 4 or 5, 12 used audio and video recordings. It thus appears that there are certain researchers using audio-andvideo recordings who have a predetermined notion of what to look for. Interestingly, with their higher rating for the second statement, they agree that they need to watch the recordings before fnding concrete aspects. One explanation for this is that while these researchers might have had predetermined research questions, they nevertheless had to explore the recordings before they can defne their inspectables (see Section 3.3.1).\nWe also aimed to quantify how often the coding scheme is modifed during the analysis, another measure for how exploratory it is. We asked respondents to rate the following statement: \"I had a coding or classifcation scheme (from the start or after observing some of the recordings) that was mostly fxed and that I applied in the rest of the analysis. \" 65% indicated that their coding scheme was not fxed (rated 1 or 2). Only 24% reported using a fxed scheme (rated 4 or 5).\nOverall, these statistics show that more than half of analyses are exploratory, evident from the evolving coding schemes and the researchers' need to immerse in the recordings before knowing what to focus the analysis on.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Transcription", "text": "We asked respondents to rate the following statement on a Likert scale of 1-5: \"All information necessary for the analysis was frst extracted from the recordings into a more convenient form for analysis. (For example, into a transcript, as quotes, or as video clips.)\" 58% of the responses indicated a strong agreement (rating of 5) with the statement. Only 15% indicated that they do not extract the information for the analysis (ratings of 1 or 2). While the statement deliberately left open what kind of extraction was used, this suggests that the recordings had to be prepared for analysis.\nThe most common form of such data preparation is transcription. The survey contained two questions that aimed to identify the role of transcripts in qualitative analysis: what type of transcripts were used (full, partial, or none), and how much they were used during analysis.\n72% of respondents reported preparing a full transcription of their recording, 20% prepared a partial transcription (\"only interesting statements\"), and 8% prepared no transcripts. As mentioned earlier, all respondents who had only audio recordings prepared a full transcription.\nTo understand our respondents' reliance on transcripts versus working with the recordings directly, we identifed four levels of transcript use based on our interviews and literature review:\n\u2022 \"I used only the transcript and did not revisit the recordings at all. \" \u2022 \"I used mainly the transcript and revisited the recordings only to clarify in case of ambiguity or errors in the transcript. \"\n\u2022 \"The analysis required information not present in the transcript, so I used both the transcript and the recordings. \" (in the following referred to as having an insufcient transcript.) \u2022 \"I primarily used the recordings instead of the transcript. \" 13% of respondents used transcripts exclusively in the analysis, and 60% used the audiovisual recordings only for corrections. 25% had insufcient transcripts and one respondent (2%) reported having a full transcript but primarily used the recordings in the analysis.\nAll respondents whose analyses involved only audio recordings either completely relied on transcripts or used recordings only for corrections. However, when both audio and video was available, the use of transcripts was split: 50% of audio-and-video analyses with a transcript used the recordings only for corrections, whereas 44% considered their transcripts insufcient. The two remaining participants either relied entirely on the transcript or on the recordings.\nThis confrms our interview fnding that in most analyses that involve audiovisual recordings, recordings are either used exclusively, for corrections, or for specifc tasks alongside transcripts. Overall, 32% of participants reportedly did not rely solely on a transcript, but required direct analysis. 19 ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Key takeaways", "text": "In summary, the following are the key fndings we can takeaway from our survey:\n\u2022 Text-based analysis is dominant: 58% of respondents extracted information from their recordings into a more convenient form for analysis. 71% used full transcripts, and 20% used partial transcript. \u2022 Transcripts are not always adequate: 32% of respondents could not rely solely on transcripts and needed to work with the recordings directly. \u2022 Most qualitative analyses that involve audiovisual recordings are exploratory: 63% are theory building. 53% of respondents did not know what to look for in the videos before analysis, and 65% changed their coding scheme during the analysis.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION: AUTOMATION IN FINDING DETECTABLES", "text": "From our interviews and survey, we found that a substantial number of qualitative analyses involve direct analysis of audiovisual recordings. In such analyses, researchers face the laborious task of locating and further analyzing interesting segments, the inspectables, in the recording (see Section 3.3.1). Since inspectables can be abstract and hard to fnd in the recordings, we found that researchers locate them using information about study structure, backtracking, or detectables (see Section 3.4). Of these, information about study structure and backtracking are time-based navigation techniques, and are already well supported by the traditional timebased media navigation interfaces in QDA tools. On the other hand, to fnd detectables, which are visual or auditory cues, researchers have to manually search the recordings.\nTo help with this tedious task of searching for detectables, automation via software support could be useful, as suggested by IP3 when prompted to imagine a magical tool that could help them with qualitative analysis: \"We have done the analysis once completely. And then we go back and see, aha, OK, is that maybe an interesting pattern which we haven't yet thought of. Then [a magical tool] would, so to say, let us extract these [. . . ]\" -IP3 In this section, we discuss whether such automation is warranted, how efective it can be, and discuss considerations for design.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Call to automation in fnding detectables", "text": "Since the detectables lend themselves more readily to detection, we propose to use simple techniques like image recognition and sound detection to automatically fnd potential locations of detectables in audiovisual segments. The goal is not to automate the entire analysis, but only to help researchers with a tedious task, that of locating possible locations of detectables of audiovisual segments.\nTo show how automation can be used to fnd detectables, we provide two examples inspired by our interview fndings: \u2022 Building on the example from Section 3.4.3, imagine we want to fnd all instances where users click the browser's back button. Since the button gets darker when it is clicked, we could capture the average brightness of the button's pixels and mark all frames where the brightness is below a certain threshold. This would lead us to all segments where the back button was clicked.\n\u2022 Imagine that we want to investigate what participants said to trigger a voice assistant. Since the assistant makes a characteristic activation sound before replying, we could automatically search for that activation sound in the recording.\nAfter marking all such instances, we could much more easily inspect the segments of the recording leading up to the response.\nWe now discuss the scope of such automation across various tasks in qualitative analysis and its application to various research questions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Scope of automation.", "text": "The most prominent application of automation is to fnd inspectables via detectables. Additionally, fnding reportables, i.e., user quotes or screenshots, may also require audiovisual cues, which can beneft from automation.\nAutomation can even be useful before the analysis. From our interviews, we found that researchers who perform partial transcription often look for interesting segments in their recordings, which they transcribe for a later in-depth analysis. This process is similar to searching for detectables, and can beneft from automation.\nThe approach to automation we propose is not dependent on the research question (see Section 4.4). It is instead limited only by whether the detectables in the recording, which are informed by the research questions, can be reliably detected by software. For example, in behavioral analysis, researchers often looks for subtle non-verbal cues in study participants. Such detectables are hard to detect automatically.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Design considerations for automation", "text": "Our call for automation brings up a classic question in qualitative research: Is it desirable to automate qualitative researchers' workfow? Earlier research has called for the use of computation techniques in qualitative analysis [7]. Work from Marathe and Toyama on qualitative analysis of textual content fnds that there is potential to automate many parts of qualitative research [27], but that researchers want automation to be transparent, and desire automation only after coming up with a codebook and coding some data. Our interview participants expressed similar concerns for automation in analysis of audiovisual content: (\"[. . . ] Sort of a good balance between not letting others analyze my data so that [as a consequence] I have no connection to the data, but also removing the redundant parts of the analysis. \" -IP1).\nWe now discuss some design considerations for automation based on the insights we obtained in our interviews, survey, and literature review: 5.2.1 Use simple, explainable, and reproducible techniques that focus on detecting primitives. It is important for automation to be transparent and allow users to operate at a concrete level, e.g., by detecting and tracking basic auditory and visual properties like color, position, and audio frequency. Automation could also combine simple operations that target primitives, e.g., to fnd all red squares that are within a rectangle, similar to existing stream or video algebras [26,39].\nWhile complex techniques like neural networks may allow us to detect more abstract, complex instances of detectables, they may also cause the researcher to lose trust in the software, especially for subjective tasks like determining whether a person in a recording is frustrated. For this reason, it is preferable to use simple deterministic algorithms that can be easily understood, replicated, and explained to others. In addition to promoting trust in automation, such an approach also results in a more reproducible research. This discussion is related to the ongoing discussion about explainable AI.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "5.2.2", "text": "Limit the role of automation to finding potential locations of detectables. Automation should take a backseat in qualitative analysis. Its role is to help the researcher fnd potential locations of detectables, not analyze the inspectables. It is important for automation to minimize false negatives even if that increases false positives. False positives can be manually inspected and removed by the researcher. False negatives, however, would require the researcher to look through the entire recording, which is what the automation was supposed to prevent.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "5.2.3", "text": "Incorporate other data when available. Software that uses automation should incorporate other data collected in user studies, when available, so that automation can use these additional data sources to help locate detectables. For example, assume that, in a study, the researcher has captured feld notes along with time codes in a digital format. These notes can then be used as an additional data source for locating detectables, e.g., to highlight video segments corresponding to all occurrences of a keyword in the feld notes. ChronoViz is an existing tool that supports visualization of time series data from multiple sources, such as sensors, digital notes, and GPS, to provide a holistic view of the data collected in user studies [14]. We envision future QDA tools to adopt a similar approach, with an added support for automatically fnding detectables.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "LIMITATIONS AND FUTURE WORK", "text": "Our work with this research is meant to be a useful frst step in understanding and supporting qualitative researchers perform audiovisual analysis. We recommend the reader to take the intended benefts of automation with reservations, and consider the following challenges.\nFor one, we do not yet know how automation will be used in practice. Will it help researchers develop insights more efectively? Or will it lead to less involvement from researchers, and thus lower the quality of research fndings? It is also conceivable that, in certain analyses, there are no repetitive cues, i.e., detectables, that can be detected reliably by software. For example, in HCI studies that aim to understand human behavior, software detection may fail, or lead to too many false positives, requiring much efort from the researcher to manually flter them. From a development perspective, it is also possible that since detectables may be unique to each analysis, the automation techniques need to be comprehensive, covering several data dimensions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CONCLUSION", "text": "In this paper, we studied how qualitative researchers work with audiovisual recordings. We found that 32% of respondents could not work solely with a transcript but relied on audiovisual recordings. An important task is to locate the inspectables to analyze in detail. Finding the abstract inspectables is achieved through time codes, by exploiting study structure, or by looking for concrete detectables that indicate the inspectables. Towards the end of the analysis, researchers look for reportables, which are pieces of information from the recordings that they report in their publication to illustrate their fndings.\nWe also discussed the potential for the search for detectables and presented our design recommendations. We also discussed the potential for the search for detectables and presented our design recommendations. Our vision is that such automation can save qualitative researchers time and efort in locating inspectables, allowing them to focus on analyzing them. We hope that our work sparks further research and development to improve the tool support for qualitative analysis of audiovisual data.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Prof. Dr. Chat Wacharamanothan and Alexander Eiselmayer for their helpful comments on the survey, Dr. Adam Fouse and Sebastian Hueber for their valuable input to our research, Oliver Nowak and Adrian Wagner for their help in preparing this manuscript, all interview and survey participants for their valuable contributions, and all reviewers for their feedback. This work was funded in part by the German B-IT Foundation.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "A Qualitative Approach to HCI Research", "journal": "Cambridge University Press", "year": "2008", "authors": "Anne Adams; Peter Lunt; Paul Cairns"}, {"title": "", "journal": "Contextual Design. Interactions", "year": "1999-01", "authors": "Hugh Beyer; Karen Holtzblatt"}, {"title": "Using Computer-Assisted Qualitative Data Analysis Software to Develop a Grounded Theory Project", "journal": "Field Methods", "year": "2006-08", "authors": "Joy D Bringer; Lynne Halley Johnston; Celia H Brackenridge"}, {"title": "Digital Replay system (DRS): A Tool for Interaction Analysis", "journal": "", "year": "2008", "authors": "Patrick Brundell; Paul Tennent; Chris Greenhalgh; Dawn Knight; Andy Crabtree; Claire O' Malley; Shaaron Ainsworth; David Clarke; Ronald Carter; Svenja Adolphs"}, {"title": "Local Standards for Sample Size at CHI", "journal": "ACM", "year": "2016", "authors": "Kelly Caine"}, {"title": "Simplifying Video Editing CHI '21", "journal": "", "year": "2002-05-08", "authors": "Juan Casares; A Chris Long; Brad A Myers; Rishi Bhatnagar; Scott M Stevens; Laura Dabbish; Dan Yocum; Albert Corbett"}, {"title": "", "journal": "ACM", "year": "", "authors": ""}, {"title": "Using Machine Learning to Support Qualitative Coding in Social Science: Shifting the Focus to Ambiguity", "journal": "", "year": "2018", "authors": "Nan-Chen Chen; Margaret Drouhard; Rafal Kocielnik; Jina Suh; Cecilia R Aragon"}, {"title": "Evolving Video Skims into Useful Multimedia Abstractions", "journal": "ACM Press/Addison-Wesley Publishing Co", "year": "1998", "authors": "Michael G Christel; Michael A Smith; C Roy Taylor; David B Winkler"}, {"title": "Fillables: Everyday Vessels as Tangible Controllers with Adjustable Haptics. In CHI '13", "journal": "", "year": "2013", "authors": "Christian Corsten; Chat Wacharamanotham"}, {"title": "Extended Abstracts on Human Factors in Computing Systems", "journal": "ACM", "year": "", "authors": ""}, {"title": "Annotations as Multiple Perspectives of Video Content", "journal": "ACM", "year": "2002", "authors": "Miguel Costa; Nuno Correia; Nuno Guimar\u00e3es"}, {"title": "Video Browsing by Direct Manipulation", "journal": "ACM", "year": "2008", "authors": "Pierre Dragicevic; Gonzalo Ramos; Jacobo Bibliowitcz; Derek Nowrouzezahrai; Ravin Balakrishnan; Karan Singh"}, {"title": "Introduction to the KWALON Experiment: Discussions on Qualitative Data Analysis Software by Developers and Users", "journal": "", "year": "2011", "authors": "Jeanine Evers; Christina Silver; Katja Mruck; Bart Peeters"}, {"title": "New Patterns in the Adoption and Use of Qualitative Software", "journal": "Field Methods", "year": "2002-05", "authors": "Nigel G Fielding; Raymond M Lee"}, {"title": "ChronoViz: A System for Supporting Navigation of Time-Coded Data", "journal": "ACM", "year": "2011", "authors": "Adam Fouse; Nadir Weibel; Edwin Hutchins; James D Hollan"}, {"title": "Spontaneous speech recognition and summarization", "journal": "", "year": "2005", "authors": "Sadaoki Furui"}, {"title": "Keyframe-Based User Interfaces for Digital Video", "journal": "Computer", "year": "2001-09", "authors": "Andreas Girgensohn; John Boreczky; Lynn Wilcox"}, {"title": "Video Object Annotation, Navigation, and Composition", "journal": "ACM", "year": "2008", "authors": "Dan B Goldman; Chris Gonterman; Brian Curless; David Salesin; Steven M Seitz"}, {"title": "Doing Qualitative Analysis In Psychology", "journal": "Psychology Press", "year": "2013", "authors": "Nicky Hayes"}, {"title": "Rapid Contextual Design: A How-to Guide to Key for User-Centered Design", "journal": "", "year": "2005", "authors": "Karen Holtzblatt; Jessamyn Burns Wendell; Shelley Wood"}, {"title": "", "journal": "", "year": "", "authors": "San Elsevier/Morgan Kaufmann;  Francisco"}, {"title": "User Interfaces for Browsing and Navigation of Continuous Multimedia Data", "journal": "", "year": "2002", "authors": "Wolfgang H\u00fcrst; Patrick Stiegeler"}, {"title": "", "journal": "ACM", "year": "", "authors": ""}, {"title": "What You See Is What You Get: on Visualizing Music", "journal": "", "year": "2005", "authors": "Eric Isaacson"}, {"title": "Interaction Analysis: Foundations and Practice", "journal": "Journal of the Learning Sciences", "year": "1995", "authors": "Brigitte Jordan; Austin Henderson"}, {"title": "DRAGON: A Direct Manipulation Interface for Frame-Accurate in-Scene Video Navigation", "journal": "", "year": "2008", "authors": "Thorsten Karrer; Malte Weiss; Eric Lee"}, {"title": "CHI '08)", "journal": "ACM", "year": "", "authors": ""}, {"title": "Comprehensible Video Thumbnails", "journal": "Computer Graphics Forum", "year": "2015", "authors": "Jongdae Kim; Charles Gray; Paul Asente; John Collomosse"}, {"title": "Scrolling Through Time: Improving Interfaces for Searching and Navigating Continuous Audio Timelines", "journal": "", "year": "2006", "authors": "Eric Lee; Henning Kiel"}, {"title": "DIVA: Exploratory Data Analysis with Multimedia Streams", "journal": "", "year": "1998", "authors": "Wendy E Mackay; Michel Beaudouin-Lafon"}, {"title": "", "journal": "ACM Press/Addison-Wesley Publishing Co", "year": "", "authors": ""}, {"title": "Semi-Automated Coding for Qualitative Research: A User-Centered Inquiry and Initial Prototypes", "journal": "ACM", "year": "2018", "authors": "Megh Marathe; Kentaro Toyama"}, {"title": "Beyond Transcription: Technology, Change, and Refnement of Method", "journal": "Forum: Qualitative Social Research", "year": "2011", "authors": "D ; Thomas Markle; Richard E West; Peter J Rich"}, {"title": "Tools for Searching, Annotation and Analysis of Speech, Music, Film and Video-A Survey", "journal": "Literary and Linguistic Computing", "year": "2007-07", "authors": "Alan Marsden; Adrian Mackenzie; Adam Lindsay; Harriet Nock; John Coleman; Greg Kochanski"}, {"title": "A Process Model of Scholarly Media Annotation", "journal": "ACM", "year": "2017", "authors": "Liliana Melgar; Marijn Koolen; Hugo Huurdeman; Jaap Blom"}, {"title": "Audiovisual Media Annotation Using Qualitative Data Analysis Software: A Comparative Analysis. The Qualitative", "journal": "Report", "year": "2018-03", "authors": "Liliana Melgar Estrada; Marijn Koolen"}, {"title": "From Social Tagging to Polyrepresentation: A Study of Expert Annotating Behavior of Moving Images", "journal": "", "year": "2015", "authors": "Liliana Mar\u00eda Melgar Estrada"}, {"title": "A Magnifer Tool for Video Data", "journal": "ACM", "year": "1992", "authors": "Michael Mills; Jonathan Cohen; Yin Yin Wong"}, {"title": "2020. Participants, Incentives and User Studies: A Survey of CHI 2019", "journal": "", "year": "", "authors": "Jenny Reinhard"}, {"title": "Qualitative Research Practice: A Guide for Social Science Students and Researchers", "journal": "SAGE Publications", "year": "2013", "authors": "Jane Ritchie; Jane Lewis; Carol Mcnaughton Nicholls; Rachel Ormston"}, {"title": "Grounded Theory Methodology", "journal": "", "year": "1994", "authors": "Anselm Strauss; Juliet Corbin"}, {"title": "Novel Techniques for Time-Compressing Speech: an Exploratory Study", "journal": "IEEE", "year": "2005", "authors": "Simon Tucker; Steve Whittaker"}, {"title": "Using Software for Qualitative Data Analysis: Research Outside Paradigmatic Boundaries", "journal": "Emerald Group Publishing Limited", "year": "2014", "authors": "Jonathan Tummons"}, {"title": "Composition and Search with a Video Algebra", "journal": "IEEE MultiMedia", "year": "1995", "authors": "Ron Weiss; Andrzej Duda; David K Giford"}, {"title": "ELAN: A Professional Framework for Multimodality Research", "journal": "", "year": "2006", "authors": "Peter Wittenburg; Hennie Brugman; Albert Russel; Alex Klassmann; Han Sloetjes"}, {"title": "", "journal": "European Language Resources Association (ELRA)", "year": "", "authors": ""}, {"title": "Advancing Qualitative Research Using Qualitative Data Analysis Software (QDAS)? Reviewing Potential Versus Practice in Published Studies using ATLAS.ti and NVivo", "journal": "", "year": "1994", "authors": "Megan Woods; Trena Paulus; David P Atkins; Rob Macklin"}], "figures": [], "doi": "10.1145/3411764.3445458"}