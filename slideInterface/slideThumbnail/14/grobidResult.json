{"authors": "Laxmi Pandey; Khalad Hasan; Ahmed Sabbir Arif; Ahmed Sabbir", "pub_date": "", "title": "Acceptability of Speech and Silent Speech Input Methods in Private and Public", "abstract": "Silent speech input converts non-acoustic features like tongue and lip movements into text. It has been demonstrated as a promising input method on mobile devices and has been explored for a variety of audiences and contexts where the acoustic signal is unavailable (e.g., people with speech disorders) or unreliable (e.g., noisy environment). Though the method shows promise, very little is known about peoples' perceptions regarding using it. In this work, frst, we conduct two user studies to explore users' attitudes towards the method with a particular focus on social acceptance and error tolerance. Results show that people perceive silent speech as more socially acceptable than speech input and are willing to tolerate more errors with it to uphold privacy and security. We then conduct a third study to identify a suitable method for providing real-time feedback on silent speech input. Results show users fnd an abstract feedback method efective and signifcantly more private and secure than a commonly used video feedback method.\u2022 Human-centered computing \u2192 Natural language interfaces; Text input.", "sections": [{"heading": "INTRODUCTION", "text": "Speech input on mobile devices continues to evolve at a rapid pace as the speech recognition technologies get better at understanding users' voice commands. This method ofers the opportunity for faster and seamless hands-free information access, especially when users' hands are busy performing other tasks or when touching public devices is to be avoided in times like the current COVID-19 situation. Prior research showed that speech input is a viable solution for accessing information on small-screen devices where it allows users to access information faster than traditional on-screen input methods [102]. A major challenge with this method, however, is users' reluctance to use speech in pubic places due to privacy and security concerns [36,37,78,92]. Additionally, voice recognition accuracy is heavily afected by ambient noise [69] and the method is not well supported for people with speech disabilities 1 .\nSilent speech input, which interprets users' lip and tongue motions into text, has been shown as a promising alternative to speech input [32,38,43,54,55,104,110]. Researchers explored diferent video-based [2,13,27,28] and advanced sensor-based [83,84,95,113] recognition methods where they showed high accuracy in speech recognition with silent speech input. A recent work [110] explored silent speech input on mobile devices, where users expressed a higher level of satisfaction with this input method over the tradition speech input. In spite of promising results, very little is known on factors such as social acceptance and error tolerance that could infuence users' willingness to use this input method. Consequently, the extent to which this input method is viable on mobile devices is an open question.\nIn this paper, we explore users' attitudes towards speech and silent speech input methods with a focus on social acceptability, and user tolerance of recognition errors in these methods. We frst conduct a crowdsourced study examining social acceptance of these methods considering diferent factors, including users' and viewers' perspectives towards using these in diferent locations and in front of diferent audiences. Results show that, in general, people prefer using silent speech input over traditional speech input. Since prior research suggests that silent speech input can be error-prone [33,82], we conducted another study to explore users' attitude towards recognition errors associated with the two methods. Results reveal that users are willing to tolerate more errors with silent speech input than speech input as it ofers a higher degree of privacy and security. Inspired by the fndings, we further investigate suitable feedback method for silent speech input. Results show that users fnd both a commonly used video and an abstract (a blinking dot) feedback efective but the latter signifcantly more private, more secure, and less intrusive than the video feedback.\nTo summarize, in this work we: i) explore the social acceptance of speech and silent speech input in diferent social contexts; ii) investigate user tolerance of recognition errors in the two methods; iii) identify suitable feedback mechanism for silent speech input; and iv) propose a set of recommendations for using silent speech input on mobile devices.", "n_publication_ref": 25, "n_figure_ref": 0}, {"heading": "RELATED WORK", "text": "This work intersects with four areas of interest: speech input, silent speech input, social acceptance of technology, and users' and viewers' perspectives.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Speech Input", "text": "Speech input enabled devices, such as personal voice assistants, allow users to communicate with computer systems using speech commands. Personal voice assistants like Siri, Google Assistant, Alexa, and Cortana can interpret human speech and handle a wide variety of tasks [53,72]. Research on speech input mainly focused on the recognition of speech [75,106,119], language models [7,17] and voice controlled systems [121]. Clark et al. [29] provides a comprehensive review of the literature on speech-based input and interaction methods. With the recent advances in speech recognition technology [1,44,87,91,108], today's voice-based commercial products [23,48,62,[116][117][118]120] can perform streaming, highaccuracy, low-latency speech recognition [15,68] to revolutionize human-computer interaction [29]. Recently, He et al. [49] presented an end-to-end speech recognizer for on-device speech recognition using a recurrent neural network, which has been deployed in the default Google keyboard on the fagship Pixel phones. Despite its popularity, studies show privacy and security concerns for the use of personal voice assistants and voice search commands in public places [36,37,78,92]. A survey 2 revealed that 39% smartphone users use the built-in voice assistants at home but only 6-14% use these in public [85]. To uphold the privacy and security of users, researchers explored whisper input, which is a variant of speech input with a signifcantly lower energy than normal speech. These works detected whispered speech using a stethoscopic microphone that contacts the skin behind the ear [79], a throat microphone [59], and a non-contact microphone by placing it very close to the front of the narrowly opened mouth [40]. Recently, Amazon included a whisper mode to their personal voice assistant Alexa 3 . When users whisper to Alexa, it whispers back to them. Some have also incorporated state-of-the-art machine learning techniques to improve the performance of whisper speech recognition [41,42,45]. However, whispers with a much lower acoustic power and relatively fat spectrum than regular speech are inherently noise-like, thus are highly susceptible to acoustic interference [76]. Moreover, long-term use of whisper voice might have negative efects on our vocal cords [103].", "n_publication_ref": 38, "n_figure_ref": 0}, {"heading": "Silent Speech Input", "text": "Silent speech input enables users to communicate with a computer system using speech commands without the need for producing any audible sound. Unlike speech input, this method allows users to communicate efciently with computer systems without hurting privacy and security or disrupting the environment. There have been several previous attempts at achieving silent speech communication. Many have explored silent speech enabled input and interaction methods that use diferent sensors (e.g., electromagnetic articulography (EMA) [38,43,50], electroencephalogram (EEG) [88], electromyography (EMG) [56-58, 74, 105, 115], ultrasound imaging [31,32,39,43,50,54,55,61], vibrational sensors of glottal activity [83,84,95,113], speech motor cortex implants [18], and non-audible murmur (NAM) microphone [51,52,80]) to recover the speech content produced without vibration of the vocal folds, by detecting tongue, facial, and throat movements. Some have developed intracortical microelectrode Brain-Computer Interfaces (BCI) to predict user's intended speech information directly from the brain activities involved in the speech production mechanism [24,30,89,111,112]. Some have also used multimodal imaging systems for speech recognition, focusing mainly on tongue visualization [55]. A recent work developed a wearable interface that places fve EMG sensors above the face to capture the neuromuscular signals for silent speech recognition [60]. Most of these works, however, use invasive, impractical, non-portable setup, impeding their scalability in real-world scenarios.\nMore recently, attempts have been made to enable silent speech communication using video-based recognition, referred to as lip reading [2, 8, 13, 19, 25, 26, 26-28, 86, 109]. For example, a work provided smartphone users access to their phone functionalities through silent speech commands [110]. It used the front camera of a smartphone to capture the motion of the mouth, then recognized the silently spoken commands using deep-learning-based image sequence recognition technology. These works suggest that videobased silent speech input method could be more user friendly and appropriate in private and public settings since it can be used without any wearable devices. It has the potential to facilitate input and interaction on private devices when the hands are not available, as well as on public devices when direct contact is not recommended in times like the current COVID-19 situation. It can also help people with speech disorder, muteness, and blindness to input and interact with computer systems, increasing their access to technologies.", "n_publication_ref": 30, "n_figure_ref": 0}, {"heading": "Social Acceptance of Technology", "text": "Previous research has explored social acceptability for body-based and device-based gestures [97][98][99]101], around device input [3], head-mounted display (HMD) input [4], and companion drones for blind people [14] in lab or public settings. In a recent work, Baier and Burmester [16] explored the social acceptability of speech input, which revealed that location infuences users' willingness to use the method in public spaces. However, no prior study has explored user attitudes and acceptance of using silent speech input. In a diferent research, Alallah et al. [5] investigated whether social acceptability studies can be conducted on crowdsourced platforms. They showed that crowdsourced platforms could be an alternative to conducting laboratory-style studies for examining social acceptability. Inspired by this work, we conducted our social acceptability study (Study 1) via crowdsourcing.\nPrior research also showed that social acceptability has a signifcant implication for technological acceptance as they are directly connected to peoples' preferences on using new technologies [63,114]. To examine the social acceptance of new technologies, researchers conducted studies from users' perspective and/or viewers' perspective [3,4,[97][98][99]. To investigate users' perspective, researchers either provided participants with a frst-hand experience using a new technology or showed them video clips on how the technology could potentially be used [3]. Later, participants were asked to consider themselves as users of the technology and express their opinion on using it in diferent contexts. While there are many social acceptability studies conducted from the users' perspective, less attention has been paid to examine social acceptance from viewers' standpoints. A few studies investigated social acceptance from the viewers' perspective where researchers elicited opinions from people watching others using a new technology in diferent contexts. Montero et al. [77] showed that considering viewers viewpoint is important, especially when using the technology in public places, as users' interactions with the technology might draw bystanders' (or the viewers') unwanted attention. Consequently, viewers' perspective are explored for wearable e-textile interface [93,94], Augmented Reality (AR) in public space [34], and public interfaces (e.g., public performance act) [96]. Additionally, some studies considered both the users' and the viewers' perspectives while evaluating the social acceptance of new technologies, such as gestural interaction on mobile devices [77], head-worn devices [4,5,65,70], data glass [64], and around device input methods [3]. These studies were commonly conducted by examining observers' impression on watching other people interacting with a technology -either in a real-world setting or in a video. In this paper, we examined the social acceptability of speech and silent speech input from both users' and viewers' perspectives.", "n_publication_ref": 29, "n_figure_ref": 0}, {"heading": "STUDY 1: SOCIAL ACCEPTABILITY 3.1 Input Modalities", "text": "Researchers have explored a number of voice and non-voice input modalities to interact with mobile devices. For instance, they investigated using speech and silent speech input methods that range from noticeable to inconspicuous [24,38,56,61,84,110,121]. Speech or voice input, which is commercially available on smartphones, requires users to make voice commands to send instructions to mobile devices. This input modality is explicit and commonly draws co-located observers' attention due to the nature of its input visibility -thus can make users feel awkward or uncomfortable with the presence of nearby users. On the other hand, silent speech input, which recognizes speech without requiring users to make acoustic signals, interprets users' commands on smartphones by tracking tongue and lip movements. This input method is more subtle than the speech input, and used when acoustics is not an option (e.g., speech-impaired people) or it is undesired (e.g., during a confdential conversation or communication in public places). On one hand, using explicit input modalities can convey clear instructions to the devices; however, this form of input might be less socially acceptable due to the visibility to co-located people. On the other hand, subtle inputs are less explicit; however, colocated observers might not readily interpret these commands, making the interaction more acceptable. Therefore, we frst conduct a study to explore the social acceptability of these two input modalities. ", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Crowdsourced Study", "text": "As discussed in the related work, researchers explored social acceptability for a wide range of input modalities, such as smartphone gestures [97,98,101], around-device interaction [3], and hand-toface input methods [67,107]. They used two common approaches: (i) allowing participants to use the technology in a particular context (e.g., public places) and (ii) showing participants videos of how the technique can be used. To collected feedback, participants are commonly asked to imagine using it in other contexts (e.g., workplace) and provide their feedback on a 5-point Likert scale. Due to the spread of COVID-19, we were unable to recruit participants to run a study in a public place. Thus, we used the second approach for our study.\nCrowdsourcing platforms have now become increasingly popular to conduct HCI user studies [4,5]. They provide researchers with an easy access to large and diverse groups of participants. Additionally, these platforms have been considered as cost-efcient solutions to run user studies remotely. Though there has been concern about the data quality from crowdsourced studies, researchers have taken certain measures to remove outliers, which have been almost as efective as laboratory or feld studies [4,5,20,46]. Consequently, we decided to use crowdsourcing platforms to run our frst study.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Online Survey", "text": "We created an online survey with Qualtrics to collect responses from participants. Figure 2 shows a sample of questions from the survey. We divided the survey questions into four sections: (i) Demographics: 14 questions to collect demographic information (e.g., age, gender) and prior experience (e.g., experience with smartphones and voice input) from participants; (ii) Users' perspectives: 6 questions asking users to share their experience of using speech and silent speech input methods by considering themselves as users of the modalities; (iii) Observers' perspectives: 6 questions were used to explore observers' perspective, i.e., seeing other people using the input modalities and (iv) Overall preference: 6 questions asking participants to provide their overall preference of using the input modalities on mobile devices. These questions were designed using both open-ended questions, single/multiple-choice questions, and 5-point Likert scale questions. The open-ended questions were used to collect descriptive responses (e.g., justifying their response to a question), while the other types of question were used to collect their preference/perception of using the input modalities and demographic information. When designing the questionnaire, we used similar questions and location-audience contexts used in previous work on social acceptance [3-5, 97, 99]. We also followed many steps listed by Boateng et al. [21], including item generation, context validity, pre-testing with a pilot study, item reductions and others.\nResearchers explored a number of ways to measure social acceptabilities of the methods under investigation. One of the commonly used methods is to elicit participants' responses to social acceptability questions through the 'audience-and-location' axes [3][4][5]97], where participants are asked to provide their social comfortness of using a method in front of diferent audiences and locations. Participants commonly respond by indicating how comfortable they were using the method on a 5-point Likert scale -Extremely comfortable, Somewhat comfortable, Neither comfortable nor uncomfortable, Somewhat uncomfortable, and Extremely uncomfortable. Therefore, we used six audiences (i.e., alone, partner, family, friends, colleagues, and strangers) and seven locations (i.e., home, shop, bus or train, pavement or sidewalk, pub or restaurant, museum or library, and workplace) to explore participants' impression of using the two input methods (i.e., speech and silent speech). As participants might not be familiar with a input method, we used a set of video clips showing users using the two methods to interact with a mobile device in two diferent contexts -in a busy caf\u00e9 surrounded by strangers and at home when alone.", "n_publication_ref": 6, "n_figure_ref": 1}, {"heading": "Participants and Study Procedure", "text": "To recruit participants, we posted the survey as a task in Amazon Mechanical Turk (AMT), a popular Crowdsourcing platform. All AMT users (i.e., workers) could see the task, however, only the workers who owned a smartphone and had a minimum of 70% approval rate on their previously completed tasks could participate. Workers were compensated with USD $1.50 for their time. We collected data from 109 crowdsourced participants. 62 of them were from the U.S., 6 were from India, 2 were from Brazil, and 1 was from Germany. 8 of them were in the age range of 18-24 years, 28 were in 25-34 years, 18 were in 35-44 years, 10 were in 45-54 years, 5 were in 55-64 years, and 2 were 65 years or older.\nThe survey was self-paced and the workers were asked to frst watch the video clips for an input method, then respond to the questions related to that method. We also clearly instructed them not to relate comfort with physical comfort (e.g., tiredness), rather focus on social and mental aspects of it when providing their responses. Similar strategies were applied in previous studies exploring the social acceptance of new input modalities [3].\nAs mentioned earlier, data collected from crowdsourcing platforms sometimes raises concerns due to the lack of direct supervision of the workers. Thus, we used the following criteria to remove outliers from our data. (i) Duplicate IP address: we removed any data with the same IP address. This outlier removal technique was also used in prior studies [4,5]. (ii) Time threshold: as participants were required to watch a set of videos before responding to the questions, they had to spend a minimum time to watch the videos and read and understand the questions before answering them. Consequently, any responses that were submitted within 3 minutes of start were excluded from our analysis. (iii) Incorrect answers: there were a few open-ended questions asking participants to provide justifcations for their responses. Any data with incorrect, incomplete, or random answers were rejected. This process excluded in total 38 participants. Hence, we analyzed the data from 71 participants.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Results", "text": "We used non-parametric analyses on the data and, thus, median values are reported. We also report the efect size (r ) for the Wilcoxon signed-rank test. Since r for the Friedman test is calculated for pairwise comparison and there is not an agreed method for calculating the confdence interval [100], Kendall's W is most commonly used to assess agreement among the raters. Hence, we report W for the Friedman test. Both r and W use the Cohen's interpretation where 0.1 constitutes a small, 0.3 constitutes a medium, and > 0.5 constitutes a large efect. We aggregated users ratings for each input across all the locations and audiences.\nFigure 3 (a) and (b) show the median of social acceptability for each input across locations and audiences, respectively, from users' perspective. A Wilcoxon signed-rank test revealed signifcant diferences between the speech and silent speech input methods across locations (z = \u22124.59, p < .05, r = 0.54). However, we found no signifcant diference between aggregated values for two input methods across audiences (z = \u22121.36, p = .17, r = 0.16). Figure 3 (c) and (d) show the median of social acceptability ratings for each input across locations and audiences, respectively, from viewers' perspective. A Wilcoxon signed-rank test showed that silent speech input was signifcantly diferent from speech input (z = \u22122.5, p < 0.05, r = 0.30) across locations. However, we did not fnd any signifcant diference between two input methods across audiences (z = \u22121.14, p = .26, r = 0.14). We also asked participants to provide their preference for using the two input methods to interact with mobile devices across locations and audiences. Figure 3 (e) shows the results. A Wilcoxon signed-rank test revealed significant diferences between speech and silent speech input methods (z = \u22123.27, p < .05, r = 0.39). We recommend caution in interpreting the \"not signifcant\" results since they yielded a small efect size (r < 0.3).", "n_publication_ref": 1, "n_figure_ref": 3}, {"heading": "Discussion", "text": "The results suggest that social acceptability for the two input modalities from users' and viewers' perspectives were diferent across locations as users considered the less noticeable input method (e.g., silent speech) as their preferred method to interact with mobile devices. Similar fndings were revealed in a prior work [5], where they suggested that less noticeable input methods (e.g., ring and touchpad) are more socially acceptable than noticeable ones (e.g., hand gestures) to interact with an HMD. The results also show that participants preferred to use silent speech input over speech input. In subjective feedback, participants expressed their interest in using silent speech input as it is more subtle and provide a high degree of privacy and security than the other method. One participant (male, 35-44 years) commented, \"I would still feel that I have a high level of privacy when using silent input\". Another participant (female, 35-44 years) wrote, \"I prefer whisper or silent because it doesn't bother others and can be used in quiet places like libraries\".\nThough the results showed users' interest in using silent speech input, there are several key questions remain unknown that could infuence their attitude towards using the method. For instance, researchers showed that silent speech input could be prone to high error rates [33,69,82,90]. Consequently, silent speech recognition accuracy could be a key factor in adopting the method. However, little is know of users' error tolerance level for silent speech input. Additionally, silent speech input recognition on mobile devices depends primarily on capturing users tongue and lip movements via the front camera. Thus, providing appropriate real-time feedback on input recognition is critical for the acceptance of the method. Therefore, in the next two studies, we explore error tolerance and suitable feedback mechanism for silent speech input.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "STUDY 2: ERROR TOLERANCE", "text": "Since the survey results revealed that users put much emphasis on privacy and security, we conducted a Wizard-of-Oz study to investigate whether they are willing to compromise the accuracy of an input method for increased privacy and security.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Apparatus", "text": "We developed a custom client/server web application with HTML5 and JavaScript for the Wizard-of-Oz study. The client and server communicated with each other using WebRTC 4 . The client interface looked and felt like the interface depicted in Fig. 1. It was launched on a Google Chrome mobile web browser (v71.0.3578.98) on a Motorola Moto G 5 Plus smartphone (150.2x74x7.7 mm, 155 g) at 1080x1920 pixels. The server was hosted on a HP Pavilion 15 laptop computer running on Linux 16.04 at 1920\u00d71080 pixels. The server interface was launched on a Google Chrome web browser (v74.0.3729.157), which included dedicated buttons for each condition for the researcher (wizard) to display the spoken and silently spoken phrases on the client side. Both devices were connected to a fast and reliable Wi-Fi network. There were no network dropouts during the study.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Participants", "text": "Twelve volunteers from the local university community participated in the user study. Their age ranged from 22 to 25 years (M = 24.25, SD = 1.48). Four of them identifed as women and eight as men. They were all experienced smartphone (at least 5 years of experience, M = 7.25, SD = 1.48) and voice assistant (at least one year of experience, M = 2.5 years, SD = 0.65) users. Most of them used multiple voice assistants, including Alexa, Cortana, Google Assistant, and Siri. Two participants used these voice assistants almost every day, eight of them used these occasionally, and the remaining two rarely used these.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Design", "text": "The study used a within-subjects design. The independent variables were method and injected error rate and the dependent variables were the qualitative metrics. In summary, the design was: 12 participants \u00d7 2 methods (speech and silent speech, counterbalanced) \u00d7 5 injected error rates (0%, 5%, 10%, 15%, and 20%, randomized) \u00d7 12 phrases from the MacKenzie and Soukoref [73] set = 1,440 phrases, in total.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Error Injection", "text": "Injected errors are commonly used in text entry research to study the efect of errors on performance and preference [6,9,11,66]. In the study, we injected 0%, 5%, 10%, 15%, and 20% misrecognition errors. A misrecognition error occurs when the recognizer incorrectly recognizes a word [12], for example, \"take a cofee break\" (\"cofee\" was replaced with \"tofee\"). The total number of misrecognition errors in a condition was calculated using the following equation: (w \u00d7 e)/100, where w is the total number of words in all presented phrases in the condition and e is the target error rate. We injected errors at word level since both speech and silent speech methods work at either word or phrase level. To inject errors, we randomly replaced a word consisting more than three letters with a similar sounding word, excluding the frst word. To assure that all participants encountered the same errors, we randomly pre-selected a subset of phrases from the MacKenzie and Soukoref [73] set, then used those with the methods in a counterbalanced order. The error injection rates were selected based on the fndings of a prior investigation the reported that user performance tend to drop signifcantly when error rate of an input method reaches 20% [12].", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Procedure", "text": "We conducted a Wizard-of-Oz study to control the error rate in each condition. Before the study, participants were told that the purpose of the study was to compare the performance of multiple speech and silent speech recognition methods that may vary in accuracy rate. The study took place at a campus cafeteria. We picked a public place for the study since its purpose was to investigate whether users were willing to tolerate more errors for the sake of increased privacy and security. Note that the survey results suggested that users are likely to be more conscious about their privacy and security when in public. Upon arrival, we demonstrated the speech and silent speech methods on the smartphone and explained the study procedure to each participant. We then collected their consents. The study started after that, where participants were instructed to enter short English phrases from the MacKenzie and Soukoref [73] set using either speech or silent speech at varying injected error rates. The methods were counterbalanced and the error rates were randomly injected to mitigate any potential learning efects. The interface displayed one phrase at a time. Participants were instructed to tap on the screen when they were done speaking or silently speaking the phrase. They all sat at a table in the cafeteria (Fig. 4). A researcher (the wizard) sat at a nearby table with the server interface launched on a laptop computer. Upon completion of each phrase, she pressed a key to display the recognized phrase and the next phrase on the smartphone. Participants were asked to speak or silently speak a phrase again when the phrase contained a misrecognized word. Upon completion of each condition (method \u00d7 injected error rate), participants completed a short questionnaire that asked them to rate their willingness to use the examined methods on a 5-point Likert scale. Upon completion of the complete study, they completed the NASA-TLX questionnaire [81] to rate the methods' perceived workload. We then held a debrief session to explain the study's actual purpose. A complete study session took about 60 minutes. ", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Results", "text": "We used non-parametric analyses on the data, thus report median values. We also report the efect size r and Kendall's W for the Wilcoxon signed-rank and Friedman tests, respectively (see Section 3.5).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Willingness to Use.", "text": "A Friedman test identifed a signifcant effect of condition on willingness to use (\u03c7 2 (9) = 94.04, p < .0001, r = 0.87). There was a signifcant efect of injected error rate on willingness to use for both the speech (\u03c7 2 (4) = 38.06, p < .0001) and silent speech (\u03c7 2 (4) = 48.00, p < .0001) methods. A Dunn's multiple comparisons test identifed a signifcant diference in willingness to use between the methods with both 10% (z = 2.75, p < .05) and 15% (z = 2.83, p < .05) error rates. Fig. 5 illustrates median willingness to use for both methods with the fve injected error rates.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Perceived Workload.", "text": "A Wilcoxon Signed-Rank test identifed a signifcant efect of method on temporal demand (z = \u22121.1, p < .05, r = 0.61) and overall performance (z = \u22122.24, p < .05, r = 0.65). However, no signifcant efect was identifed on mental demand (z = \u22121.93, p = .05, r = 0.55), physical demand (z = \u22120.93, p = .35, r = 0.27), efort (z = \u22121.45, p = .15, r = 0.42), or the level of frustration (z = \u22120.99, p = .32). Fig. 6 illustrates median Raw TLX (RTLX) scores for both methods. We analyzed the subscales individually, which is a common modifcation made to NASA-TLX [47]. Note that the evidence is inconclusive about whether RTLX is more sensitive, less sensitive, or equally sensitive compared to the original version, thus Hart [47] left it to the researchers' discretion.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Discussion", "text": "Results revealed that 0% and 5% error rates yielded the highest and 20% error rate yielded the lowest willingness to use ratings for both methods. This is not surprising since prior investigations reported that user performance with an input method is the best between 0% and 5% error rates, slightly drops between 5% and 10% error rates, and the worst at 20% error rate [10,12]. Interestingly, for 10% and 15% error rates, the willingness to use ratings for speech dropped at a higher rate that silent speech (Fig. 5). A post hoc analysis failed to identify a signifcant diference between 0-5% and 10-15% error rates for silent speech, while these two groups were signifcantly diferent for speech. This suggests that users were willing to tolerate more errors in silent speech. When asked about this during the debrief session, all participants (100%) responded that it was mostly due to concerns about their privacy and security. They feared that speech will violate their privacy and security in public places, especially when they are surrounded by unknown people. One participant (female, 22 years) commented, \"Sometimes, I feel very hesitant to type with my voice publicly because I always feel that someone else is listening to me\". In contrast, participants felt that silent speech is more private and more secure, thus were willing to compromise accuracy to some extent. One participant (male, 23 years) commented, \"[Silent speech] is very useful for sharing important information in public\".\nThere was a signifcant diference in temporal demand and overall performance for the two methods. Most participants felt that silent speech required more time to use than speech (Fig. 6). The debrief session revealed that it was because participants silently spoke the phrases at a much slower rate than speech assuming that it will increase the method's accuracy (although in reality it had no efect since we used a Wizard-of-Oz setup). This also signifcantly afected their overall rating of the method. There was no signifcant diference in mental demand, physical demand, efort, and frustration. However, we recommend caution in interpreting these results since in the study participants used the methods while seated at a table. Although we did not instruct them on how to hold the device, they all held the device with both hands for clear view of the interface (Fig. 1) and rested their elbow on the table for comfort (Fig. 4). Hence, the results may difer when the methods are evaluated in a standing position or while walking.", "n_publication_ref": 2, "n_figure_ref": 4}, {"heading": "STUDY 3: VISUAL FEEDBACK", "text": "Providing appropriate feedback on the system status is the key usability principle while designing any system. Efcient visual feedback helps users to interpret the system status correctly, enabling them to access information rapidly and accurately [71]. However, designing efective visual feedback for mobile devices is challenging due to their limited display space. Besides, some participants of Study 2 commented that the video feedback method occupies much of the smartphone real estate, leaving a little or no space for additional input and interaction tasks (Fig. 1). We, therefore, conducted a user study to fnd out whether it is feasible to replace the commonly used video feedback with a more compact, abstract feedback method.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Apparatus", "text": "We used the same client/server architecture as the last study, but with an updated user interface (Fig. 7). Further, we hosted the app on GitHub 5 to enable people outside the campus network access the client. Six participants used Apple iOS-based smartphones, while the remaining six used Android-based smartphones. Ten of them used a Google Chrome mobile web browser (> v84), while the remaining two used a Safari browser (> v85) to access the client app. The wizard used a Microsoft Surface Book 3 (34.3 cm display, i7 CPU at 1.90GHz, 16GB RAM) to launch the server interface on a Google Chrome web browser (v85.0.4183.102). We did not record any network dropouts during the study. (1) video feedback that always displays the video captured by the device's front-facing camera on the screen (left) and (2) abstract feedback that displays a grey or a blinking red dot at the top right corner of the device based on whether the camera can see the lips or not, respectively (right).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Feedback Methods", "text": "We implement the following two types of visual feedback:\n\u2022 Abstract feedback. The abstract feedback method is designed to provide minimal feedback on silent speech input.\nFor this, we used a grey dot at the top right corner of the device that turns red and starts blinking when the system tracks the lips (similar to the video recording button on most mobile device). The dot turns grey and stops blinking when the device is unable to see the lips. We use this feedback as it ofers a higher of privacy (does not show users' face or lips) and use minimum screen space on the device. \u2022 Video feedback. The video feedback method provides detailed information about users' lip by showing the video captured by the device's front-facing camera. We place the video on the screen as constant feedback to users about the systems status. Though this form of feedback provides precise information on whether the camera can see users' lips, it consumes a considerable portion of the screen real-estate.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Participants", "text": "Twelve participants (6 female, 6 male) aged 23 to 34 years (M = 28.75, SD = 2.89) participated in this study. All the participants reported being right-handed, using smartphones for the last 8.58 years (SD = 2.29), and using at least one voice assistant system for 2.26 years (SD = 2.24). None of the participants had prior experience using silent speech input. Note that none of the participants participated in the previous studies.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Error Injection", "text": "We injected errors in this study for two reasons. First, to increase the validity of the study since none of the current recognition tems are 100% accurate. Besides, a fully accurate system would have altered some participants about the Wizard-of-Oz setup. Second, to investigate whether users perceive the frequency in which errors occur diferently with diferent feedback methods. For error injection, we used the same approach as the previous study. However, here we maintained a constant 5% error rate over all sessions and injected tracking error rather than misrecognition error. The 5% error rate was chosen as it was found to be an acceptable error rate in various text entry system [6,9,11]. A tracking error occurs when the system fails to track the lips because they are out of sight or range, or due to technical issues, resulting in missing words in the fnal text, for example, \"take it to the recycling depot\" (\"recycling\" is removed). We injected tracking error since the purpose of visual feedback on a recognition system is usually to inform users that it is receiving the tracking signals. Hence, tracking error is more appropriate to evaluate the efciency of visual feedback than misrecognition error.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Design", "text": "The study used a within-subjects design. The independent variables was feedback and the dependent variables the qualitative metrics. In summary, the design was: 12 participants \u00d7 2 feedback methods (video and abstract, counterbalanced) \u00d7 30 phrases from MacKenzie & Soukoref set [73] with 5% injected error = 720 phrases, in total.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Procedure", "text": "The study was conducted remotely due to the spread of COVID-19. We scheduled a video call with each participant ahead of time. They were told that the purpose of the study was to evaluate two diferent types of visual feedback on a working silent speech recognizer. They were instructed to join the call from a quiet room to avoid any interference during the study. A researcher (the wizard) demonstrated the system and the feedback methods, explained tracking error (that the inability to track the lips results in missing words in the recognized phrase), collected their consents and demographics, and provided all instructions via the video call. The researcher provided the participants with a link to the client app, which they accessed on their smartphone using their preferred web browser. They were instructed to activate the airplane mode but keep the Wi-Fi enabled to avoid any interruptions due to incoming calls. The system displayed one phrase at a time. Participants were asked to silently speak the phrase then tap on the screen to see the recognition and the next phrase. The researcher displayed the recognized phrase and updated the presented phrase using the server interface. We did not instruct the participants on how to hold the device but informed them that the blinking red dot will turn grey when the system cannot track the lips during the graphical feedback condition. The researcher observed all interactions with the smartphone to manually turn the blinking red dot to grey when the front-facing camera is unlikely to capture the lips due to the holding posture or angle. Error correction was not required in this study. Upon completion of the study, participants completed a short questionnaire that asked them to rate various aspect of the two feedback methods on a 5-point Likert scale. We then held a debrief session to inform the participants about the actual nature of the study. The complete study session was recorded using a screen recorder.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "We used non-parametric analyses on the data, thus report median values. We also report the efect size r for the Wilcoxon signed-rank test.\nA Wilcoxon signed-rank test identifed a signifcant efect of feedback on whether the method provides enough details about lip detection (z = \u22122.06, p < .05, r = 0.6), occludes, interrupts, and interferes with the task at hand (z = \u22122.84, p < .01, , r = 0.82), and compromise privacy and security (z = \u22122.41, p < .05, r = 0.7). However, there was no signifcant efect on efectiveness (z = \u22120.30, p = .76, r = 0.09), perceived speed (z = \u22121.34, p = .18, r = 0.39), perceived accuracy (z = \u22120.71, p = .48, r = 0.2), or the overall preference (z = \u22121.56, p = .12, r = 0.45). Fig. 8 illustrates median ratings of all aspects of the two feedback methods. ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Discussion", "text": "Participants found both feedback methods equally efective. They found the video feedback signifcantly more informative than abstract feedback. This is not surprising since video feedback displayed a real-time video captured by the device's front-facing camera. Interestingly, participants found the abstract feedback to be the least intrusive (does not occlude, interrupt, or interfere with the task at hand) and most private and secure (does not compromise the user's privacy and security). Once participant (female, 31 years) commented, \"I have privacy concerns with video feedback, I don't want to see my phone camera on when using apps all the time\". Another participant (male, 27 years) wrote, \"In my opinion, the video feedback mode will always gonna be a concern for my privacy and security\". In terms of willingness to use, participants were slightly leaning towards the abstract feedback, but this diference was not statistically signifcant (medium efect size). This is not necessarily a bad thing since it can be interpreted as, users are impartial about the methods, thus using an abstract feedback method is an acceptable design choice. Participants found both methods to be equally reliable (did not compromise accuracy), but interestingly they felt the system with video feedback was slower (statistically not signifcant) although both used the same Wizard-of-Oz setup. We speculate this is because participants were looking at the video while speaking, which increased the mental demand due to information processing, giving them the impression that it was slower. One limitation of these fndings is the lack of generalizability in terms of personality, culture, and ethnic background. Although, the study questionnaire used questions from the SUS questionnaire [22] and custom questions prepared following the Dix et al. [35] guideline, they were not formally validated for the efects of personality, culture, and ethnic background.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "FINAL REFLECTION", "text": "Our general intuition may provide initial guidance regarding speech and silent speech input that the latter is likely to be more acceptable than the former due to the nature of the method (it is subtle and less visible). However, without empirical data, it is difcult to come to a conclusion as users' perception towards using the method might be infuenced by various factors, such as where they are using the method, in front of whom they are using it, and their acceptance towards the errors committed by the methods. The study results confrm that silent speech input is more socially acceptable as it is subtle, more secure, and less attention-seeking than speech input. Moreover, our results afrm that users are willing to accept more recognition errors with silent speech input than speech input. This is primarily due to the fact that the method is more private, secure, and does not trigger feelings of discomfort. Consequently, users expressed their intention to use the method even with a higher rate of errors than speech input. However, they also showed their preference in limiting the error rate within a reasonable threshold (e.g., 5-10%) for both input methods. We also observed that there is a possible linkage between perceived privacy and security and feedback design for silent speech input. Though video feedback provides users with detailed information (e.g., whether lip movements are captured by the camera), participants expressed their concerns about using this feedback method as it may operate in an always-on manner, continually tracking and analyzing lip movements from the camera. These results further confrm users' strong intention to ensure a high level of privacy and security while inputting on mobile devices.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "LIMITATIONS AND FUTURE WORK", "text": "In this paper, we took a step toward understanding users' perception about using silent speech input method from social acceptance, error tolerance, and feedback design perspectives. While an in-thewild study would have provided further insights into these issues in more realistic usage contexts, due to the COVID-19 pandemic, it was not an option available to us. Our results encourage a further exploring on these issues in an in-the-wild study. For Study 2 and 3, we recruited participants from a western country which limits the generalizability of the data across diferent culture and ethnic background. We acknowledge that a larger and more diverse sample would have further afrmed the fndings. Additionally, in Study 3, we investigated only one type of abstract feedback (e.g., blinking dot) for silent speech input, leaving out other possible abstract feedback (e.g., sinusoid icons) that could also infuence users' impression towards silent speech input. Further investigation is needed to identify any diferences or similarities between a wider range of feedback methods. Last but not least, our studies were conducted with Wizard-of-Oz mimicking a mobile silent speech input method. Hence, we were unable to study other technical factors (e.g., silent speech processing delay) that could have afected users' willingness to use the method. It would be interesting to develop an app to enable silent speech input on mobile devices to perform a longitudinal study examining users' perception towards the input modality.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CONCLUSION", "text": "In this paper, we investigated users' impression towards using silent speech input method on mobile devices from social acceptance, error tolerance, and feedback design perspectives. In a crowdsourced survey, we found out that in general people preferred using silent speech input over the traditional speech input. We observed that users were more comfortable using silent speech input in different public and private locations but expressed their concerns about input recognition, privacy, and security issues. Consequently, we conducted a study examining users' error tolerance with both input methods, where results revealed their willingness to tolerate more errors for the sake of privacy and security. In the fnal study exploring suitable feedback for silent speech input, we observed that users found both a video and an abstract feedback methods efective. Yet, they found the latter to be signifcantly more private and secure than the commonly used video feedback. We learned that designing solutions for silent speech input requires careful consideration of various factors and privacy concerns as well as people's tolerance towards using it on mobile devices.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Convolutional Neural Networks for Speech Recognition", "journal": "", "year": "2014-10", "authors": "Ossama Abdel-Hamid; Abdel-Rahman Mohamed; Hui Jiang; Li Deng; Gerald Penn; Dong Yu"}, {"title": "Deep Lip Reading: A Comparison of Models and an Online Application", "journal": "", "year": "2018-06", "authors": "Triantafyllos Afouras; Joon Son Chung; Andrew Zisserman"}, {"title": "Are You Comfortable Doing That? Acceptance Studies of around-Device Gestures in and for Public Settings", "journal": "Association for Computing Machinery", "year": "2014", "authors": "David Ahlstr\u00f6m; Khalad Hasan; Pourang Irani"}, {"title": "Observer: Whose Comfort Level Should We Consider When Examining the Social Acceptability of Input Modalities for Head-Worn Display", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Fouad Alallah; Ali Neshati; Yumiko Sakamoto; Khalad Hasan; Edward Lank; Pourang Bunt;  Irani"}, {"title": "Crowdsourcing vs Laboratory-Style Social Acceptability Studies? Examining the Social Acceptability of Spatial User Interactions for Head-Worn Displays", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Fouad Alallah; Ali Neshati; Nima Sheibani; Yumiko Sakamoto; Andrea Bunt; Pourang Irani; Khalad Hasan"}, {"title": "WiseType: A Tablet Keyboard with Color-Coded Visualization and Various Editing Options for Error Correction", "journal": "Canadian Human-Computer Communications Society", "year": "2019", "authors": "Ohoud Alharbi; Ahmed Sabbir Arif; Wolfgang Stuerzlinger; Mark D Dunlop; Andreas Komninos"}, {"title": "Bayesian Language Model Interpolation for Mobile Speech Input", "journal": "", "year": "2011", "authors": "Cyril Allauzen; Michael Riley"}, {"title": "Improved Speaker Independent Lip Reading Using Speaker Adaptive Training and Deep Neural Networks", "journal": "", "year": "2016", "authors": "Ibrahim Almajai; Stephen Cox; Richard Harvey; Yuxuan Lan"}, {"title": "Predicting the Cost of Error Correction in Character-Based Text Entry Technologies", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Ahmed Sabbir; Arif ; Wolfgang Stuerzlinger"}, {"title": "Predicting the Cost of Error Correction in Character-Based Text Entry Technologies", "journal": "ACM", "year": "2010", "authors": "Ahmed Sabbir; Arif ; Wolfgang Stuerzlinger"}, {"title": "User Adaptation to a Faulty Unistroke-Based Text Entry Technique by Switching to an Alternative Gesture Set", "journal": "Canadian Information Processing Society", "year": "2014", "authors": "Ahmed Sabbir; Arif ; Wolfgang Stuerzlinger"}, {"title": "User Adaptation to a Faulty Unistroke-Based Text Entry Technique by Switching to an Alternative Gesture Set", "journal": "Canadian Information Processing Society", "year": "2014", "authors": "Ahmed Sabbir; Arif ; Wolfgang Stuerzlinger"}, {"title": "LipNet: End-to-End Sentence-level Lipreading", "journal": "", "year": "2016-12", "authors": "M Yannis; Brendan Assael; Shimon Shillingford; Nando Whiteson;  De Freitas"}, {"title": "", "journal": "", "year": "", "authors": "Avila Mauro; Markus Soto;  Funk"}, {"title": "Assessing the Social Acceptability of Companion Drones for Blind Travelers in Public Spaces", "journal": "Association for Computing Machinery", "year": "2018-10-08", "authors": ""}, {"title": "End-to-End Attention-Based Large Vocabulary Speech Recognition", "journal": "", "year": "2016-03", "authors": "Dzmitry Bahdanau; Jan Chorowski; Dmitriy Serdyuk"}, {"title": "Not Just About the User: Acceptance of Speech Interaction in Public Spaces", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Monique Faye Baier; Michael Burmester"}, {"title": "On-Demand Language Model Interpolation for Mobile Speech Input", "journal": "", "year": "2010", "authors": "Brandon Ballinger; Cyril Allauzen; Alexander Gruenstein; Johan Schalkwyk"}, {"title": "Neurotrophic Electrode: Method of Assembly and Implantation into Human Motor Speech Cortex", "journal": "Journal of Neuroscience Methods", "year": "2008", "authors": "Jess Bartels; D Andreasen; P Ehirim; Hui Mao; P Kennedy"}, {"title": "Alternative Visual Units for an Optimized Phoneme-Based Lipreading System", "journal": "", "year": "2019", "authors": "Helen L Bear; Richard Harvey"}, {"title": "The Viability of Crowdsourcing for Survey Research", "journal": "Behavior Research Methods", "year": "2011-03", "authors": "Tara S Behrend; David J Sharek; Adam W Meade; Eric N Wiebe"}, {"title": "Best Practices for Developing and Validating Scales for Health, Social, and Behavioral Research: A Primer", "journal": "Frontiers in public health", "year": "2018", "authors": "O Godfred;  Boateng; B Torsten;  Neilands; A Edward;  Frongillo; Sera L Hugo R Melgar-Qui\u00f1onez;  Young"}, {"title": "SUS: A Quick and Dirty Usability Scale. Usability evaluation in industry", "journal": "", "year": "1996", "authors": "John Brooke"}, {"title": "Automatic Pruning of Grammars in a Multi-Application Speech Recognition Interface", "journal": "", "year": "2008", "authors": " Christopher Ralph Brown"}, {"title": "", "journal": "Brain-Computer Interfaces for Speech Communication. Speech Communication", "year": "2010-04", "authors": "Jonathan S Brumberg; Alfonso Nieto-Castanon; Philip R Kennedy; Frank H Guenther"}, {"title": "Out of Time: Automated Lip Sync in the Wild", "journal": "", "year": "2016", "authors": "Son Joon; Andrew Chung;  Zisserman"}, {"title": "Lip Reading in Profle", "journal": "", "year": "2017", "authors": "Son Joon; Andrew Chung;  Zisserman"}, {"title": "Lip Reading in the Wild", "journal": "Springer International Publishing", "year": "2017", "authors": "Son Joon; Andrew Chung;  Zisserman"}, {"title": "Learning to Lip Read Words by Watching Videos", "journal": "Computer Vision and Image Understanding", "year": "2018-08", "authors": "Son Joon; Andrew Chung;  Zisserman"}, {"title": "The State of Speech in HCI: Trends, Themes and Challenges", "journal": "Interacting with Computers", "year": "2019-06", "authors": "Leigh Clark; Philip Doyle; Diego Garaialde; Emer Gilmartin; Stephan Schl\u00f6gl; Jens Edlund; Matthew Aylett; Jo\u00e3o Cabral; Cosmin Munteanu; Justin Edwards"}, {"title": "Spatial Filtering and Single-Trial Classifcation of Eeg During Vowel Speech Imagery", "journal": "Association for Computing Machinery", "year": "2009", "authors": "Charles S Dasalla; Hiroyuki Kambara; Yasuharu Koike; Makoto Sato"}, {"title": "Prospects for a Silent Speech Interface Using Ultrasound Imaging", "journal": "", "year": "2006", "authors": "B Denby; Y Oussar; G Dreyfus; M Stone"}, {"title": "Speech Synthesis from Real Time Ultrasound Images of the Tongue", "journal": "", "year": "2004", "authors": "B Denby; M Stone"}, {"title": "Challenges in Adopting Speech Recognition", "journal": "Commun. ACM", "year": "2004-01", "authors": "Li Deng; Xuedong Huang"}, {"title": "situ with bystanders of augmented reality glasses: perspectives on recording and privacymediating technologies", "journal": "Association for Computing Machinery", "year": "", "authors": "Tamara Denning; Zakariya Dehlawi; Tadayoshi Kohno"}, {"title": "", "journal": "Human-Computer Interaction", "year": "2003", "authors": "Alan Dix; Janet E Finlay; Gregory D Abowd; Russell Beale"}, {"title": "Voice Activated Personal Assistant: Acceptability of Use in the Public Space", "journal": "Springer International Publishing", "year": "2014", "authors": "Easwara Aarthi;  Moorthy; Phuong L Kim-;  Vu"}, {"title": "Evaluating the Social Acceptability of Voice Based Smartwatch Search", "journal": "", "year": "2016", "authors": "Christos Efthymiou; M Halvey"}, {"title": "Development of a (silent) Speech Recognition System for Patients Following Laryngectomy", "journal": "Medical Engineering & Physics", "year": "2008-05", "authors": "M J Fagan; S R Ell; J M Gilbert; E Sarrazin; P M Chapman"}, {"title": "Silent Vs Vocalized Articulation for a Portable Ultrasound-Based Silent Speech Interface", "journal": "", "year": "2010", "authors": "Victoria M Florescu; L Crevier-Buchman; B Denby; T Hueber; Antonia Colazo-Simon; Claire Pillot-Loiseau; P Roussel-Ragot; C Gendrot; S Quattrocchi"}, {"title": "Silentvoice: Unnoticeable Voice Input by Ingressive Speech", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Masaaki Fukumoto"}, {"title": "Deep Neural Network Training for Whispered Speech Recognition Using Small Databases and Generative Model Sampling", "journal": "International Journal of Speech Technology", "year": "2017-12", "authors": "Hynek Shabnam Ghafarzadegan; John H Bo\u0159il;  Hansen"}, {"title": "Generative Modeling of Pseudo-Whisper for Robust Whispered Speech Recognition", "journal": "", "year": "2016", "authors": "Hynek Shabnam Ghafarzadegan; John H L Bo\u0159il;  Hansen"}, {"title": "Speech, and Language Processing", "journal": "", "year": "2016-10", "authors": ""}, {"title": "Isolated Word Recognition of Silent Speech Using Magnetic Implants and Sensors", "journal": "Medical Engineering & Physics", "year": "2010-12", "authors": "J M Gilbert; S I Rybchenko; R Hofe; S R Ell; M J Fagan; R K Moore; P Green"}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "journal": "", "year": "2013", "authors": "Alex Graves; Mohamed Abdel-Rahman; Geofrey Hinton"}, {"title": "Whispered Speech Recognition Using Deep Denoising Autoencoder and Inverse Filtering", "journal": "", "year": "2017-12", "authors": "T \u00d0or\u0111e; Slobodan T Grozdi\u0107;  Jovi\u010di\u0107"}, {"title": "Using Attention Testing to Select Crowdsourced Workers and Research Participants", "journal": "Social Science Computer Review", "year": "2019-06", "authors": "Anja S G\u00f6ritz; Kathrin Borchert; Matthias Hirth"}, {"title": "NASA-task Load Index (NASA-TLX); 20 Years Later", "journal": "", "year": "2006", "authors": "G Sandra;  Hart"}, {"title": "Speech Recognition Interface System Suitable for Window Systems and Speech Mail Systems", "journal": "", "year": "1997", "authors": "Hideki Hashimoto; Yoshifumi Nagata; Shigenobu Seto; Yoichi Takebayashi; Hideaki Shinchi; Koji Yamaguchi"}, {"title": "Streaming End-to-End Speech Recognition for Mobile Devices", "journal": "", "year": "2019", "authors": "Yanzhang He; Tara N Sainath; Rohit Prabhavalkar; Ian Mcgraw; Raziel Alvarez; Ding Zhao; David Rybach; Anjuli Kannan; Yonghui Wu; Ruoming Pang; Qiao Liang; Deepti Bhatia; Yuan Shangguan; Bo Li; Golan Pundak; Khe Chai Sim; Tom Bagby; Kanishka Shuo-Yiin Chang; Alexander Rao;  Gruenstein"}, {"title": "Automatic Recognition of Speech Without Any Audio Information", "journal": "", "year": "2011", "authors": "Panikos Heracleous; Norihiro Hagita"}, {"title": "Unvoiced Speech Recognition Using Tissue-Conductive Acoustic Sensor", "journal": "EURASIP J. Adv. Signal Process", "year": "2007", "authors": "Panikos Heracleous; Tomomi Kaino; H Saruwatari; K Shikano"}, {"title": "Silent-Speech Enhancement Using Body-Conducted Vocal-Tract Resonance Signals", "journal": "Speech Communication", "year": "2010-04", "authors": "Tatsuya Hirahara; Makoto Otani; Shota Shimizu; Tomoki Toda; Keigo Nakamura; Yoshitaka Nakajima; Kiyohiro Shikano"}, {"title": "An Introduction to Voice Assistants", "journal": "Medical Reference Services Quarterly", "year": "2018-01", "authors": "B Matthew;  Hoy ; Alexa; Cortana Siri; More "}, {"title": "Eigentongue Feature Extraction for an Ultrasound-Based Silent Speech Interface", "journal": "", "year": "2007", "authors": "T Hueber; G Aversano; G Chollet; B Denby; G Dreyfus; Y Oussar; P Roussel; M Stone"}, {"title": "Development of a Silent Speech Interface Driven by Ultrasound and Optical Images of the Tongue and Lips", "journal": "Speech Communication", "year": "2010-04", "authors": "Thomas Hueber; Elie-Laurent Benaroya; G\u00e9rard Chollet; Bruce Denby; G\u00e9rard Dreyfus; Maureen Stone"}, {"title": "Speech Interfaces Based Upon Surface Electromyography", "journal": "Speech Communication", "year": "2010-04", "authors": "Charles Jorgensen; Sorin Dusan"}, {"title": "Sub Auditory Speech Recognition Based on Emg Signals", "journal": "", "year": "2003", "authors": "C Jorgensen; D D Lee; S Agabont"}, {"title": "Towards Continuous Speech Recognition Using Surface In INTERSPEECH", "journal": "", "year": "2006", "authors": "S Jou; Tanja Schultz; Matthias Walliczek; F Kraft; Alexander H Waibel"}, {"title": "Adaptation for Soft Whisper Recognition Using a Throat Microphone", "journal": "", "year": "2004", "authors": "Chen Szu;  Jou"}, {"title": "Alterego: A Personalized Wearable Silent Speech Interface", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Arnav Kapur; Shreyas Kapur; Pattie Maes"}, {"title": "SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using Deep Neural Networks", "journal": "Association for Computing Machinery", "year": "2019", "authors": "Naoki Kimura; Michinari Kono"}, {"title": "Server Based Speech Recognition User Interface for Wireless Devices", "journal": "", "year": "2003", "authors": "Peter F King"}, {"title": "Social Acceptability in HCI: A Survey of Methods, Measures, and Design Strategies", "journal": "CHI Association for Computing Machinery", "year": "2020", "authors": "Marion Koelle; Swamy Ananthanarayan; Susanne Boll"}, {"title": "All about Acceptability? Identifying Factors for the Adoption of Data Glasses", "journal": "", "year": "2017", "authors": "Marion Koelle; Abdallah El Ali; Vanessa Cobus; Wilko Heuten; Susanne Cj Boll"}, {"title": "Don't Look at Me That Way! Understanding User Attitudes Towards Data Glasses Usage", "journal": "Association for Computing Machinery", "year": "2015", "authors": "Marion Koelle; Matthias Kranz; Andreas M\u00f6ller"}, {"title": "Investigating Error Injection to Enhance the Efectiveness of Mobile Text Entry Studies of Error Behaviour", "journal": "", "year": "2020", "authors": "Andreas Komninos; Emma Nicol; Mark Dunlop"}, {"title": "Designing Socially Acceptable Hand-to-Face Input", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Doyoung Lee; Youryang Lee; Yonghwan Shin; Ian Oakley"}, {"title": "Accurate and Compact Large Vocabulary Speech Recognition on Mobile Devices", "journal": "", "year": "2013", "authors": "A Xin Lei; A Senior; Jefrey Scott Gruenstein;  Sorensen"}, {"title": "Research on Human-Computer Interaction Mode of Speech Recognition Based on Environment Elements of Command and Control System", "journal": "", "year": "2019", "authors": "Ning Li; Tuoyang Zhou; Yingwei Zhou; Chen Guo; Deqiang Fu; Xiaoqing Li; Zijing Guo"}, {"title": "NotifEye: using interactive glasses to deal with notifcations while walking in public", "journal": "Association for Computing Machinery", "year": "2014-11-11", "authors": "Andr\u00e9s Lucero; Akos Vetek"}, {"title": "Visual Design for the User Interface, Part 1: Design Fundamentals", "journal": "Journal of Biocommunication", "year": "1994", "authors": "Patrick J Lynch"}, {"title": "Alexa Vs. Siri Vs. Cortana Vs. Google Assistant: A Comparison of Speech-Based Natural User Interfaces", "journal": "Springer International Publishing", "year": "2018", "authors": "Gustavo L\u00f3pez; Luis Quesada; Luis A Guerrero"}, {"title": "Phrase sets for evaluating text entry techniques", "journal": "Association for Computing Machinery", "year": "2003-04-05", "authors": "I ; Scott Mackenzie; R William Soukoref"}, {"title": "Session Independent Non-Audible Speech Recognition Using Surface Electromyography", "journal": "", "year": "2005", "authors": "L Maier-Hein; F Metze; T Schultz; A Waibel"}, {"title": "Personalized Speech Recognition on Mobile Devices", "journal": "", "year": "2016", "authors": "Ian Mcgraw; Rohit Prabhavalkar; Raziel Alvarez; Montse Gonzalez Arenas; Kanishka Rao; David Rybach; Ouais Alsharif; Ha\u015fim Sak; Alexander Gruenstein; Fran\u00e7oise Beaufays; Carolina Parada"}, {"title": "Reconstruction of Continuous Voiced Speech from Whispers", "journal": "", "year": "2013", "authors": "I Mcloughlin; J Li; Yan Song"}, {"title": "Would you do that? understanding social acceptance of gestural interfaces", "journal": "Association for Computing Machinery", "year": "2010-09-07", "authors": "S Calkin; Jason Montero; Mark T Alexander; Sriram Marshall;  Subramanian"}, {"title": "Privacy Concerns for Use of Voice Activated Personal Assistant in the Public Space", "journal": "International Journal of Human-Computer Interaction", "year": "2015-04", "authors": "Easwara Aarthi;  Moorthy; Phuong L Kim-;  Vu"}, {"title": "Non-Audible Murmur Recognition Input Interface Using Stethoscopic Microphone Attached to the Skin", "journal": "", "year": "2003", "authors": "Y Nakajima; H Kashioka; K Shikano; N Campbell"}, {"title": "Non-Audible Murmur Recognition Input Interface Using Stethoscopic Microphone Attached to the Skin", "journal": "", "year": "2003", "authors": "Y Nakajima; H Kashioka; K Shikano; N Campbell"}, {"title": "NASA Task Load Index (TLX) V 1.0: Paper and Pencil Package", "journal": "", "year": "1986", "authors": " Nasa"}, {"title": "Audio-Visual Speech Recognition", "journal": "", "year": "2000", "authors": "Chalapathy Neti; Gerasimos Potamianos; Juergen Luettin; Iain Matthews; Herve Glotin"}, {"title": "Denoising of Human Speech Using Combined Acoustic and Em Sensor Signal Processing", "journal": "", "year": "2000", "authors": "L C Ng; G C Burnett; J F Holzrichter; T J Gable"}, {"title": "The Physiological Microphone (pmic): A Competitive Alternative for Speaker Assessment in Stress Detection and Speaker Verifcation", "journal": "Speech Communication", "year": "2010-04", "authors": "A Sanjay;  Patil; H L John;  Hansen"}, {"title": "Everybody is Talking About Virtual Assistants, But How are People Really Using Them", "journal": "", "year": "2018", "authors": "Dr Marta Perez Garcia; Sarita Safon Lopez; Hector Donis"}, {"title": "Deep Complementary Bottleneck Features for Visual Speech Recognition", "journal": "IEEE International Conference on Acoustics, Speech and Signal Processing", "year": "2016", "authors": "Stavros Petridis; Maja Pantic"}, {"title": "Signal Modeling Techniques in Speech Recognition", "journal": "", "year": "1993-09", "authors": "J W Picone"}, {"title": "EEGbased Speech Recognition -Impact of Temporal Efects", "journal": "", "year": "2009-01", "authors": "Anne Porbadnigk; Marek Wester; ; ; Tanja Schultz"}, {"title": "EEG-Based Speech Recognition -Impact of Temporal Efects", "journal": "", "year": "2009-01", "authors": "Anne Porbadnigk; Marek Wester; ; ; Tanja Schultz"}, {"title": "Recent Advances in the Automatic Recognition of Audiovisual Speech", "journal": "", "year": "2003-09", "authors": "G Potamianos; C Neti; G Gravier; A Garg; A W Senior"}, {"title": "Conference Name: IEEE 2011 Workshop on Automatic Speech Recognition and Understanding Number", "journal": "IEEE Signal Processing Society", "year": "2011", "authors": "Daniel Povey; Arnab Ghoshal; Gilles Boulianne; Lukas Burget; Ondrej Glembek; Nagendra Goel; Mirko Hannemann; Petr Motlicek; Yanmin Qian; Petr Schwarz; Jan Silovsky; Georg Stemmer; Karel Vesely"}, {"title": "Biometric Recognition: Security and Privacy Concerns", "journal": "IEEE Security Privacy", "year": "2003-03", "authors": "S Prabhakar; S Pankanti; A K Jain"}, {"title": "The AT Efect: How Disability Afects the Perceived Social Acceptability of Head-Mounted Display Use", "journal": "Association for Computing Machinery", "year": "2016", "authors": "Halley Profta; Reem Albaghli; Leah Findlater; Paul Jaeger; Shaun K Kane"}, {"title": "Designing wearable computing technology for acceptability and accessibility", "journal": "", "year": "", "authors": "P Halley;  Profta"}, {"title": "Exploiting Nonacoustic Sensors for Speech Encoding", "journal": "", "year": "2006-03", "authors": "T F Quatieri; K Brady; D Messing; J P Campbell; W M Campbell; M S Brandstein; C J Weinstein; J D Tardelli; P D Gatewood"}, {"title": "Designing the Spectator Experience", "journal": "Association for Computing Machinery", "year": "2005", "authors": "Stuart Reeves; Steve Benford; Claire O' Malley; Mike Fraser"}, {"title": "Gestures all around us: user diferences in social acceptability perceptions of gesture based interfaces", "journal": "Association for Computing Machinery", "year": "2009-09-15", "authors": "Julie Rico; Stephen Brewster"}, {"title": "Usable Gestures for Mobile Interfaces: Evaluating Social Acceptability", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Julie Rico; Stephen Brewster"}, {"title": "Usable Gestures for Mobile Interfaces: Evaluating Social Acceptability", "journal": "Association for Computing Machinery", "year": "2010", "authors": "Julie Rico; Stephen Brewster"}, {"title": "Modern Statistical Methods for HCI", "journal": "Springer", "year": "", "authors": "Judy Robertson; Maurits Kaptein"}, {"title": "Tap Input as an Embedded Interaction Method for Mobile Devices", "journal": "Association for Computing Machinery", "year": "2007", "authors": "Jonna Sami Ronkainen; Saana H\u00e4kkil\u00e4; Ashley Kaleva; Jukka Colley;  Linjama"}, {"title": "Comparing Speech and Keyboard Text Entry for Short Messages in Two Languages on Touchscreen Phones", "journal": "", "year": "2018-01", "authors": "Sherry Ruan; Jacob O Wobbrock; Kenny Liou; Andrew Ng; James A "}, {"title": "Laryngeal Hyperfunction During Whispering: Reality or Myth? Journal of voice : ofcial journal of the Voice Foundation", "journal": "", "year": "2006", "authors": "A Rubin; V Praneetvatakul; C Shirley Gherson; R Moyer;  Satalof"}, {"title": "The Tongue and Ear Interface: A Wearable System for Silent Speech Recognition", "journal": "Association for Computing Machinery", "year": "2014", "authors": "Himanshu Sahni; Abdelkareem Bedri; Gabriel Reyes; Pavleen Thukral; Zehua Guo; Thad Starner; Maysam Ghovanloo"}, {"title": "Modeling Coarticulation in Emg-Based Continuous Speech Recognition", "journal": "Speech Communication", "year": "2010-04", "authors": "Tanja Schultz; Michael Wand"}, {"title": "Speech Recognition for Mobile Devices at Google", "journal": "Springer", "year": "2010", "authors": "Mike Schuster"}, {"title": "Exploring the Use of Hand-to-Face Input for Interacting with Head-Worn Displays", "journal": "Association for Computing Machinery", "year": "2014", "authors": "Marcos Serrano; Barrett M Ens; Pourang P Irani"}, {"title": "Speech Recognition with Primarily Temporal Cues", "journal": "Science", "year": "1995", "authors": "R V Shannon; F Zeng; V Kamath; J Wygonski; M Ekelid"}, {"title": "Combining Residual Networks with Lstms for Lipreading", "journal": "", "year": "2017", "authors": "Themos Stafylakis; Georgios Tzimiropoulos"}, {"title": "Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands", "journal": "Association for Computing Machinery", "year": "2018", "authors": "Ke Sun; Chun Yu; Weinan Shi; Lan Liu; Yuanchun Shi"}, {"title": "Brain-Wave Recognition of Sentences", "journal": "Proceedings of the National Academy of Sciences of the United States of America", "year": "1998", "authors": "P Suppes; B Han; Z Lu"}, {"title": "Brain Wave Recognition of Words", "journal": "Proceedings of the National Academy of Sciences of the United States of America", "year": "1997", "authors": "P Suppes; Z Lu; B Han"}, {"title": "Comparison Between Electroglottography and Electromagnetic Glottography", "journal": "The Journal of the Acoustical Society of America", "year": "1999-12", "authors": "R Ingo; Brad H Titze; Gregory C Story; John F Burnett; Lawrence C Holzrichter; Wayne A Ng;  Lea"}, {"title": "User-Defned Game Input for Smart Glasses in Public Space", "journal": "Association for Computing Machinery", "year": "2015", "authors": "Ying-Chao Tung; Chun-Yen Hsu; Han-Yu Wang; Silvia Chyou; Jhe-Wei Lin; Pei-Jung Wu; Andries Valstar; Mike Y Chen"}, {"title": "Session-Independent Emg-Based Speech Recognition", "journal": "", "year": "2011", "authors": "Michael Wand; Tanja Schultz"}, {"title": "Interactive User Interface Using Speech Recognition and Natural Language Processing", "journal": "", "year": "2001", "authors": "Dean Weber"}, {"title": "Object Interactive User Interface Using Speech Recognition and Natural Language Processing", "journal": "", "year": "2002", "authors": "Dean Weber"}, {"title": "Object Interactive User Interface Using Speech Recognition and Natural Language Processing", "journal": "", "year": "2002-05-08", "authors": "Dean Weber"}, {"title": "Survey of the Speech Recognition Techniques for Mobile Devices", "journal": "", "year": "2006", "authors": "D Zaykovskiy"}, {"title": "Automobile Speech-Recognition Interface", "journal": "", "year": "2010", "authors": "You Zhang; Jefery J Faneuf; William Hidden; James T Hotary; Steven C Lee; Vasu Iyengar"}, {"title": "Justspeak: Enabling Universal Voice Control on Android", "journal": "", "year": "2014", "authors": "Yu Zhong; T V Raman; Casey Burkhardt; Fadi Biadsy; Jefrey P Bigham"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Two example videos used in the survey: (a) a user is interacting with a mobile device with silent speech input in a public place, (b) a video clip showing users lip movements and the recognized text, and (c) another video showing a user using speech input on a mobile device in a private room.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Example of survey questions to collect users feedback on using silent speech input (a) in seven locations; and (b) in front of six audiences.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Medians of social acceptability for two input methods from users' perspective across (a) location, (b) audiences and from viewers' perspective across (c) location and (d) audiences, and (e) users' overall preference for two input methods. The error bars represent \u00b11 standard deviation (SD).", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Two participants taking part in the second study at a cafeteria.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Median willingness to use ratings for speech and silent speech with the fve injected error rates on a 5-point Likert scale, where where 1 to 5 represented Very unlikely to Very likely. The error bars represent \u00b11 standard deviation (SD).", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Median RTLX scores of the workload related to speech and silent speech methods. The error bars represent \u00b11 standard deviation (SD).", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: The two visual feedback methods used in the study:(1) video feedback that always displays the video captured by the device's front-facing camera on the screen (left) and (2) abstract feedback that displays a grey or a blinking red dot at the top right corner of the device based on whether the camera can see the lips or not, respectively (right).", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Median ratings of various aspects of the two feedback methods on a 5-point Likert scale, where where 1 to 5 represented Strongly disagree to Strongly agree. The error bars represent \u00b11 standard deviation (SD).", "figure_data": ""}], "doi": "10.1145/3411764.3445430"}