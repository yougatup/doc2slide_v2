{
  "abstractText": {
    "page": 0,
    "region": {
      "x1": 52.8120002746582,
      "x2": 295.5605773925781,
      "y1": 435.5420837402344,
      "y2": 587.5989990234375
    },
    "text": "ABSTRACT Visual blends are an advanced graphic design technique to seamlessly integrate two objects into one. Existing tools help novices create prototypes of blends, but it is unclear how they would improve them to be higher fdelity. To help novices, we aim to add structure to the iterative improvement process. We introduce a method for improving prototypes that uses secondary design dimensions to explore a structured design space. This method is grounded in the cognitive principles of human visual object recognition. We present VisiFit – a computational design system that uses this method to enable novice graphic designers to improve blends with computationally generated options they can select, adjust, and chain together. Our evaluation shows novices can substantially improve 76% of blends in under 4 minutes. We discuss how the method can be generalized"
  },
  "figures": [{
    "caption": "Figure 6: Examples of initial and improved prototypes from the VisiFit user study. The frst column shows three blends that were deemed “improved and publishable”. The second column shows three blends that were deemed “improved but not publishable”. The third column shows two blends that were deemed “not improved and not publishable”.",
    "captionBoundary": {
      "x1": 53.4483642578125,
      "x2": 559.807861328125,
      "y1": 351.2541198730469,
      "y2": 378.79888916015625
    },
    "figType": "Figure",
    "imageText": [],
    "name": "6",
    "page": 10,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 559.0,
      "y1": 83.0,
      "y2": 338.0
    }
  }, {
    "caption": "Figure 1 shows two examples of the VisiBlends output and the result of novices using VisiFit to improve them in under 4 minutes. Our evaluation shows that novices can quickly and easily iterate on prototypes to create seamless and aesthetic blends.",
    "captionBoundary": {
      "x1": 317.9549865722656,
      "x2": 559.5869140625,
      "y1": 197.0421905517578,
      "y2": 236.114990234375
    },
    "figType": "Figure",
    "imageText": ["•", "Three", "preliminary", "investigations", "into", "the", "process", "of", "improv-", "ing", "prototypes", "of", "visual", "blends:", "a", "demonstration", "of", "how", "fully", "automatic", "systems", "fail,", "an", "analysis", "of", "patterns", "used", "by", "professionals,", "and", "a", "co-design", "process", "with", "graphic", "artists.", "•", "Three", "design", "principles", "for", "a", "computational", "approach", "to", "improving", "visual", "blends.", "•", "A", "method", "of", "using", "secondary", "design", "dimensions", "to", "structure", "the", "improvement", "process.", "This", "method", "is", "grounded", "in", "the", "neuroscience", "of", "human", "visual", "object", "recognition.", "•", "VisiFit,", "a", "system", "that", "applies", "the", "method", "and", "design", "princi-", "ples", "in", "a", "pipeline", "of", "computational", "tools.", "•", "An", "evaluation", "of", "VisiFit", "showing", "that", "in", "under", "4", "minutes,", "novices", "can", "substantially", "improve", "blends", "in", "76%", "of", "cases", "and", "create", "blends", "suitable", "to", "publish", "on", "social", "media", "in", "70%", "of", "cases.", "This", "paper", "makes", "the", "following", "contributions:"],
    "name": "1",
    "page": 1,
    "regionBoundary": {
      "x1": 327.9166564941406,
      "x2": 559.7216796875,
      "y1": 240.8761749267578,
      "y2": 417.11199951171875
    }
  }, {
    "caption": "Figure 4: Three visual blends improved by graphic designers. For each blend, the columns show what two objects were initially blended, what the initial blend from VisiBlends produced, what secondary dimension(s) the artists improved, and the resulting improved blend.",
    "captionBoundary": {
      "x1": 53.79791259765625,
      "x2": 558.453857421875,
      "y1": 372.9311218261719,
      "y2": 400.47589111328125
    },
    "figType": "Figure",
    "imageText": [],
    "name": "4",
    "page": 6,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 559.0,
      "y1": 83.0,
      "y2": 359.0
    }
  }, {
    "caption": "Figure 5: The fve steps of the VisiFit pipeline for improving blends. In steps 1 and 2, the user extracts the main shape of both objects and adjusts their overlap. In steps 3-5, the user blends the images. There are two options for selecting a silhouette, fve options for color blending (only four are shown), and a tool to select and re-apply internal details. Each step builds on the selected output from the previous step (indicated by a blue border.) Once the user is happy with the blend, they select it as the fnal output (indicated in a green border.)",
    "captionBoundary": {
      "x1": 53.7979736328125,
      "x2": 295.57696533203125,
      "y1": 521.7271118164062,
      "y2": 625.970458984375
    },
    "figType": "Figure",
    "imageText": [],
    "name": "5",
    "page": 7,
    "regionBoundary": {
      "x1": 60.0,
      "x2": 288.0,
      "y1": 83.0,
      "y2": 508.0
    }
  }, {
    "caption": "Figure 2: An illustration of VisiBlends workfow that helps people create prototypes for a blend representing “Visit New York this autumn.”. The VisiBlends prototypes convey the idea, but are often very rough. The goal of VisiFit is to improve these initial prototypes into seamless and aesthetic blends.",
    "captionBoundary": {
      "x1": 53.7979736328125,
      "x2": 558.202880859375,
      "y1": 492.8301086425781,
      "y2": 520.3748779296875
    },
    "figType": "Figure",
    "imageText": [],
    "name": "2",
    "page": 3,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 559.0,
      "y1": 83.0,
      "y2": 479.0
    }
  }, {
    "caption": "Figure 7: Three examples of visual blends that could be turned into animated blends. Each row shows the original visual blend on the left, the reference video in the middle, and the animated blend on the right. Motion is annotated in red.",
    "captionBoundary": {
      "x1": 53.56489562988281,
      "x2": 295.1376953125,
      "y1": 459.8971252441406,
      "y2": 509.35577392578125
    },
    "figType": "Figure",
    "imageText": [],
    "name": "7",
    "page": 11,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 307.0,
      "y1": 83.0,
      "y2": 446.0
    }
  }, {
    "caption": "Figure 3: Blends created by Fast Style Transfer (top) compared to blends produced by an artist (bottom). The FST blends fail because this problem cannot be solved with an indiscriminate, global application of one object’s style onto another. Experts take apart and blend objects in a more nuanced way, preserving relevant characteristics of each object to keep each one identifable in the fnal blend.",
    "captionBoundary": {
      "x1": 53.797943115234375,
      "x2": 295.6487121582031,
      "y1": 333.1271057128906,
      "y2": 404.4996337890625
    },
    "figType": "Figure",
    "imageText": [],
    "name": "3",
    "page": 4,
    "regionBoundary": {
      "x1": 53.0,
      "x2": 306.0,
      "y1": 166.0,
      "y2": 320.0
    }
  }],
  "sections": [{
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 53.31680679321289,
        "x2": 294.8095703125,
        "y1": 621.2330932617188,
        "y2": 707.3472290039062
      },
      "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. CHI ’21, May 8–13, 2021, Yokohama, Japan © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445089"
    }, {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7171020507812,
        "y1": 436.1972351074219,
        "y2": 464.3110046386719
      },
      "text": "to other blending problems, and how computational tools can support novices by enabling them to explore a structured design space quickly and efciently."
    }, {
      "page": 0,
      "region": {
        "x1": 317.9549560546875,
        "x2": 558.20068359375,
        "y1": 481.2770690917969,
        "y2": 512.782958984375
      },
      "text": "CCS CONCEPTS • Human-centered computing → Interactive systems and tools."
    }, {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 509.54022216796875,
        "y1": 529.7501220703125,
        "y2": 550.2999877929688
      },
      "text": "KEYWORDS Computational design, Design tools, Iterative design"
    }, {
      "page": 0,
      "region": {
        "x1": 317.6600036621094,
        "x2": 559.4303588867188,
        "y1": 564.1857299804688,
        "y2": 619.0050048828125
      },
      "text": "ACM Reference Format: Lydia B. Chilton, Ecenaz Jen Ozmen, Sam Ross, and Vivian Liu. 2021. VisiFit: Structuring Iterative Improvement for Novice Designers. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3411764. 3445089"
    }]
  }, {
    "paragraphs": [{
      "page": 0,
      "region": {
        "x1": 317.9549560546875,
        "x2": 559.5825805664062,
        "y1": 657.316162109375,
        "y2": 707.3469848632812
      },
      "text": "Iterative improvement is the essence of the iterative design process. No design is perfect at inception, thus iteration through prototypes is necessary to improve it. If a prototype passes an evaluation, it should become a new, higher fdelity prototype that can be tested and potentially iterated upon again. In case studies of improved"
    }, {
      "page": 1,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5613098144531,
        "y1": 87.45320129394531,
        "y2": 225.156005859375
      },
      "text": "software usability by the Nielsen Norman Group [40], median improvement per stage of iteration was 38%, leading to overall usability improvements of 165%. Iteration is not just an aspect of usability engineering, it is a fundamental part of the design process that generalizes across many domains. In web design, designers start with a wireframe prototype and move to a minimum viable product. In mechanical design, designers improve upon initial proofs of concept by iterating upon features and prototype reliability. In graphic design, designers sketch prototypes and then move onto higher-fdelity mockups. In each domain, iteration looks diferent, but the objective is the same – to extend the prototype to move closer to the goal. To help novice designers in a meaningful and practical way, we need tools to support iteration."
    }, {
      "page": 1,
      "region": {
        "x1": 53.57720947265625,
        "x2": 295.5623779296875,
        "y1": 229.91920471191406,
        "y2": 367.6210021972656
      },
      "text": "Although there are many existing tools that support other phases of the design process such as brainstorming, prototyping, evaluation, and fnal design execution, there is a lack of tools focusing on iteration [17]. Only 6% of 148 creativity support tools from 1999- 2018 focus on iteration. Iteration tools are similar to brainstorming and prototyping tools in that they help people explore a design space. However, they are more difcult to build because they have more constraints. Unlike general prototyping tools, iterating on prototypes must be constrained further to build on ideas that were validated in the previous prototypes. Iteration still involves searching the design space, but the tools that were previously used to explore an expansive design space are not the right tools to explore a more constrained one."
    }, {
      "page": 1,
      "region": {
        "x1": 53.50199890136719,
        "x2": 295.42742919921875,
        "y1": 372.38421630859375,
        "y2": 586.7999877929688
      },
      "text": "Like all prototyping tools, iteration tools must be domain-specifc so they can efectively operate on the materials of that domain. We focus on the difcult design challenge of making visual blends [3]. Visual blends are an advanced graphic design technique used to convey a message visually in journalism, advertising, and public service announcements. They combine two visual symbols into one object to convey a new meaning. For example, in Figure 1 the Guggenheim Museum is blended with an acorn to convey the message “Visit New York City this autumn”. Visual blends are a canonical example of a creative design challenge [25, 43] because they are open-ended enough to encapsulate all aspects of the design process, but well-defned enough to test in a short time frame. Moreover, cognitive scientists consider blending to be an important aspect of general creativity for its ability to “create new meaning out of old.” [15] Currently, tools already exist to help people brainstorm and create initial prototypes [9] by fnding the right images and arrangements to use for the blend. However, visual blends generally require an expert with Photoshop skills to execute the design and it would be faster, easier, and more empowering for novices to improve blends by themselves, without relying on an expert."
    }, {
      "page": 1,
      "region": {
        "x1": 53.7979736328125,
        "x2": 295.5589599609375,
        "y1": 591.5621948242188,
        "y2": 696.3889770507812
      },
      "text": "We perform several formative studies to learn how experts approach the iterative improvement of visual blends. From an analysis of blends created by experts and a participatory design process with graphic designers, we learned that blends do not simply blend the surface-level style of two objects, they combine the secondary visual dimensions of both objects such as silhouette, color and internal details. Based on this observation, we present a method for structuring the iterative improvement process of blends based on secondary design dimensions. In this method, the improvement process is frst broken into stages that blend each of the dimensions separately."
    }, {
      "page": 1,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.198974609375,
        "y1": 87.45320129394531,
        "y2": 104.60797119140625
      },
      "text": "Then the results of each stage are combined into a single blended output."
    }, {
      "page": 1,
      "region": {
        "x1": 317.65899658203125,
        "x2": 559.5827026367188,
        "y1": 109.37123107910156,
        "y2": 192.27899169921875
      },
      "text": "We present VisiFit – a computational design tool that allows novice graphic designers to improve a prototype of a visual blend. A prior system called VisiBlends [9] helps novices create rough initial prototypes of blends by overlaying two objects with the same shape. VisiFit helps users refne those rough prototypes into seamless and aesthetic blends. VisiFit structures the process of creating second iterations by introducing a pipeline of computational tools that allow users to quickly and easily edit secondary design dimensions."
    }, {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.720458984375,
        "y1": 427.5302429199219,
        "y2": 466.6029968261719
      },
      "text": "We conclude with a discussion of how secondary design dimensions can help structure iteration in other felds and how pipelines of computational design tools can support the iterative design process."
    }],
    "title": {
      "page": 0,
      "region": {
        "x1": 317.9549865722656,
        "x2": 420.68597412109375,
        "y1": 642.9620971679688,
        "y2": 649.81298828125
      },
      "text": "1 INTRODUCTION"
    }
  }, {
    "paragraphs": [],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 421.7441711425781,
        "y1": 484.8050842285156,
        "y2": 491.656005859375
      },
      "text": "2 RELATED WORK"
    }
  }, {
    "paragraphs": [{
      "page": 1,
      "region": {
        "x1": 317.7489929199219,
        "x2": 559.7156372070312,
        "y1": 514.8502197265625,
        "y2": 707.3469848632812
      },
      "text": "Design tools and creativity support tools (CSTs) have a rich tradition of accelerating innovation and discovery [49] by supporting the design process. A survey of 143 papers from 1999-2018 on creativity support tools (CSTs) found that there are papers supporting all phases of the design process: ideation, exploration, prototyping, implementation, evaluation, and process/pipeline, and iteration. [17]. Many of these tools support more than one phase of the design process. However, not all phases of the design process are equally represented in the literature. In fact, a majority of these tools focused on either very early or very late phases of the design process. Of the systems in the survey, 45% support ideation [30, 50, 59], 41% support implementation, including high-fdelity tools [57] or low-fdelity tools for prototyping or sketching [10, 21, 31, 32], and 18% supported evaluation through feedback [36, 63] or expert annotation [51]. However, only 6% of the systems surveyed supported iteration, and only 4% supported the related task of design management or pipelines. More research is needed on how to support iteration more efectively — that is, how to help designers improve"
    }, {
      "page": 2,
      "region": {
        "x1": 53.46699905395508,
        "x2": 294.2134094238281,
        "y1": 87.45320129394531,
        "y2": 104.60797119140625
      },
      "text": "on an initial prototype to get closer to their fnal design goal. Our work in this paper focuses on this problem."
    }],
    "title": {
      "page": 1,
      "region": {
        "x1": 317.9549865722656,
        "x2": 406.1114196777344,
        "y1": 500.49237060546875,
        "y2": 507.3432922363281
      },
      "text": "2.1 Design Tools"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 53.48400115966797,
        "x2": 295.56280517578125,
        "y1": 136.76820373535156,
        "y2": 230.635009765625
      },
      "text": "Existing systems that explicitly aid iteration use a number of approaches. One class of iteration applications uses crowds to iterate towards better solutions [33]. This can be by mixing features of previous designs [66], responding to community feedback [27], hiring experts [45], or identifying weak points and fxing them [28]. All of these use the strength of multiple people’s viewpoints to iterate. However, crowds can introduce errors and may be difcult to steer toward your particular vision. Therefore, it is often useful to provide designers with single user tools for iteration."
    }, {
      "page": 2,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.5555725097656,
        "y1": 235.39820861816406,
        "y2": 351.1830139160156
      },
      "text": "Another class of iteration tools has the user produce a prototype, and then computationally generate the rest of the design. If the user is unhappy with the outcome, they can regenerate, alter their input, or adjust parameters. Several applications apply this method to generate multi-tracked music from a simple input melody. This can be done using rules and constraints [14, 61] or implicit patterns learned by deep learning [35]. Having the computer generate outcomes is especially usable for novices; it allows them to recognize good outcomes, even if they cannot produce them. This seems to work well in music, which has many mathematical rules, but it is unclear if it works as well in other domains."
    }, {
      "page": 2,
      "region": {
        "x1": 53.525604248046875,
        "x2": 295.4256896972656,
        "y1": 355.94622802734375,
        "y2": 482.69000244140625
      },
      "text": "A third way to support iteration is to provide rich undo history to allow users control and freedom while exploring the design space. This is often done in the drawing domain both for single users [39] and for multiple users who want to draw collaboratively [67]. In the creative design process, exploration is clearly important [8], and supporting that is essential. In VisiFit, we use aspects of all three of these approaches. We target key properties of the prototype that need improving and focus iteration on these properties. We provide computational tools to generate outcomes that novices could not produce themselves. We allow users to explore design alternatives and to adjust parameters so they can achieve results they are satisfed with."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 53.79800033569336,
        "x2": 164.25262451171875,
        "y1": 122.41410064697266,
        "y2": 129.2650146484375
      },
      "text": "2.2 Iteration Support"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.0264892578125,
        "y1": 514.8502197265625,
        "y2": 597.7579956054688
      },
      "text": "Computational tools have long been a promising approach to aid design because they can search a design space and help meet a constraint. The power of computational or computer-aided design has been shown in many felds such as: education [34], medicine [22], games [52], urban planning [5], and accessibility [18]. The system designer must defne the space and the search parameters, as well as provide design patterns for solutions that can be adapted to diferent inputs. [2, 64, 65]"
    }, {
      "page": 2,
      "region": {
        "x1": 53.79742431640625,
        "x2": 295.5567321777344,
        "y1": 602.5211791992188,
        "y2": 707.3469848632812
      },
      "text": "Computational design tools have had particularly strong adoption in graphic design problems like optimizing layout [7, 11, 41, 56], making icons [4, 6], and providing inspiration through mood boards [29, 60] and relevant examples [12, 30]. This is also true in the 3D domain, where computational tools can be used to search a design space and create multiple mesh and texture variations of objects (i.e. trees or airplanes) that can make computer generated scenes more diverse [37, 54]. Deep learning has also been applied to generate new designs that ft user specifcations [38, 62]. In this paper, we address a specifc kind of graphic design problem of that"
    }, {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.5326538085938,
        "y1": 87.45320129394531,
        "y2": 115.5670166015625
      },
      "text": "requires blending two objects into one in order to convey a new meaning. To our knowledge, none of the existing computational design tools have addressed this problem."
    }, {
      "page": 2,
      "region": {
        "x1": 317.65899658203125,
        "x2": 559.7164306640625,
        "y1": 120.33021545410156,
        "y2": 290.90899658203125
      },
      "text": "Although these tools can be fully automatic, some of the most useful tools are interactive and allow users to explore and guide the process. We take much inspiration from Side Views [55], an application that allows users to preview the efect of various image editing menu options, like those in Photoshop. By providing previews, users are able to recognize rather than recall the right tool to use. This also helps users adjust parameters of key properties and chain tools together to explore an even wider section of the search space. In VisiFit, we also take the interactive approach to computational design. Like Side Views, VisiFit allows users to preview and adjust tools, as well as chain them together. However, VisiFit is not just a tool for exploration - it is targeted at achieving a specifc goal; multiple tools are chained together in a pipeline that explores each of the three key visual properties needed to complete a blend. This allows the user to explore the design space and iterate in a structured fashion towards their goal."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 53.79800033569336,
        "x2": 293.7218017578125,
        "y1": 500.4960632324219,
        "y2": 507.34698486328125
      },
      "text": "2.3 Computational Approaches to Design Tools"
    }
  }, {
    "paragraphs": [{
      "page": 2,
      "region": {
        "x1": 317.65899658203125,
        "x2": 559.721435546875,
        "y1": 328.5492248535156,
        "y2": 521.0460205078125
      },
      "text": "Visual blends are an advanced graphic design technique where two objects are blended into one to convey a message symbolically. They represent a canonical and very challenging design problem [3] because the two objects must be blended into one object, yet still remain individually identifable so the viewer can tell what objects were blended. When asked to defne design, Charles Eames once said, “Design is a plan for arranging elements to accomplish a particular purpose”. [42] In a visual blend, the objects to blend are the elements, the way they overlap is their arrangement, and the particular purpose is the seamless blend of the objects to convey a message. Because they are a difcult design challenge, visual blends are studied by several diverse felds. In cognitive science, researchers study how creativity emerges from conceptual blending [43]. In visual communication, researcher study how visual blends convey meaning through through context and implicature rather than through explicit language [15, 16, 44, 58]. Creativity and cognition researchers study how computers might achieve creativity by creating visual blends [26]."
    }, {
      "page": 2,
      "region": {
        "x1": 316.9419860839844,
        "x2": 559.7218017578125,
        "y1": 525.8092041015625,
        "y2": 707.3469848632812
      },
      "text": "An existing system called VisiBlends [9] helps novices with the frst step of the design process: creating a prototype. Figure 2 shows an illustration of the VisiBlends workfow to create a visual blend for the message “Visit New York this autumn”. The user must frst identify two abstract concepts to visually blend, for example, New York City and autumn. Next, the users must brainstorm simple, iconic objects associated with the concepts. From their list of associated objects, they must fnd images of those objects that can serve as symbols of the concept. For each image, users annotate the main shape of the object (i.e. whether it is a sphere, cylinder, box, circle, or rectangle) and whether the shape covers “all of the object” or “part of the object”. With the images and annotations, VisiBlends automatically searches over all pairs of objects to fnd two that have the same basic shape, but for one object the shape covers only “part of the object”. It then automatically synthesizes a prototype of the blend by cropping, scaling, positioning and rotating the objects to ft together. For example, in Figure 2 the acorn is a cylinder and"
    }, {
      "page": 3,
      "region": {
        "x1": 53.57400131225586,
        "x2": 294.2114562988281,
        "y1": 550.0482177734375,
        "y2": 621.9979858398438
      },
      "text": "the “part of” the Guggenheim is also a cylinder. Thus, the acorn is positioned onto the cylindrical part of the Guggenheim to produce a blend prototype where both the acorn and the Guggenheim are visible. Lastly, the user selects the best prototype based on the shape ft and the meaning implied by the blend. Once the user selects a prototype, they must complete the fnished design either on their own or by hiring a graphic artist."
    }, {
      "page": 3,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.4259033203125,
        "y1": 626.7601928710938,
        "y2": 698.7100219726562
      },
      "text": "The reason VisiBlends matches objects on shape is based on the neuroscience of human visual object recognition. The human visual object recognition system is hierarchical in what features it uses to recognize an object [53]. 3D shape is the primary feature used by the brain to determine what an object is; after that, it uses secondary features like color, distinct edges and surface information [46]. By combining two objects that have the same shape but diferent"
    }, {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2030639648438,
        "y1": 550.0482177734375,
        "y2": 578.159912109375
      },
      "text": "secondary details, the objects will appear blended into one, yet still individually identifable – which is one of the major challenges of creating a visual blend."
    }],
    "title": {
      "page": 2,
      "region": {
        "x1": 317.9549865722656,
        "x2": 501.347900390625,
        "y1": 314.195068359375,
        "y2": 321.0459899902344
      },
      "text": "3 BACKGROUND: VISUAL BLENDS"
    }
  }, {
    "paragraphs": [{
      "page": 3,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.4349975585938,
        "y1": 621.1171875,
        "y2": 660.18896484375
      },
      "text": "To explore approaches to iteration we conducted three preliminary investigations that informed the three design principles we propose for improving blends. We tie it all together into a general technique for structuring the iterative improvement of blends."
    }],
    "title": {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 522.5006713867188,
        "y1": 593.8120727539062,
        "y2": 613.612060546875
      },
      "text": "4 FORMATIVE STUDIES OF BLENDING ITERATION"
    }
  }, {
    "paragraphs": [{
      "page": 3,
      "region": {
        "x1": 317.6409912109375,
        "x2": 559.716796875,
        "y1": 690.1931762695312,
        "y2": 707.345947265625
      },
      "text": "Advances in deep learning have shown impressive results in manipulating images. An early and prominent result is deep style transfer"
    }, {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.2811584472656,
        "y1": 87.45320129394531,
        "y2": 148.4429931640625
      },
      "text": "[19] which trains a model on a visual style, such as Van Gogh’s Starry Night, and applies that style on any image to make it look like Van Gogh painted it in the Starry Night style. This technique has the potential to automatically improve prototypes of visual blends by training on the style of one object and applying it to another."
    }, {
      "page": 4,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5574951171875,
        "y1": 438.13824462890625,
        "y2": 564.8820190429688
      },
      "text": "To explore the potential of deep style transfer, we took four blend prototypes from the VisiBlends test set, and applied deep style transfer to them. For each pair of images in the blend, we selected which object to learn the style of and which object to apply the style to. We used an implementation of style transfer from the popular Fast Style Transfer (FST) paper [19] which only requires a single image to learn style from and has impressive results on transferring artistic style. We tried multiple combinations of hyperparameters (epochs, batch size, and iterations) until we saw no noticeable improvements in the results. We also tried input images of the same object and diferent ways of cropping it, in case the algorithm was sensitive to any particular image."
    }, {
      "page": 4,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.43280029296875,
        "y1": 569.6452026367188,
        "y2": 707.3469848632812
      },
      "text": "Although the algorithm was able to extract styles and apply them, the results fell far short of the bar for creating convincing blends. Figure 3 shows Deep Style Transfer results (top) and blends made by artists we commissioned to produce high fdelity blends. To blend orange and baseball, FST frst learned the orange style. However, when it applied that learned style to the baseball, while it preserved the baseball’s characteristic red seams, it simply turned its white texture into a blotchy orange color that is not reminiscent of the fruit. In contrast, the artist who blended it used the texture and stem of the orange, in addition to the red seams of the baseball. This made both objects highly identifable. The computer used the overall look of the orange, but didn’t separately consider its elements as it mixed and matched the parts."
    }, {
      "page": 4,
      "region": {
        "x1": 317.62298583984375,
        "x2": 558.4384765625,
        "y1": 87.45320129394531,
        "y2": 192.27899169921875
      },
      "text": "Similarly, for the apple and burger blend, the burger style applied to the apple just turned the apple brown, because the predominant color of a burger is brown. We also explored what would happen if we isolated part of the image by hand and applied the style only within that area. To mimic the artist, we isolated the burger bun and applied the apple style to it. The results are better, but still disappointing. Although the burger has the color and texture of an apple, it does not appear as blended as the artist’s version. The artist chose to mix the apple color and the bun color to give a sense of both objects in that element."
    }, {
      "page": 4,
      "region": {
        "x1": 317.62298583984375,
        "x2": 558.2070922851562,
        "y1": 197.0421905517578,
        "y2": 290.90899658203125
      },
      "text": "We conclude that these existing style transfer results do not easily apply to visual blends. Blends are not just about applying high-level “style”, they require designers to consider the individual elements and how they might be ft together. If we trained a model on thousands of visual blends, we might be able to make progress on this problem, but we would need to create those thousands of visual blends, and even so, results would not be guaranteed. Instead we want to explore semi-automatic approaches that augment people’s ability to create blends."
    }, {
      "page": 4,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.8057250976562,
        "y1": 296.23504638671875,
        "y2": 345.7040100097656
      },
      "text": "Design Principle 1. To help users achieve better results than fully automatic systems, structure the problem into subtasks and provide interactive tools specifc to each subtask. Fully automatic tools do not always achieve desired results and give you little control in how to fx them."
    }],
    "title": {
      "page": 3,
      "region": {
        "x1": 317.9549865722656,
        "x2": 527.115234375,
        "y1": 675.839111328125,
        "y2": 682.6900024414062
      },
      "text": "4.1 Shortcomings of Deep Style Transfer"
    }
  }, {
    "paragraphs": [{
      "page": 4,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.7177734375,
        "y1": 383.55322265625,
        "y2": 455.50201416015625
      },
      "text": "To investigate potential structures for improving blends we analyzed how professional artists improved prototypes. We paid three professional artists to make visual blends based on 13 prototypes made by novices using VisiBlends. Of those 13 images, the artists told us that two did not need editing because the output from VisiBlends was a perfectly acceptable blend. However, the other 11 blends needed signifcant iteration."
    }, {
      "page": 4,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.712646484375,
        "y1": 460.2652282714844,
        "y2": 597.968017578125
      },
      "text": "In our analysis of professionally improved blends, we looked for secondary visual dimensions experts used to improve visual blends. As discussed in the Background section, VisiBlends creates prototypes by matching two objects with the same basic shape. This is because neuroscience has discovered that the human visual object recognition system is hierarchical in what features it uses to recognize an object: the primary feature it uses to recognize and object is its basic 3D shape, and the secondary features it uses to recognize an object are color, distinct edges, and surface information. If prototypes of blends are made by blending the primary feature used to recognize and object, then it is logical to improve prototypes by blending the secondary features used to recognize an object."
    }, {
      "page": 4,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.712646484375,
        "y1": 602.731201171875,
        "y2": 641.802978515625
      },
      "text": "We performed this visual dimension-based analysis on the 11 improved blends and found that three visual properties were suffcient to explain almost all of the improvements the artists made. Figure 4 shows examples of these dimensions:"
    }, {
      "page": 4,
      "region": {
        "x1": 333.92413330078125,
        "x2": 558.2042236328125,
        "y1": 656.9292602539062,
        "y2": 707.3469848632812
      },
      "text": "• Color: The frst row shows the result of blending a red Lego brick and a diamond ring. The initial blend has good shape ft, but the artist improved the blend by adding the color of the diamond back into the Lego. This creates the illusion of diamond-like facets into the Lego."
    }, {
      "page": 5,
      "region": {
        "x1": 69.76065063476562,
        "x2": 295.4241638183594,
        "y1": 87.0676498413086,
        "y2": 225.156005859375
      },
      "text": "• Silhouette: The second row shows the result of blending an oblong Lego brick and a popsicle. The initial blend is decent, but the artist improved it by applying the silhouette of the popsicle back into the Lego. (Additionally, they blended the color of the popsicle back into the Lego. This lends a textured, popsicle-like red color to the Lego, rather than a smooth plastic-like red.) • Internal Details: The third row shows the result of blending an orange with the head of a snowman. The initial blend is clearly a low-fdelity prototype — the idea is clear, but the details are unrefned. In addition to applying the color and silhouette of the snowman head, they extracted the facial details of the snowman head and placed them on the orange."
    }, {
      "page": 5,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.2154235839844,
        "y1": 235.29322814941406,
        "y2": 285.32501220703125
      },
      "text": "Throughout all the examples, we found that artists used one or more of these three secondary visual dimensions to improve the prototypes. Note that these dimensions are largely independent of one another. Thus, we believe that the three visual dimensions can be used together to guide the process of improving prototypes."
    }, {
      "page": 5,
      "region": {
        "x1": 53.79667663574219,
        "x2": 294.04998779296875,
        "y1": 290.65106201171875,
        "y2": 340.1189880371094
      },
      "text": "Design Principle 2. Identify secondary dimensions of the design space to structure the iteration process. For visual blends, the key secondary dimension are: color, silhouette and internal details. We refer to this method as using secondary design dimensions."
    }],
    "title": {
      "page": 4,
      "region": {
        "x1": 317.9549865722656,
        "x2": 497.4205322265625,
        "y1": 369.1990661621094,
        "y2": 376.04998779296875
      },
      "text": "4.2 Analysis of professional blends"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.560791015625,
        "y1": 372.38421630859375,
        "y2": 499.12799072265625
      },
      "text": "The three visual dimensions provide high-level structure for improving blends, but we wanted to know if there were actionable activities associated with this structure that are useful when improving blends. To investigate this, we worked with two graphic artists in multiple one-hour sessions over a period of three weeks to observe and probe their process. Both designers worked in Photoshop and had created numerous print ads although neither had made visual blends before. The goal of these sessions was to introduce them to the secondary visual dimensions and to see a) if they found them useful to structure their process, b) what actions they took to improve the blends based on these dimensions, and c) whether novices would be able to replicate their success."
    }, {
      "page": 5,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.560791015625,
        "y1": 503.8912353515625,
        "y2": 608.7169799804688
      },
      "text": "To familiarize the artists with the concept of visual blends, we showed them examples of professionally made blends and asked them to recreate two of them in Photoshop. They found the task challenging, but through trial and error they were ultimately satisfed with their results. Next, we introduced them to the principles of blending based on color, silhouette and details. We discussed with them how we thought those principles could have been used to create the blends. Then we gave the artists prototypes of blends and asked them to improve them, referencing the visual dimensions when applicable."
    }, {
      "page": 5,
      "region": {
        "x1": 53.59199905395508,
        "x2": 295.55877685546875,
        "y1": 613.480224609375,
        "y2": 707.3469848632812
      },
      "text": "The concepts of color, silhouette, and internal details were intuitive to the artists, and they readily used them to improve the blends. Blending color was a familiar idea to them, and it was very easy for them to do in Photoshop. An efective tool one artist used for blending was the \"Multiply\" feature, which preserved both the color and the texture of each object, as seen in the top row of Figure 4. Both artists were surprised at how efectively silhouettes could be used in blends. They tried using the concept of silhouette blending in blends such as the middle row of Figure 4 and were pleased"
    }, {
      "page": 5,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7145385742188,
        "y1": 87.45320129394531,
        "y2": 148.4429931640625
      },
      "text": "with the results. The idea of extracting and reapplying details was natural to them, as they had employed analogous features in Photoshop (i.e. magic wand) to manipulate details before. However, even with industry tools, extraction was often tedious. In general, both designers thought that if they worked on the basis of these visual dimensions, they could recreate any visual blend."
    }, {
      "page": 5,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7124633789062,
        "y1": 153.20619201660156,
        "y2": 236.114990234375
      },
      "text": "The artists both note that there were additional techniques they would use to produce and even higher fdelity blends. One artist mentioned the addition or removal of shadows. The other mentioned making a background that would complement the blend. However, when restricted to these three visual dimensions, they could produce a second iteration with substantially reduced seams and enhanced aesthetic quality. If they were producing a pixelperfect print ad, they would want to do a third iteration."
    }, {
      "page": 5,
      "region": {
        "x1": 317.9549560546875,
        "x2": 559.7135620117188,
        "y1": 240.87721252441406,
        "y2": 466.25201416015625
      },
      "text": "As we observed the artists using Photoshop to execute their improvements, we noticed two parts of their process that novice designers would struggle to replicate. First, almost all of the tools the artists used in Photoshop are not available in the typical applications novices use to quickly edit images. The simple flters, cropping, and movement aforded by Instagram, presentation software, and Mac Preview aren’t enough to improve blends. Even simple color changing operations like \"Multiply\" are not available in most enduser tools. This is probably because most end-user tools focus on operations that can be applied to one image at a time. For blending, operations have to apply to two objects. Second, these tools often require multiple steps and tedious low-level manipulation. Applying the silhouette from one object to another is a process with multiple steps including positioning, object extraction, appropriate layer composition, and edge cleanup. Extracting details like the snowman face are tedious, even with the magic wand tool, which largely operates based on pixel color similarity. Instead of making users think in pixels, we want to provide higher-level abstractions, such as the separation of foreground from background or the separation of details from a base. To create operations that novices can use, we need to provide tools at a higher-level of abstraction than pixels."
    }, {
      "page": 5,
      "region": {
        "x1": 317.9549255371094,
        "x2": 559.8056640625,
        "y1": 471.57806396484375,
        "y2": 542.9639892578125
      },
      "text": "Design Principle 3. Provide novices with high-level tools related to the secondary visual dimensions that can preview results without requiring expert knowledge or tedious, lowlevel manipulation. In VisiFit, these tools include (1) extracting and applying silhouettes, (2) blending colors between two objects, and (3) extracting and replacing internal details from one object to another."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 53.79800033569336,
        "x2": 234.08177185058594,
        "y1": 358.03106689453125,
        "y2": 364.8819885253906
      },
      "text": "4.3 Co-Design with Graphic Artists"
    }
  }, {
    "paragraphs": [{
      "page": 5,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.187255859375,
        "y1": 580.6041870117188,
        "y2": 707.3469848632812
      },
      "text": "To help novices iteratively improve visual blends, we created a system called VisiFit that leverages computational tools to help users easily extract and combine visual properties of each image into a blend. First the user improves the cropping of each image, then improves the three secondary visual dimensions one at a time. At each step, they are presented with blend options that are automatically created by the system. However, they are free to interactively edit them. VisiFit is implemented as a Flask-based web application. It uses Numpy, OpenCV, and Tensorfow [1]. It builds on the Fabric.js canvas element to implement interactive image manipulation. Figure 5 shows the fve steps of the interface in the order that users see them."
    }, {
      "page": 6,
      "region": {
        "x1": 53.50199890136719,
        "x2": 294.0454406738281,
        "y1": 430.1502380371094,
        "y2": 447.3039855957031
      },
      "text": "Inputs. VisiFit takes in two inputs that are both outputs from VisiBlends:"
    }, {
      "page": 6,
      "region": {
        "x1": 64.70359802246094,
        "x2": 295.0338134765625,
        "y1": 461.5412292480469,
        "y2": 577.3259887695312
      },
      "text": "(1) An ordered pair of images that have a shape match. We refer to them as Object A and Object B. In Object A, the shape covers the entire object. In Object B, the shape covers only the main body of the object, leaving out parts of the object outside the shape. When blending the images, Object A will be mapped onto Object B. (2) The positioning parameters to align Object A to the shape in Object B: x-scale factor, y-scale factor, angle of rotation, and center position. In the prototype of the blend, Object A is cropped, scaled, and positioned to ft into the shape of Object B."
    }, {
      "page": 6,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.4251708984375,
        "y1": 591.5621948242188,
        "y2": 685.4299926757812
      },
      "text": "Step 1. Extract main shapes When the page loads, the system shows Object A and the results of automatic cropping. Object A is an image of a single object that we want removed from its background. This is a classic computer vision problem: segmenting the salient object in an image. Deep learning approaches have been reported to be a fast and accurate approach to automatic object extraction, so we use the Tensorfow implementation of a pre-trained model for deeply supervised salient object detection [24] and use the mask it provides to crop the images."
    }, {
      "page": 6,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5582275390625,
        "y1": 690.1931762695312,
        "y2": 707.3469848632812
      },
      "text": "The user sees the output for Object A and decides if it is acceptable. If it is, they select it and move to the next step. If not, they"
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2054443359375,
        "y1": 430.1502380371094,
        "y2": 447.3039855957031
      },
      "text": "can decide to improve the object using Interactive Grabcut [47], a traditional computer vision algorithm for foreground extraction."
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2047729492188,
        "y1": 452.0672302246094,
        "y2": 513.0579833984375
      },
      "text": "For Object B, users must use Interactive Grabcut to extract the main shape from the image. Our provided interface for Interactive Grabcut has users frst draw a rectangle that encloses the entire object to extract. Then it produces a foreground extraction shown to users, who can mark extraneous pieces for removal by drawing on the image and running Grabcut again."
    }, {
      "page": 6,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.7157592773438,
        "y1": 517.8211669921875,
        "y2": 589.77001953125
      },
      "text": "We used a classic interactive approach rather than a fully automatic approach because identifying parts or shapes within an image is very difcult. Traditional automatic approaches like Hough Transforms [13] do not work well on most images. Deep learning approaches are fairly good at segmenting objects within images [20] but are not yet capable enough at identifying the internal parts of objects."
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7171020507812,
        "y1": 594.533203125,
        "y2": 666.4830322265625
      },
      "text": "Step 2. Automatically align objects and adjust position. After both objects have had their main shape cropped, the system automatically produces a new prototype using simple afne transformations that move, scale, position, and rotate the objects. Users are free to adjust the alignment with direct manipulation on the Fabric.js HTML5 canvas, just as they would in any image editing application."
    }, {
      "page": 6,
      "region": {
        "x1": 317.9549560546875,
        "x2": 559.185546875,
        "y1": 671.2451782226562,
        "y2": 699.35693359375
      },
      "text": "Step 3 Select a silhouette option. When blending two objects, the blend can use the silhouette of either Object A or B, because they are very close in shape and size. The system automatically"
    }, {
      "page": 7,
      "region": {
        "x1": 53.46699905395508,
        "x2": 294.0461120605469,
        "y1": 657.316162109375,
        "y2": 685.4299926757812
      },
      "text": "creates two versions of the blend - one with the silhouette of Object A and one with the silhouette of Object B. The user must select which silhouette looks better."
    }, {
      "page": 7,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.0450134277344,
        "y1": 690.1931762695312,
        "y2": 707.3469848632812
      },
      "text": "To create the two silhouetted prototypes, the system uses the inverses of the cropped images from Step 1, layers one inverse on"
    }, {
      "page": 7,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2034301757812,
        "y1": 87.45320129394531,
        "y2": 115.5670166015625
      },
      "text": "top of the other original image, and positions them according to the coordinates in Step 2. This efectively creates a mask to produce the silhouette of the object."
    }, {
      "page": 7,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.3739013671875,
        "y1": 120.33021545410156,
        "y2": 159.4019775390625
      },
      "text": "Step 4. Select and adjust color blending options. Color is the next dimension to include in the blend. There are 5 options for color blending. The user can keep the original colors, or use one of four adjustable tools to blend on the colors of objects:"
    }, {
      "page": 7,
      "region": {
        "x1": 333.91766357421875,
        "x2": 559.58349609375,
        "y1": 167.38209533691406,
        "y2": 436.9809875488281
      },
      "text": "• Transparency. We layer Object A onto Object B with 50% transparency to allow the colors of both objects to come through, although somewhat weakly. The user can adjust the transparency level with a slider. • Color Blend. We use K-means clustering to determine the most common color in the main shape of Object B. We then do an additive color blend with the color of Object A. This only works well when one object is very light - otherwise the color turns very dark. • Multiply colors. Multiplying two images is a way to combine colors in a way that preserves characteristics from both. Whereas transparency will always balance between the two, multiplication blends both of the colors (and their shadings) simultaneously. All three examples in Figure 4 use multiply to blend colors. For example, in the Lego and ring example, multiplying colors allowed the Lego to take on the red color and keep the shading of both objects so that the facets of the diamond and the bumps on the Lego can both be seen. • Replace color. We use K-means clustering to determine the most common colors in the main shapes of Object A and B. We replace Object A’s most common color with Object B’s most common color and provide users with an adjustable threshold controlling the degree of color replacement. They can also choose to blend the image with colors they select from an eye dropper tool (not shown in Figure 5)."
    }, {
      "page": 7,
      "region": {
        "x1": 317.52447509765625,
        "x2": 559.7147216796875,
        "y1": 445.3502197265625,
        "y2": 550.176025390625
      },
      "text": "Step 5. Select and re-apply internal details to blend. The last visual dimension to include is internal details - these are smaller objects or salient features that help identify the object. In the snowman and orange blend, the snowman is not as iconic without his facial details. Thus, we want to extract them from the original Object B and place them back on Object A. Again, we use Interactive Grabcut to allow the user to select and refne what details to extract. While we could have used other tools such as context-aware select, Grabcut worked well on our test set and was a method users had already become familiar with in earlier stages of the pipeline."
    }, {
      "page": 7,
      "region": {
        "x1": 317.72625732421875,
        "x2": 558.4320068359375,
        "y1": 554.939208984375,
        "y2": 659.7650146484375
      },
      "text": "VisiFit encourages users to follow a linear workfow through each of the tools. They can see efects previewed on their iteration and choose whether or not to include them. But users are not constrained to one path through the pipeline; they can take multiple paths and add an unrestricted number of edits to the secondary visual dimensions if they so choose. The linear workfow is the default because it allows users to start on a simple path through their structured iteration. At the end, the user selects the blend they are most satisfed with and the system fnishes by showing them the initial blend and the improved blend side by side."
    }],
    "title": {
      "page": 5,
      "region": {
        "x1": 317.9549865722656,
        "x2": 417.17327880859375,
        "y1": 566.2501220703125,
        "y2": 573.1010131835938
      },
      "text": "5 VISIFIT SYSTEM"
    }
  }, {
    "paragraphs": [{
      "page": 7,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.4373779296875,
        "y1": 690.1931762695312,
        "y2": 707.345947265625
      },
      "text": "To evaluate whether VisiFit helps novice designers substantially improve prototypes of visual blends, we conducted a user study"
    }, {
      "page": 8,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.4286804199219,
        "y1": 87.45320129394531,
        "y2": 115.5670166015625
      },
      "text": "where participants used VisiFit to improve 11 VisiBlends prototypes. Two experts then rated those blends to judge whether they were substantially improved over the initial prototype."
    }, {
      "page": 8,
      "region": {
        "x1": 53.7979736328125,
        "x2": 295.0341491699219,
        "y1": 120.33021545410156,
        "y2": 203.23797607421875
      },
      "text": "To choose the prototypes to improve, we frst listed all the blends mentioned in VisiBlends and found 15 candidates. Of these, 2 were already good blends and did not need improvement. Two others had signifcant similarities to blends used in the analysis and formative studies, having blended upon the same or similar objects. Hence, they would not have been fair to use in the evaluation and were thus excluded. This left an evaluation set of 11 diverse blends for diferent objects."
    }, {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5574951171875,
        "y1": 208.00123596191406,
        "y2": 279.95001220703125
      },
      "text": "We recruited 11 novice designers (7 female, average age = 21.5) for a 1-hour long study who were paid $20 for their time. First, they were introduced to the concept of visual blends and shown examples of initial prototypes with their improved versions. Then, they had two blends to practice using the tools on. During this practice session, the experimenter answered questions, demonstrated features, and gave suggestions on how to use the tool."
    }, {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.02984619140625,
        "y1": 284.7132263183594,
        "y2": 345.7040100097656
      },
      "text": "In the next 44 minutes, participants used the segmentation tools to extract the main objects from all 22 images (System Steps 1 and 2) and blend the pairs into 11 improved blends (System Steps 3, 4, and 5). They had two minutes for Steps 1 and 2 and another two minutes for Steps 3, 4 and 5, for a total of 4 minutes to create each blend. All results were saved by the system."
    }, {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.56146240234375,
        "y1": 350.46722412109375,
        "y2": 378.5799865722656
      },
      "text": "After the data was collected, we paid two expert graphic designers $60 per hour to look at every iterated blend and answer two questions for each of them:"
    }, {
      "page": 8,
      "region": {
        "x1": 69.76065063476562,
        "x2": 294.0455627441406,
        "y1": 386.5600891113281,
        "y2": 426.0220031738281
      },
      "text": "• Does the iterated blend present substantial improvement over the prototype? • Is the iterated blend of sufcient quality to post on social media?"
    }, {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.4274597167969,
        "y1": 434.3912353515625,
        "y2": 517.2990112304688
      },
      "text": "The most important question to answer was the frst one: does the tool help with substantial improvements? Small faws in the execution were allowed, but the objects had to be seamlessly and aesthetically blended to count as an improvement. Our second question was how often these iterated blends were good enough for social media publication (i.e. a student club announcement post). Publication would mean that both objects were clearly identifable and blended with no pronounced faws."
    }, {
      "page": 8,
      "region": {
        "x1": 53.7979736328125,
        "x2": 295.560791015625,
        "y1": 522.0621948242188,
        "y2": 626.8880004882812
      },
      "text": "Social media is much more forgiving than print publication. Print publications must be pixel-perfect, well-lit, and high defnition. To meet this bar, a graphic designer should still use a professional tool like Photoshop. However, on social media, the images are often smaller, lower resolution, published more frequently, and for a smaller audience (such as student clubs, classes or majors) - so perfection is not as important. Additionally, the prevalence of lowfdelity user-generated content like memes and self-shot videos lowers the expectation of precision on social media, placing the emphasis on the message."
    }],
    "title": {
      "page": 7,
      "region": {
        "x1": 317.9549865722656,
        "x2": 403.77685546875,
        "y1": 675.839111328125,
        "y2": 682.6900024414062
      },
      "text": "6 EVALUATION"
    }
  }, {
    "paragraphs": [{
      "page": 8,
      "region": {
        "x1": 53.59590148925781,
        "x2": 295.5662841796875,
        "y1": 657.316162109375,
        "y2": 707.3397216796875
      },
      "text": "During the study, the 11 participants attempted to improve a total of 121 blends. Six data points were lost due to errors in the saving process, leaving 115 blends as data points. The judges were introduced to their task with examples of prototypes and their VisiFit-improved counterparts, like the pairs seen in Figure 4 (which were done by the"
    }, {
      "page": 8,
      "region": {
        "x1": 317.62298583984375,
        "x2": 558.20068359375,
        "y1": 87.45320129394531,
        "y2": 126.5260009765625
      },
      "text": "authors with graphic design background). For calibration, judges were shown blends of varying quality, to demonstrate what was considered \"substantial improvement\" and what was considered \"suitable for publication on social media\"."
    }, {
      "page": 8,
      "region": {
        "x1": 317.62298583984375,
        "x2": 559.7124633789062,
        "y1": 131.28822326660156,
        "y2": 258.031982421875
      },
      "text": "After studying the blends resulting from each participant, the judges answered our two questions for all VisiFit-improved blends. Both questions on \"improvement\" and \"suitability for publication\" were highly subjective; however, the raters had “fair agreement” on both questions. They agreed on “substantial improvement” 71.3% of the time (κ = .23) and agreed on “suitability for publication” 73.9% of the time (κ = .37). In particular, there was one blend which they disagreed on every time. Both raters had well-reasoned answers for their diferences and rather than forcing them to agree or introducing another rater, we split the diference and looked at the overall average rates of \"substantial improvement\" and \"suitability for publication\" to report the success of the tool."
    }, {
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2071533203125,
        "y1": 262.7952575683594,
        "y2": 312.8269958496094
      },
      "text": "Overall, people using the tool made substantial improvements to the blend 76.1% of the time. Additionally, those blends were of publishable quality 70.4% of the time. These metrics demonstrate how VisiFit enables novices to quickly and easily complete a difcult iteration task."
    }, {
      "page": 8,
      "region": {
        "x1": 317.6860046386719,
        "x2": 558.4320678710938,
        "y1": 317.5902404785156,
        "y2": 400.49798583984375
      },
      "text": "Judges reported that blends were substantially improved when the parts of the objects looked correctly layered. This efect was achieved in a number of ways through VisiFit: when the silhouette tool was used to mask one object and produce clean borders, when the internal detail extraction tool foregrounded important parts of the bottom image, (i.e. the acorn hat detail in the Guggenheim-acorn blend of Figure 1), or when the colors were blended compositely (i.e. the corn and McDonald’s blend in Figure 6.)"
    }, {
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7205810546875,
        "y1": 405.26123046875,
        "y2": 477.2099914550781
      },
      "text": "For 10 of the 11 images, it was possible for at least one of the 11 participants to create an improved and publishable blend. There are several possible reasons why there was variability in user performance. One was subjectivity; some novice users were able to create high quality blends but chose versions that the judges did not rate as improvements. Judging one’s own work is hard, because creators grow attached to their work and struggle to see it objectively."
    }, {
      "page": 8,
      "region": {
        "x1": 317.65899658203125,
        "x2": 559.7203979492188,
        "y1": 481.9732360839844,
        "y2": 630.635009765625
      },
      "text": "A second and more important reason is the limitation of some of the tools. Cropping entire objects, applying a silhouette, and all four methods of blending colors worked as expected every time. However, the Interactive Grabcut tools for extracting parts of objects was sometimes problematic, since some details were too small to extract properly. While Grabcut is fast and easy, it does not have pixel-level precision. It often helped to improve the blend, but it sometimes weakened their suitability for publication. The VisiFit-improved blend could still be used as a guide when creating a pixel-perfect version in a professional image editing tool. For example, for the blend of the orange slice and the barbeque grill featured in Figure 1, the idea of the blend is clear and improved, but the execution had enough faws for it to be not suitable for publication."
    }, {
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.3700561523438,
        "y1": 635.398193359375,
        "y2": 685.4299926757812
      },
      "text": "There was one prototype that no user was able to improve. The burger and light bulb blend (Figure 6) left a seam between the burger and the light bulb every time. This example points out a limitation of VisiFit that we could fx this by implementing a fll tool to reduce the appearance of seams."
    }, {
      "page": 8,
      "region": {
        "x1": 317.9549865722656,
        "x2": 558.2020874023438,
        "y1": 690.1931762695312,
        "y2": 707.3469848632812
      },
      "text": "Overall, given the speed of the tool, participants thought that the results were well worth the efort they put into it [8]. During"
    }, {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.0461120605469,
        "y1": 87.45320129394531,
        "y2": 104.60797119140625
      },
      "text": "the study, several participants mentioned that the tool was fast and produced results they would not otherwise know how to achieve."
    }],
    "title": {
      "page": 8,
      "region": {
        "x1": 53.79800033569336,
        "x2": 115.01986694335938,
        "y1": 642.9620971679688,
        "y2": 649.81298828125
      },
      "text": "6.1 Results"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 53.46406555175781,
        "x2": 295.5554504394531,
        "y1": 146.73121643066406,
        "y2": 240.5980224609375
      },
      "text": "The two main contributions of this paper are 1) the method of using secondary design dimensions to structure the iterative improvement process and 2) the VisiFit system that helps novices improve blends with a pipeline of computational tools. In this discussion we want to explore how the computational tools could generalize to the needs of expert designers and how secondary design dimensions can be applied to domains beyond visual blends. Additionally, we discuss the intellectual and engineering challenges that come with applying these ideas to new domains."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 134.54714965820312,
        "y1": 132.37710571289062,
        "y2": 139.22802734375
      },
      "text": "7 DISCUSSION"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 53.46699905395508,
        "x2": 294.27508544921875,
        "y1": 295.6722412109375,
        "y2": 433.375
      },
      "text": "Although VisiFit is meant to help novices, we co-designed it with 2 graphic artists who were eager to use it as a rapid prototyping tool despite their prior domain knowledge. Thus, we wanted to see what impressions experts would have on the system and showed the tool to two professional designers (D1 and D2). D1 is a media communications director at a medium-sized organization with over twenty years of experience. D2 is a freelance graphic designer with over 10 years of experience. Both expressed a need to efciently create novel and eye-catching visuals for social media that are beyond the quality produced by tools such as Canva. Both designers had used visual blends in their professional work before, but did not know the name for the concept and did not have a strategy for producing them."
    }, {
      "page": 9,
      "region": {
        "x1": 53.79798889160156,
        "x2": 295.5582275390625,
        "y1": 438.13824462890625,
        "y2": 586.7999877929688
      },
      "text": "We presented them with the same blend examples from the user study and asked them to perform the same task: use the tool to iterate on the blend prototypes and create seamless and aesthetic blends. Both were impressed by how quickly and easily the blending tools helped them explore the design space. All of the basic operations were familiar to them from their experience with Photoshop, but they expressed surprise and relief to see results generated so quickly. D2 said “Sometimes I spend hours pixel pushing just to test an idea. I love being able to test an idea quickly.”. D1 likened it to flter previews on Instagram which she loves to use to make photos more interesting on social media. Even for professional designers who are adept at using pixel-perfect tools, there is a need to provide high-level tools that can preview results without low-level manipulation (Design Principle 3)."
    }, {
      "page": 9,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.55694580078125,
        "y1": 591.5621948242188,
        "y2": 707.3469848632812
      },
      "text": "When using VisiFit, both made blend improvements in a manner diferent from novice designers. D1 especially liked to push the boundaries, to try extracting and blending the less obvious options within the secondary design dimensions. D1 almost always started by looking at the inputs and formulating a plan. However, as D1 proceeded through the workfow, she found better and more surprising ideas from the fare and focus nature of VisiFit. The system helped D1 explore the design space while keeping multiple threads open at a time. From this interaction, we believe that structuring blend improvement around secondary dimensions has value even for professional designers (Design Principle 2)."
    }, {
      "page": 9,
      "region": {
        "x1": 316.445556640625,
        "x2": 559.7144165039062,
        "y1": 87.45320129394531,
        "y2": 225.156005859375
      },
      "text": "D2 was impressed by the way the computational tools worked and particularly so for object extraction. He found Interactive Grabcut impressive in how efective it was on shape extraction but unimpressive in how unsuccessful it could be when selecting internal details. After multiple attempts with the tool, he noted that he would have preferred either better precision during user interaction or a better automatic approach. This raised an important limitation - VisiFit only provided one tool to extract internal details. Having a back-up tool (such as shaped-based cropping) could have relieved user frustration. This reinforces Design Principle 1 - that automatic tools don’t always achieve desired results - and stresses that the system must provide multiple interactive tools specifc for each subtask so that users have control over the creative process."
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549560546875,
        "x2": 558.203369140625,
        "y1": 229.91920471191406,
        "y2": 290.90899658203125
      },
      "text": "Overall, we believe that computational design tools for structured iteration can be as useful to professional designers as they are to novices. Both groups need to explore design spaces quickly and easily. Although experts have the ability to do this with existing tools, a pipeline of computational design tools could make this more efcient and attainable for designers of all experience levels."
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 53.79800033569336,
        "x2": 267.8673095703125,
        "y1": 268.3670959472656,
        "y2": 288.1671142578125
      },
      "text": "7.1 Professional designers’ impressions of VisiFit"
    }
  }, {
    "paragraphs": [{
      "page": 9,
      "region": {
        "x1": 317.5249938964844,
        "x2": 559.1901245117188,
        "y1": 321.32623291015625,
        "y2": 382.3160095214844
      },
      "text": "While the VisiFit system is tailored to the domain of visual blends, we believe the method of editing secondary design dimensions can be used to help novices structure the improvement process for other blending domains. We discuss three domains that it could be generalized to: animated visual blends, fashion design, and furniture design."
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549560546875,
        "x2": 558.20703125,
        "y1": 394.3022155761719,
        "y2": 488.16900634765625
      },
      "text": "7.2.1 Animated Visual Blends. One way to further enhance visual blends is to add motion to them. Although it would be easy to add arbitrary motion, it would be ideal if the motion complemented the message. The top panel of Figure 7 shows a visual blend for condom and action that implies the message “condoms are for action.” (The clapperboard is a symbol of action.) This blend is already efective at conveying the message, but to enhance it we could add motion from the clapperboard onto the condom wrapper. We call this type of motion graphic an animated blend."
    }, {
      "page": 9,
      "region": {
        "x1": 317.9549560546875,
        "x2": 559.7158203125,
        "y1": 492.9322204589844,
        "y2": 608.7169799804688
      },
      "text": "We propose that animated blends can be created by using secondary dimensions of motion to iterate on static blends. To start, we need a reference video that shows typical motion made by one of the objects in the blend. To structure the iteration, we can extract and transfer dimensions of motion from this reference video. The important secondary design dimensions of motion include: the pattern of motion (circular motion, path segments, appearance/disappearance, expansion/contraction, or gradient changes), the speed of motion, acceleration, and timing of the motion. All or some of these dimensions of motion can be applied to create a seamless and aesthetic animated blend."
    }, {
      "page": 9,
      "region": {
        "x1": 317.95489501953125,
        "x2": 559.7124633789062,
        "y1": 613.480224609375,
        "y2": 707.3469848632812
      },
      "text": "Figure 7 shows three animated blends. For each one, it shows the static visual blend, the reference video of one or both objects in motion, and how these secondary dimensions of motion can be added to the visual blend. For the condom and action animation, the pattern of motion of the clapperboard (up-and-down path segments) can be added to the blend to reinforce the message of “action”. For the astronaut and food animation, the speed and arc of motion that the real astronaut travels with can be applied to the static eggastronaut blend. For the tea and sunrise animation, two reference"
    }, {
      "page": 10,
      "region": {
        "x1": 53.46699905395508,
        "x2": 295.4256591796875,
        "y1": 408.47222900390625,
        "y2": 469.4630126953125
      },
      "text": "videos can be applied to the blend. First, the pattern of motion of the tea bag (up-and-down path segments) can be applied to the sun. Second, the pattern of motion of the sunrise (gradient change) can be applied to the sky. Although transferring one type of motion would be sufcient, adding two types of motion further enhances the visual appeal and meaning of the blend."
    }, {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.2783508300781,
        "y1": 474.2262268066406,
        "y2": 524.2569580078125
      },
      "text": "Computational tools would be needed to extract and reapply the aforementioned dimensions of motion. Parameters for each dimension would become points of interactions for users. These tools could then be chained together into a pipeline similar to VisiFit to structure the iterative improvement of animated blends."
    }, {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.5646057128906,
        "y1": 537.711181640625,
        "y2": 598.7009887695312
      },
      "text": "7.2.2 Fashion and Furniture Design. In furniture and fashion design, one type of problem is to combine diferent styles to achieve a new purpose. One way to do this is arguably a type of blending - to borrow from the functional and stylistic elements of both styles to create a new hybrid style. Two examples of hybrid styles include athleisure clothing and “updated classics” in furniture design."
    }, {
      "page": 10,
      "region": {
        "x1": 69.76065063476562,
        "x2": 295.564697265625,
        "y1": 608.0888061523438,
        "y2": 680.4219970703125
      },
      "text": "• Athleisure is a clothing style that takes the fabrics and styles of athletic clothing and adapts them to non-athletic environments such as work, school, or other social environments [23, 48]. • Updated classics is a style of furniture design that takes the rich feel of classic furniture and adapts it to modern and utility-driven needs of the 21st century."
    }, {
      "page": 10,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.191162109375,
        "y1": 408.47222900390625,
        "y2": 600.969970703125
      },
      "text": "identifed which two objects to blend, a tool would help users determine what secondary dimensions to extract and apply from each object. For example, a fashion designer could operate on the dimensions of material, silhouette (neckline, hemline, leg width, etc.), color/pattern, fabrication (seam placement, grain direction, etc.), and details (closures, stitching, etc.) When combining these dimensions to create a hybrid style such as athleisure, designers often use the stretchy material of athletic clothing, the details and colors of street clothing, and a mix of silhouettes found in the gym, street, or workplace. This combination of traits helps designers achieve both comfort and socially appropriate styles. A similar set of dimensions could be used for furniture design to achieve a blend of classic sophistication with modern convenience. For example, an updated classic chair could use the classic shape of a Louis XIV chair, but fabricate it out of plastic (as is common in modern chairs) to make it easier to move and clean. It could also reduce some of the ornamentation on the silhouette to take on aspects of a minimal modern look."
    }, {
      "page": 10,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.7216796875,
        "y1": 605.7332153320312,
        "y2": 677.6820068359375
      },
      "text": "We believe this blending process can also be structured by chaining together computational tools for each of the secondary design dimensions. This process should be interactive, using human judgment not only to guide the search, but also to constantly consider aspects outside the dimensions such as the social acceptability of the design, the appeal to the target market, and whether its construction is feasible within desired price points."
    }, {
      "page": 10,
      "region": {
        "x1": 53.79800033569336,
        "x2": 294.2110290527344,
        "y1": 690.1931762695312,
        "y2": 707.3469848632812
      },
      "text": "We propose that creating items within these hybrid styles could be structured using secondary design dimensions. Once the user"
    }],
    "title": {
      "page": 9,
      "region": {
        "x1": 317.9549865722656,
        "x2": 553.2642822265625,
        "y1": 306.9720764160156,
        "y2": 313.822998046875
      },
      "text": "7.2 Generalization to other blending problems"
    }
  }, {
    "paragraphs": [{
      "page": 11,
      "region": {
        "x1": 53.52899932861328,
        "x2": 295.5634765625,
        "y1": 550.6651611328125,
        "y2": 655.4920043945312
      },
      "text": "The major intellectual challenge of applying these techniques to any new domain is discovering what its secondary design dimensions are. For VisiFit, we were able to observe the dimensions from examples and from co-design sessions. Additionally, we were guided by what is known about the neuroscience of human visual object recognition. If one or more of those approaches is not available in a new domain, signifcant trial and error may be required to identify those dimensions. An exciting challenge would be to use computational tools to automatically (or semi-automatically) discover the secondary design dimensions of a new domain."
    }, {
      "page": 11,
      "region": {
        "x1": 53.79800033569336,
        "x2": 295.55987548828125,
        "y1": 660.2542114257812,
        "y2": 708.1678466796875
      },
      "text": "The major engineering challenge of applying these design principles to a new domain is to build computational tools that can help explore each dimension with high-level tools rather than low-level manipulation. Deep learning has provided new hope for such tools, but there are still limitations to what deep learning systems can do,"
    }, {
      "page": 11,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.5827026367188,
        "y1": 87.45320129394531,
        "y2": 104.60797119140625
      },
      "text": "especially with limited data. This is an open challenge: to quickly create new computational tools for the dimensions of new domains."
    }, {
      "page": 11,
      "region": {
        "x1": 317.5249938964844,
        "x2": 559.5825805664062,
        "y1": 109.37123107910156,
        "y2": 214.197021484375
      },
      "text": "For any new blending domain, there is also the possibility that some blends are too complex to be structured around secondary dimensions due to complex interactions between dimensions. For example, when the DNA of two parents are combined to make a ofspring, the ofspring certainly has a blend of the parents features, but there are so many features that the combinations become too complex to choose from. There may be too many dependencies between dimensions that make designing at a high level impossible. When considering the dimensions of a new domain, one should look out for such dependencies."
    }],
    "title": {
      "page": 11,
      "region": {
        "x1": 53.79800033569336,
        "x2": 136.00897216796875,
        "y1": 536.3120727539062,
        "y2": 543.1629638671875
      },
      "text": "7.3 Limitations"
    }
  }, {
    "paragraphs": [{
      "page": 11,
      "region": {
        "x1": 317.6409912109375,
        "x2": 559.5825805664062,
        "y1": 242.8701934814453,
        "y2": 358.6549987792969
      },
      "text": "Iterative improvement is the essence of the iterative design process. Although there are many existing tools that support other phases of the design process (brainstorming, prototyping, evaluation, and fnal design execution) there are a lack of tools focusing on iteration [17]. We present a tool that helps novices iteratively improve on prototypes of visual blends. Visual blends are an advanced graphic design technique to combine two visual symbols into one object to convey a message symbolically. Tools already exist to help novices create initial prototypes of visual blends. However, novices do not have tools or strategies to support them through the next iteration that would take the blend from lower to higher fdelity."
    }, {
      "page": 11,
      "region": {
        "x1": 317.6860046386719,
        "x2": 559.7164306640625,
        "y1": 363.4182434082031,
        "y2": 555.9149780273438
      },
      "text": "We conducted three preliminary investigations on how to iteratively improve visual blends. This included an exploration of automatic tools, analysis of expert examples, and a co-design process with graphic designers. From these studies we derived three design principles that can be employed in the creation of iteration tools. Additionally, we introduce a method of using secondary design dimension to structure the iterative improvement process. Based on this method, we can create computational tools to help users explore the design space during iteration. For visual blends, the secondary design dimensions are color, silhouette, and internal details. The computational tools we implemented to explore each of these dimensions used a combination of deep learning, computer vision techniques, and parametric control for fne-tuning. The principles are demonstrated in our system VisiFit – a pipeline of computational design tools to iterate on prototypes of visual blends. Our evaluation shows that when using VisiFit, novices substantially improve blends 76% of the time. Their blends were of sufcient quality for publication on social media 70% of the time."
    }, {
      "page": 11,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.721435546875,
        "y1": 560.67822265625,
        "y2": 643.5870361328125
      },
      "text": "Although creating visual blends is a domain-specifc problem, it is emblematic of many design challenges which involve the blending or remixing of existing things to produce novel meaning or purpose. We discuss how these principles could be reapplied in three other blending domains: animated blends, hybrid fashion design, and hybrid furniture design. These domains have their own secondary dimensions which can be used to structure the iterative improvement process."
    }],
    "title": {
      "page": 11,
      "region": {
        "x1": 317.9549865722656,
        "x2": 405.7296142578125,
        "y1": 228.51608276367188,
        "y2": 235.36700439453125
      },
      "text": "8 CONCLUSION"
    }
  }, {
    "paragraphs": [{
      "page": 11,
      "region": {
        "x1": 321.197998046875,
        "x2": 558.968017578125,
        "y1": 670.6480712890625,
        "y2": 675.4669799804688
      },
      "text": "[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,"
    }, {
      "page": 11,
      "region": {
        "x1": 333.19183349609375,
        "x2": 559.3805541992188,
        "y1": 678.619140625,
        "y2": 707.3469848632812
      },
      "text": "Craig Citro, Greg S. Corrado, Andy Davis, Jefrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geofrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike"
    }, {
      "page": 12,
      "region": {
        "x1": 53.79400634765625,
        "x2": 295.2237854003906,
        "y1": 88.83009338378906,
        "y2": 715.2369995117188
      },
      "text": "Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. http://tensorfow.org/ Software available from tensorfow.org. [2] Maneesh Agrawala, Wilmot Li, and Floraine Berthouzoz. 2011. Design Principles for Visual Communication. Commun. ACM 54, 4 (April 2011), 60–69. https: //doi.org/10.1145/1924421.1924439 [3] Pete Barry. 2016. The Advertising Concept Book: Think Now, Design Later (Third). Thames & Hudson, London, UK. 296 pages. [4] Gilbert Louis Bernstein and Wilmot Li. 2015. Lillicon: Using Transient Widgets to Create Scale Variations of Icons. ACM Trans. Graph. 34, 4, Article 144 (July 2015), 11 pages. https://doi.org/10.1145/2766980 [5] Dino Borri and Domenico Camarda. 2009. The Cooperative Conceptualization of Urban Spaces in AI-assisted Environmental Planning. In Proceedings of the 6th International Conference on Cooperative Design, Visualization, and Engineering (Luxembourg, Luxembourg) (CDVE’09). Springer-Verlag, Berlin, Heidelberg, 197– 207. http://dl.acm.org/citation.cfm?id=1812983.1813012 [6] Zoya Bylinskii, Nam Wook Kim, Peter O’Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfster, Fredo Durand, Bryan Russell, and Aaron Hertzmann. 2017. Learning Visual Importance for Graphic Designs and Data Visualizations. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (Qu&#233;bec City, QC, Canada) (UIST ’17). ACM, New York, NY, USA, 57–69. https://doi.org/10.1145/3126594.3126653 [7] Zoya Bylinskii, Nam Wook Kim, Peter O’Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfster, Fredo Durand, Bryan Russell, and Aaron Hertzmann. 2017. Learning Visual Importance for Graphic Designs and Data Visualizations. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (Québec City, QC, Canada) (UIST ’17). Association for Computing Machinery, New York, NY, USA, 57–69. https://doi.org/10.1145/3126594.3126653 [8] Erin Cherry and Celine Latulipe. 2014. Quantifying the Creativity Support of Digital Tools through the Creativity Support Index. ACM Trans. Comput.-Hum. Interact. 21, 4, Article 21 (June 2014), 25 pages. https://doi.org/10.1145/2617588 [9] Lydia B. Chilton, Savvas Petridis, and Maneesh Agrawala. 2019. VisiBlends: A Flexible Workfow for Visual Blends. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI ’19). Association for Computing Machinery, New York, NY, USA, Article 172, 14 pages. https://doi.org/10.1145/3290605.3300402 [10] Nicholas Davis, Chih-PIn Hsiao, Kunwar Yashraj Singh, Lisa Li, Sanat Moningi, and Brian Magerko. 2015. Drawing Apprentice: An Enactive Co-Creative Agent for Artistic Collaboration. In Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition (Glasgow, United Kingdom) (C&C ’15). Association for Computing Machinery, New York, NY, USA, 185–186. https://doi.org/10.1145/ 2757226.2764555 [11] Niraj Ramesh Dayama, Kashyap Todi, Taru Saarelainen, and Antti Oulasvirta. 2020. GRIDS: Interactive Layout Design with Integer Programming. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376553 [12] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jefrey Nichols, and Ranjitha Kumar. 2017. Rico: A Mobile App Dataset for Building Data-Driven Design Applications. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology (Québec City, QC, Canada) (UIST ’17). Association for Computing Machinery, New York, NY, USA, 845–854. https://doi.org/10.1145/3126594.3126651 [13] Richard O. Duda and Peter E. Hart. 1972. Use of the Hough Transformation to Detect Lines and Curves in Pictures. Commun. ACM 15, 1 (Jan. 1972), 11–15. https://doi.org/10.1145/361237.361242 [14] Morwaread M. Farbood, Egon Pasztor, and Kevin Jennings. 2004. Hyperscore: A Graphical Sketchpad for Novice Composers. IEEE Comput. Graph. Appl. 24, 1 (Jan. 2004), 50–54. https://doi.org/10.1109/MCG.2004.1255809 [15] G. Fauconnier and M. Turner. 2002. The Way We Think: Conceptual Blending and the Mind’s Hidden Complexities. Basic Books, New York, NY, USA. [16] Charles Forceville. 1994. Pictorial Metaphor in Advertisements. Metaphor and Symbolic Activity 9, 1 (1994), 1–29. [17] Jonas Frich, Lindsay MacDonald Vermeulen, Christian Remy, Michael Mose Biskjaer, and Peter Dalsgaard. 2019. Mapping the Landscape of Creativity Support Tools in HCI. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI ’19). ACM, New York, NY, USA, Article 389, 18 pages. https://doi.org/10.1145/3290605.3300619 [18] Krzysztof Z. Gajos, Daniel S. Weld, and Jacob O. Wobbrock. 2010. Automatically Generating Personalized User Interfaces with Supple. Artif. Intell. 174, 12-13 (Aug. 2010), 910–950. https://doi.org/10.1016/j.artint.2010.05.005 [19] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 2016. Image Style Transfer Using Convolutional Neural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, New York, NY, USA, 2414–2423. [20] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron."
    }, {
      "page": 12,
      "region": {
        "x1": 317.95458984375,
        "x2": 559.380615234375,
        "y1": 88.82929992675781,
        "y2": 699.3780517578125
      },
      "text": "[21] Björn Hartmann, Scott R. Klemmer, Michael Bernstein, Leith Abdulla, Brandon Burr, Avi Robinson-Mosher, and Jennifer Gee. 2006. Refective Physical Prototyping Through Integrated Design, Test, and Analysis. In Proceedings of the 19th Annual ACM Symposium on User Interface Software and Technology (Montreux, Switzerland) (UIST ’06). ACM, New York, NY, USA, 299–308. https://doi.org/10.1145/1166253.1166300 [22] Narayan Hegde, Jason D Hipp, Yun Liu, Michael Emmert-Buck, Emily Reif, Daniel Smilkov, Michael Terry, Carrie J Cai, Mahul B Amin, Craig H Mermel, Phil Q Nelson, Lily H Peng, Greg S Corrado, and Martin C Stumpe. 2019. Similar image search for histopathology: SMILY. npj Digital Medicine 2, 1 (2019), 56. https://doi.org/10.1038/s41746-019-0131-z [23] Elizabeth Holmes. 2015. Athleisure: A Workout Look for Every Occasion. https://www.wsj.com/video/athleisure-a-workout-look-for-everyoccasion/D0174829-3288-40F1-9E12-0F420E38AA9A.html [24] Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. S. Torr. 2019. Deeply Supervised Salient Object Detection with Short Connections. IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 4 (2019), 815–828. https://doi.org/10.1109/ TPAMI.2018.2815688 [25] P. Karimi, M. L. Maher, K. Grace, and N. Davis. 2019. A computational model for visual conceptual blends. IBM Journal of Research and Development 63, 1 (2019), 5:1–5:10. [26] P. Karimi, M. L. Maher, k. Grace, and N. Davis. 2019. A Computational Model for Visual Conceptual Blends. IBM J. Res. Dev. 63, 1, Article 1 (Jan. 2019), 10 pages. https://doi.org/10.1147/JRD.2018.2881736 [27] Joy Kim, Maneesh Agrawala, and Michael S. Bernstein. 2017. Mosaic: Designing online creative communities for sharing works-in-progress. In Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW. Association for Computing Machinery, New York, New York, USA, 246–258. https://doi.org/ 10.1145/2998181.2998195 arXiv:1611.02666 [28] Joy Kim, Sarah Sterman, Allegra Argent Beal Cohen, and Michael S. Bernstein. 2017. Mechanical novel: Crowdsourcing complex work through refection and revision. In Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW. Association for Computing Machinery, New York, New York, USA, 233–245. https://doi.org/10.1145/2998181.2998196 arXiv:1611.02682 [29] Janin Koch, Andrés Lucero, Lena Hegemann, and Antti Oulasvirta. 2019. May AI? Design ideation with cooperative contextual bandits. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, New York, USA, 1–12. [30] Ranjitha Kumar, Arvind Satyanarayan, Cesar Torres, Maxine Lim, Salman Ahmad, Scott R. Klemmer, and Jerry O. Talton. 2013. Webzeitgeist: Design Mining the Web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Paris, France) (CHI ’13). ACM, New York, NY, USA, 3083–3092. https: //doi.org/10.1145/2470654.2466420 [31] James A. Landay. 1996. SILK: Sketching Interfaces Like Krazy. In Conference Companion on Human Factors in Computing Systems (Vancouver, British Columbia, Canada) (CHI ’96). ACM, New York, NY, USA, 398–399. https://doi.org/10.1145/ 257089.257396 [32] James Lin, Mark W. Newman, Jason I. Hong, and James A. Landay. 2000. DENIM: Finding a Tighter Fit Between Tools and Practice for Web Site Design. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (The Hague, The Netherlands) (CHI ’00). ACM, New York, NY, USA, 510–517. https://doi.org/10.1145/332040.332486 [33] Greg Little, Lydia B. Chilton, Max Goldman, and Robert C. Miller. 2010. TurKit: Human Computation Algorithms on Mechanical Turk. In Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology (New York, New York, USA) (UIST ’10). ACM, New York, NY, USA, 57–66. https: //doi.org/10.1145/1866029.1866040 [34] J. Derek Lomas, Jodi Forlizzi, Nikhil Poonwala, Nirmal Patel, Sharan Shodhan, Kishan Patel, Ken Koedinger, and Emma Brunskill. 2016. Interface Design Optimization As a Multi-Armed Bandit Problem. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA) (CHI ’16). ACM, New York, NY, USA, 4142–4153. https://doi.org/10.1145/2858036.2858425 [35] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J. Cai. 2020. Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376739 [36] Kurt Luther, Amy Pavel, Wei Wu, Jari-lee Tolentino, Maneesh Agrawala, Björn Hartmann, and Steven P. Dow. 2014. CrowdCrit: Crowdsourcing and Aggregating Visual Design Critique. In Proceedings of the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work &#38; Social Computing (Baltimore, Maryland, USA) (CSCW Companion ’14). ACM, New York, NY, USA, 21–24. https://doi.org/10.1145/2556420.2556788 [37] J. Marks, B. Andalman, P. A. Beardsley, W. Freeman, S. Gibson, J. Hodgins, T. Kang, B. Mirtich, H. Pfster, W. Ruml, K. Ryall, J. Seims, and S. Shieber. 1997. Design Galleries: A General Approach to Setting Parameters for Computer Graphics and Animation. In Proceedings of the 24th Annual Conference on Computer Graphics and"
    }, {
      "page": 13,
      "region": {
        "x1": 53.79800033569336,
        "x2": 176.6275634765625,
        "y1": 63.2986946105957,
        "y2": 67.34698486328125
      },
      "text": "CHI ’21, May 8–13, 2021, Yokohama, Japan"
    }, {
      "page": 13,
      "region": {
        "x1": 53.80097961425781,
        "x2": 295.2236022949219,
        "y1": 88.83009338378906,
        "y2": 205.23211669921875
      },
      "text": "Interactive Techniques (SIGGRAPH ’97). ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 389–400. https://doi.org/10.1145/258734.258887 [38] Justin Matejka, Michael Glueck, Erin Bradner, Ali Hashemi, Tovi Grossman, and George Fitzmaurice. 2018. Dream Lens: Exploration and Visualization of LargeScale Generative Design Datasets. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI ’18). Association for Computing Machinery, New York, NY, USA, Article 369, 12 pages. https://doi.org/10.1145/3173574.3173943 [39] Brad A. Myers, Ashley Lai, Tam Minh Le, Young Seok Yoon, Andrew Faulring, and Joel Brandt. 2015. Selective undo support for painting applications. In Conference on Human Factors in Computing Systems - Proceedings, Vol. 2015-April. Association for Computing Machinery, New York, New York, USA, 4227–4236. https://doi.org/10.1145/2702123.2702543 [40] J. Nielsen. 1993. Iterative user-interface design. Computer 26, 11 (1993), 32–41. [41] Peter O’Donovan, Aseem Agarwala, and Aaron Hertzmann. 2014. Learning"
    }, {
      "page": 13,
      "region": {
        "x1": 53.80097961425781,
        "x2": 295.2237854003906,
        "y1": 208.38209533691406,
        "y2": 579.8259887695312
      },
      "text": "Layouts for Single-Page Graphic Designs. IEEE Transactions on Visualization and Computer Graphics 20, 8 (2014), 1200–1213. [42] The Editors of WHY Magazine. 1972. Design Q & A: Charles and Ray Eames. https://www.hermanmiller.com/stories/why-magazine/design-q-and-acharles-and-ray-eames/ [43] Francisco Pereira. 2007. Creativity and AI: A Conceptual Blending Approach. Mouton de Gruyter, Berlin, Germany. [44] Savvas Petridis and Lydia B. Chilton. 2019. Human Errors in Interpreting Visual Metaphor. In Proceedings of the 2019 on Creativity and Cognition (San Diego, CA, USA) (C&C ’19). Association for Computing Machinery, New York, NY, USA, 187–197. https://doi.org/10.1145/3325480.3325503 [45] Daniela Retelny, Sébastien Robaszkiewicz, Alexandra To, Walter S. Lasecki, Jay Patel, Negar Rahmati, Tulsee Doshi, Melissa Valentine, and Michael S. Bernstein. 2014. Expert Crowdsourcing with Flash Teams. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (Honolulu, Hawaii, USA) (UIST ’14). ACM, New York, NY, USA, 75–85. https://doi.org/10.1145/ 2642918.2647409 [46] M. Riddoch and G. Humphreys. 2001. Object Recognition. In B. Rapp (Ed.), Handbook of Cognitive Neuropsychology. Psychology Press, Hove, England. [47] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. 2004. \"GrabCut\": Interactive Foreground Extraction Using Iterated Graph Cuts. In ACM SIGGRAPH 2004 Papers (Los Angeles, California) (SIGGRAPH ’04). ACM, New York, NY, USA, 309–314. https://doi.org/10.1145/1186562.1015720 [48] Sam Sanders. 2015. For The Modern Man, The Sweatpant Moves Out Of The Gym. https://www.npr.org/2015/04/08/397138654/for-the-modern-man-thesweatpant-moves-out-of-the-gym [49] Ben Shneiderman. 2007. Creativity Support Tools: Accelerating Discovery and Innovation. Commun. ACM 50, 12 (Dec. 2007), 20–32. https://doi.org/10.1145/ 1323688.1323689 [50] Pao Siangliulue, Joel Chan, Steven P. Dow, and Krzysztof Z. Gajos. 2016. IdeaHound: Improving Large-scale Collaborative Ideation with Crowd-Powered Realtime Semantic Modeling. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (Tokyo, Japan) (UIST ’16). ACM, New York, NY, USA, 609–624. https://doi.org/10.1145/2984511.2984578 [51] Vikash Singh, Celine Latulipe, Erin Carroll, and Danielle Lottridge. 2011. The Choreographer’s Notebook: A Video Annotation System for Dancers and Choreographers. In Proceedings of the 8th ACM Conference on Creativity and Cognition (Atlanta, Georgia, USA) (C&C ’11). Association for Computing Machinery, New York, NY, USA, 197–206. https://doi.org/10.1145/2069618.2069653 [52] Gillian Smith, Jim Whitehead, and Michael Mateas. 2010. Tanagra: A Mixedinitiative Level Design Tool. In Proceedings of the Fifth International Conference on the Foundations of Digital Games (Monterey, California) (FDG ’10). ACM, New York, NY, USA, 209–216. https://doi.org/10.1145/1822348.1822376 [53] Robert J Sternberg. 2011. Cognitive Psychology. Cengage Learning, Boston, MA, USA. [54] Sou Tabata, Hiroki Yoshihara, Haruka Maeda, and Kei Yokoyama. 2019. Automatic Layout Generation for Graphical Design Magazines. In ACM SIGGRAPH 2019"
    }, {
      "page": 13,
      "region": {
        "x1": 519.2670288085938,
        "x2": 558.20166015625,
        "y1": 63.2986946105957,
        "y2": 67.34698486328125
      },
      "text": "Chilton, et al."
    }, {
      "page": 13,
      "region": {
        "x1": 317.9549865722656,
        "x2": 559.3826904296875,
        "y1": 88.83009338378906,
        "y2": 570.2100219726562
      },
      "text": "Posters (Los Angeles, California) (SIGGRAPH ’19). ACM, New York, NY, USA, Article 9, 2 pages. https://doi.org/10.1145/3306214.3338574 [55] Michael Terry and Elizabeth D. Mynatt. 2002. Side Views: Persistent, onDemand Previews for Open-Ended Tasks. In Proceedings of the 15th Annual ACM Symposium on User Interface Software and Technology (Paris, France) (UIST ’02). Association for Computing Machinery, New York, NY, USA, 71–80. https://doi.org/10.1145/571985.571996 [56] Kashyap Todi, Jussi Jokinen, Kris Luyten, and Antti Oulasvirta. 2019. Individualising Graphical Layouts with Predictive Visual Search Models. ACM Trans. Interact. Intell. Syst. 10, 1, Article 9 (Aug. 2019), 24 pages. https://doi.org/10.1145/3241381 [57] Rajan Vaish, Shirish Goyal, Amin Saberi, and Sharad Goel. 2018. Creating Crowdsourced Research Talks at Scale. In Proceedings of the 2018 World Wide Web Conference (Lyon, France) (WWW ’18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 1–11. https://doi.org/10.1145/3178876.3186031 [58] Margot van Mulken, Rob le Pair, and Charles Forceville. 2010. The impact of perceived complexity, deviation and comprehension on the appreciation of visual metaphor in advertising across three European countries. Journal of Pragmatics 42, 12 (2010), 3418 – 3430. https://doi.org/10.1016/j.pragma.2010.04.030 [59] Hao-Chuan Wang, Dan Cosley, and Susan R. Fussell. 2010. Idea Expander: Supporting Group Brainstorming with Conversationally Triggered Visual Thinking Stimuli. In Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work (Savannah, Georgia, USA) (CSCW ’10). Association for Computing Machinery, New York, NY, USA, 103–106. https://doi.org/10.1145/1718918.1718938 [60] Jingdong Wang and Xian-Sheng Hua. 2011. Interactive image search by color map. ACM Transactions on Intelligent Systems and Technology (TIST) 3, 1 (2011), 1–23. [61] Kento Watanabe, Yuichiroh Matsubayashi, Kentaro Inui, Tomoyasu Nakano, Satoru Fukayama, and Masataka Goto. 2017. LyriSys: An interactive support system for writing lyrics based on topic transition. In International Conference on Intelligent User Interfaces, Proceedings IUI. Association for Computing Machinery, New York, New York, USA, 559–563. https://doi.org/10.1145/3025171.3025194 [62] Ariel Weingarten, Ben Lafreniere, George Fitzmaurice, and Tovi Grossman. 2019. DreamRooms: Prototyping Rooms in Collaboration with a Generative Process. In Proceedings of the 45th Graphics Interface Conference on Proceedings of Graphics Interface 2019 (Kingston, Canada) (GI’19). Canadian HumanComputer Communications Society, Waterloo, CAN, Article 19, 9 pages. https: //doi.org/10.20380/GI2019.19 [63] Anbang Xu, Shih-Wen Huang, and Brian Bailey. 2014. Voyant: Generating Structured Feedback on Visual Designs Using a Crowd of Non-experts. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &#38; Social Computing (Baltimore, Maryland, USA) (CSCW ’14). ACM, New York, NY, USA, 1433–1444. https://doi.org/10.1145/2531602.2531604 [64] Lixiu Yu, Aniket Kittur, and Robert E. Kraut. 2014. Distributed Analogical Idea Generation: Inventing with Crowds. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI ’14). ACM, New York, NY, USA, 1245–1254. https://doi.org/10.1145/2556288.2557371 [65] Lixiu Yu, Aniket Kittur, and Robert E. Kraut. 2014. Searching for Analogical Ideas with Crowds. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI ’14). ACM, New York, NY, USA, 1225–1234. https://doi.org/10.1145/2556288.2557378 [66] Lixiu Yu and Jefrey V. Nickerson. 2011. Cooks or Cobblers?: Crowd Creativity Through Combination. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada) (CHI ’11). ACM, New York, NY, USA, 1393–1402. https://doi.org/10.1145/1978942.1979147 [67] Zhenpeng Zhao, Sriram Karthik Badam, Senthil Chandrasegaran, Deok Gun Park, Niklas Elmqvist, Lorraine Kisselburgh, and Karthik Ramani. 2014. SkWiki: A multimedia sketching system for collaborative creativity. In Conference on Human Factors in Computing Systems - Proceedings. Association for Computing Machinery, New York, New York, USA, 1235–1244. https://doi.org/10.1145/ 2556288.2557394"
    }],
    "title": {
      "page": 11,
      "region": {
        "x1": 317.9549865722656,
        "x2": 387.3695983886719,
        "y1": 657.9061279296875,
        "y2": 664.7570190429688
      },
      "text": "REFERENCES"
    }
  }]
}