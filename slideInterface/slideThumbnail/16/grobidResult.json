{"authors": "Maria D Molina; Naeemul Hassan; S Shyam Sundar; Thai Le; Md Main; Uddin Rony; Dongwon Lee", "pub_date": "", "title": "Does Clickbait Actually Atract More Clicks? Three Clickbait Studies You Must Read", "abstract": "Studies show that users do not reliably click more often on headlines classifed as clickbait by automated classifers. Is this because the linguistic criteria (e.g., use of lists or questions) emphasized by the classifers are not psychologically relevant in attracting interest, or because their classifcations are confounded by other unknown factors associated with assumptions of the classifers? We address these possibilities with three studies-a quasi-experiment using headlines classifed as clickbait by three machine-learning models (Study 1), a controlled experiment varying the headline of an identical news story to contain only one clickbait characteristic (Study 2), and a computational analysis of four classifers using real-world sharing data (Study 3). Studies 1 and 2 revealed that clickbait did not generate more curiosity than non-clickbait. Study 3 revealed that while some headlines generate more engagement, the detectors agreed on a classifcation only 47% of the time, raising fundamental questions about their validity.\u2022 Human-centered computing \u2192 Interaction design; Empirical studies in interaction design.", "sections": [{"heading": "INTRODUCTION", "text": "Click-through is the currency of the modern Internet, with content creators striving to garner clicks by baiting users. As a result, we are bombarded daily with a plethora of clickbait, or headlines designed to persuade users to click on them by evoking curiosity and intrigue [1,2,6,36,40]. Clickbait media used to be associated with low quality and often misleading articles from unreliable sources (e.g., conspiracy, junk science, and satire sites) [7,28,36], but increasingly, respectable news outlets of established mainstream media have resorted to these strategies as well in order to gain user attention in a crowded information space [36,38]. Because clickbait is often associated with scams and misinformation, many argue that articles utilizing clickbait should be detected and demoted or downranked in news aggregators [1], with extensive eforts in the academic community devoted to building algorithms for automatic detection of clickbait. This is exemplifed by the 2017 Clickbait Challenge where teams of scholars worked on clickbait detection solutions to automatically detect this potentially malicious content [34]. But, are clickbait headlines clickbaity? Do users actually click more on clickbait? Literature across diferent felds show mixed results. On the one hand, research reveals a clear user preference toward clickbait content [36,44]. However, other research reveals that this is not always the case [29,30,37]. There are a few possible reasons for the conficting fndings. First, it is possible that the diferences derive from the distinct operationalizations of clickbait, with each study utilizing diferent characteristics of clickbait to study its efects. For example, two studies [29,37] tested two styles or characteristics of clickbait-forward-referencing (akin to the \"wh\" characteristic in the present study, (i.e., who, what, where, when, why) and questions (i.e., headlines that ask a question), and compared user engagement and perceptions between these two styles and non-clickbait headlines. In Scacco and Muddiman [37], users engaged more with nonclickbait compared to either of the two clickbait styles. In Molyneux and Coddington [29], the question headline was perceived as lower quality than the non-clickbait headline, although the efects were small. On the other hand, Chakraborty [6] and Rony and colleagues [36] employed several characteristics of clickbait, including questions like in Scacco and Muddiman [37] and Molyneux and Coddington [29], but also other characteristics such as listicles, demonstrative adjectives and hyperboles. Likewise, Venneti and Alam [44] utilized several linguistic characteristics in their study, such as the use of exclamation and question marks, length and structure of the headline, as well as entities of the headline such as important events and fgures. Contrary to Scacco and Muddiman [37] and Molyneux and Coddington [29], Rony and colleagues [36] and Venneti and Alam [44] found that, on average, clickbait received more engagement than non-clickbait. The diference in the characteristics used to defne clickbait among these studies suggests that the conficting fndings might be because some characteristics of clickbait might be more successful than others at generating engagement. Initial evidence of this possibility is revealed in Lockwood's study [24]. The author's analysis of headlines of academic articles revealed that while positive framing and arousing framing increased attention to an academic article, framing the title as a question made no diference, and utilizing wordplay negatively infuenced attention.\nA second possibility for the mixed results of the efects of clickbait found in the literature could be the diferent methodological approaches to study clickbait engagement. Notably, past studies conducted using experimental designs and content analysis [29,37] fnd that non-clickbait is more engaging than clickbait. Studies using automatic detection [36,44] reveal the opposite pattern. While it is possible that the null results from experimental studies is due to the low external validity of the method, which does not allow participants to interact with headlines as they normally would during their regular browsing, it is also possible that the engagement captured through computational methods do not represent engagement with clickbait per se, but third variables embedded in the assumptions of each clickbait detector. For example, while some computational models are trained with data labeled as clickbait (or non-clickbait) by human coders (see [6] for example), other models utilize source assumptions for labeling of data (see [26] for example). This represents a fundamental diference in the conceptual defnition and understanding of clickbait. Likewise, classifcation systems to detect clickbait are built using diferent machine learning approaches and techniques. For example, while some detectors are built using traditional machine learning (Na\u00efve Bayes) [13], others utilize more complex deep learning models [36]. The distinct assumptions of classifcation systems are refected in the manner in which their training data are coded (manual coding compared to source assumption), the type of machine learning model employed, and so on, thereby resulting in systems that, although accurate, classify clickbait using their unique idiosyncratic criteria. Such diversity in assumptions and approaches used by automated clickbait detectors raise important questions about the validity of such systems.\nIn this paper, we explore whether clickbait is indeed \"clickbaity\" by analyzing whether some characteristics of clickbait are more engaging than others and whether the diferences in engagement between clickbait and non-clickbait derived from computational models represent diferent assumptions of the detectors other than \"clickbaitiness\" of the headlines per se. For this purpose, we conducted two experimental studies involving human subjects and a computational study of scraped clickbait headlines that compared four clickbait classifers using a testing sample of real-world share data. In the following section, we explain why certain characteristics of headlines might be more engaging than others and why diferences in the basic assumptions of computational models might be problematic for its classifcation.", "n_publication_ref": 39, "n_figure_ref": 0}, {"heading": "COGNITIVE PROCESSING OF HEADLINES", "text": "The headline serves an important role in a news story because it helps orient readers to the information, summarizing its key ideas, and serving as an attention grabber [2,12]. As Dor [12] explains, a headline is \"a communicative device whose function is to produce the optimal level of afnity between the content of the story and the reader's context of interpretation, in order to render the story optimally relevant for the reader\" (p. 720). Rooted in traditional print journalism as essentially a tool for improving usability of newspapers, the headline has become a critically important feature of digital media because a click on it is directly related to revenue. While news organizations acknowledge the importance of a click, they work to accomplish that goal without compromising their journalistic standards [40]. This is not the case for other types of content that circulate online. Many unreliable websites (e.g., junk science and conspiracy theories) utilize strategies such as clickbait to lure audiences into reading their content, but end up not meeting users' expectations once they click on the headline [6].\nBut, how do clickbaits gain user attention? The way in which headlines are written or framed has powerful efects on how the story is perceived. By framing the headline in a particular manner, the author makes some aspects of the text more salient compared to others, in turn promoting a particular defnition, interpretation, or proposition [16]. Making particular pieces of information more noticeable and meaningful enhances the likelihood that the reader will perceive them, process them, store them in memory, and engage with them. For example, in Tannenbaum [41], the author presented three diferent groups of participants with the same article varying only in how the headline of a trial story was framed-guilty, innocent, or neutral. Participants who were assigned to the guilty condition were more likely to identify the person of the news article as guilty, while those in the innocent condition were more likely to think the person was innocent. Similarly, in Ecker and colleagues [15], participants exposed to misleading headlines in commentary articles were less likely to engage in inferential reasoning, compared to participants who were presented with headlines congruent with the article. The diferent efects in user perception and processing of information occurs because the headline is the frst item to be encoded into memory and it helps readers assign relevance. The headline provides signals to the reader such that \"the observer allocates cognitive resources toward certain environmental features because the communicator deems that information as more relevant to the observer than other information\" [27] (p. 131). As such, users will read the story with a particular schema derived from the headline [4,11,15]. This schema, in turn, facilitates retrieval of information related to the headline, thus biasing the manner in which the story is processed by readers.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Efects of Headlines in User Engagement with Content", "text": "Not only do headlines infuence how readers interpret the text, but even preceding that, it helps users decide if they should click and read the story in the frst place. Even though clicking behavior is often associated with interest, there are several additional predictors of this type of engagement, including cognitive, afective, and pragmatic reasons [22]. For example, Tenenboim and Cohen [43] reveal that while users click more often on sensationalist content, they comment more on public-afairs content. This is because sensationalist content arouses curiosity and clicking on it enables self-experience. On the other hand, commenting behavior helps construct group identity and allows for self-expression. This example reveals that even though users might be equally interested in sensationalist and public-afairs content, they express their interest through diferent engagement actions available on social media. Similarly, Kormelink and Meijer [22] found that users will sometimes fnd an article interesting, yet will not click on it. This occurs when the headline is informationally complete (the user feels there is nothing additional to learn from reading the story) or when there is an associative gap (the headline does not tell enough for the user to want to click on it). In other words, for users to be persuaded into clicking, there should be a perfect balance between providing enough information to raise curiosity and leaving the user wanting to know more about the topic. While clicking is an important metric directly associated with advertisement revenue, there are other important social media metrics to consider. One of them is the number of shares an article receives. The number of shares is an important metric for content creators because it refects a mechanism to increase future readership or clicks. Simply put, sharing contributes to the virality of content, in turn increasing the number of possible clicks. But, what motivates users to share one piece of online content over another? Reasons include information utility, opinion leadership, emotional impact, relevance, entertainment, and social cohesion [3,14,33,42]. Importantly, a large percentage of articles in social media are shared without being clicked upon, which means the users are persuaded to share by the headline alone. A large-scale Twitter analysis [17] revealed that nearly 60% of the shared URLs are never clicked upon. This means that the headline of the article on its own is sufcient to trigger the information utility, emotional impact, and other motivations needed for a user to share an article.\nContent creators are well aware of the importance of headlines in generating content engagement. As such, they rely on diferent strategies to persuade users into clicking. One strategy is the use of linguistic properties to persuade users by generating a \"curiosity gap\" [6,25]. The next section will expound on the psychology behind this strategy-also known as clickbait.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "The Psychology of Clickbait", "text": "As a tool of persuasion, clickbait employs linguistic strategies that take advantage of the curiosity gap [6,25], where the headline generates \"enough curiosity among the readers such that they become compelled to click on the link to fll the knowledge gap\" [6] (p. 1). This cognitive phenomenon, known as \"information gap\" [25], pertains to the curiosity triggered when a user's informational reference point is elevated beyond the users' current understanding or knowledge of a particular topic. When the user is alerted about such a gap, s/he will do the needful to close that gap, which in the case of clickbait headlines means reading the article to satiate the need to know. Importantly, however, attaining curiosity is a difcult task. For example, research reveals that clickbait headlines might raise curiosity or annoyance [2]. However, they are more likely to raise curiosity when they are perceived as creative, and thus are often preferred over merely informative headlines [19]. Ecker and colleagues [15] found that mismatched headlines infuence memory and comprehension, but only to the extent that users do not perceive that they are being deceived. Loewenstein's [25] \"information gap\" hypothesis, in fact, explains several precursors to achieve the curiosity gap, namely awareness of the information gap and previous knowledge about the topic. The author states that curiosity will be greater when information is perceived as likely to close that knowledge gap and when the piece is perceived as providing insight (or a quick solution) rather than incremental solutions. Given the several factors needed to arouse curiosity, it is not surprising that we see mixed fndings regarding the efects of clickbait headlines on actual user engagement [5,36,37,44]. This raises the question: What will it take for a headline to accomplish the ideal level of curiosity to persuade users to click on it or share it?\nIndustry and academia have explored this question and agree on several stylistic features that induce greater engagement with content [5,36]. Seven characteristics are recurrent: questions, lists, wh words (why, where, when, what), demonstrative adjectives (e.g., here, this, these), positive superlatives, (e.g., best, bigger), negative superlatives (e.g., worst, ever), and modals (e.g., should, would) (see Table 1 for defnitions). Nonetheless, despite agreement that these stylistic features are common among clickbait headlines, studies have found conficting results. One possibility for these mixed results is that some characteristics are more successful than others. In this paper, we explore which of these seven characteristics or stylistic features of clickbait are better at generating engagement, if indeed they are more efective than non-clickbait headline, and what are the psychological mechanisms that lead to higher engagement. More formally, we propose: RQ1: What is the relationship between the seven characteristics of clickbait and a) user clicking behavior (read more) and b) user sharing behavior? RQ2: What is the relationship between the seven characteristics of clickbait and a) perceived deception, b) perceived entertainment, c) perceived credibility, and c) curiosity arousal?", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "ASSUMPTIONS OF CLICKBAIT DETECTORS AND EFFECTS ON CLASSIFICATION", "text": "As explained in the previous section, to be considered clickbait, a headline should evoke interest while also leaving the user wanting to know more about the topic. Identifying which headlines meet this criterion is challenging, especially for a computational model because the concept must be defned with sufcient level of concreteness with features that represent \"clickbaitiness\" for machine distinction. As such, there are important decisions and assumptions that scholars must make when building computational models for the detection of clickbait. The frst decision to make is what data to use for training the algorithm and how to label it. While some scholars opt for human labeling of data by asking annotators to identify if a headline is clickbait or not (e.g., [6]), others are based on weak supervision techniques such as assuming that certain sources are more likely to produce clickbait headlines (e.g., [13,26]) and thereby labeling all headlines from a given source as clickbaits.\nStill others (e.g., [23]) use machine learning to generate synthetic given by researchers for labeling data), source assumptions assume that reliable news organizations (e.g., New York Times) do not use clickbait strategies, deliberately overlooking the fact that this is not always the case [36].\nAnother important decision to make when building automated clickbait detectors is the type of machine learning model to use. For example, some detectors are based on traditional machine learning such as Na\u00efve Bayes or Support Vector Machine (e.g. [13]), while others employ a more complex deep learning model (e.g. [36]). Each model has unique assumptions and characteristics that can afect the classifcation decision. Classical Na\u00efve Bayes with bag-of-words features, for example, does not fully take into account the sequential dependency between words. This means that each feature or word is independent, and there is no intermediate representation between word and output. Thus, for Na\u00efve Bayes models with bagof-words (excluding punctuation marks), the headline \"Want to Hear Biden and Harris Plans for Next Year?\" is the same as \"Biden and Harris Want to Hear Plans for Next Year. \" The model would not diferentiate between the two, and classify both in the same way (i. e., both clickbait or both non-clickbait). On the other hand, more sophisticated deep learning models learn representations or features of the input texts and use these features for prediction, and unlike in Na\u00efve Bayes (with bag-of-words), these features can be interpreted as interactions among the words in the sentence. Additionally, deep learning models can learn shortcuts that might generalize to some unseen text but are sometimes not very meaningful [18]. Thus, for deep learning models, the aforementioned headlines might not be equivalent. In other words, the model could classify the frst as clickbait and the second as non-clickbait (or the other way around). While a Na\u00efve Bayes classifer can learn the dependency among words located near each other by using n-gram features with n>1 or by converting the headline using word2vec instead of bag-ofwords, this still does not consider the relative position of a word or phrase in a sentence. A deep learning architecture such as recurrent neural network (RNN) [8] or BERT [10] takes into account both word dependency and its relative position in a sentence.\nThe diferent characteristics of clickbait detectors (e.g., how data were labeled for training: human coding vs. source assumption vs. machine generation) or the type of machine learning model employed (Na\u00efve Bayes, Support Vector Machine, Deep Learning), represent another possible explanation for the mixed results we see in the literature, and may explain why computationally based studies reveal greater engagement with clickbait (vs. non-clickbait headlines), while experimental studies do not show this pattern. It is possible that the diferent assumptions of each detector might have resulted in systems that, although accurate, classify clickbait using their unique conceptualization and operationalization of clickbait. If this is the case, when comparing the classifcation of diferent clickbait detectors on the same headlines, the agreement between the detectors would be low. On the other hand, if indeed the classifcation systems are all classifying the same concept, the agreement among the classifers should be high. We test this question further in this paper. More formally:\nRQ3: What is the relationship between clickbait detectors varying in the labeling of data used for training (human annotated data vs. weak supervision with source assumptions) and type of model (traditional machine learning vs. deep learning) and their level of agreement with headline classifcation?\nIn summary, we aim to investigate if clickbait headlines are actually more engaging than non-clickbait headlines by testing possible reasons behind the mixed fndings in the literature. Specifcally, we test two possibilities: 1) that some clickbait characteristics generate more curiosity than others and thus diferential engagement levels and 2) that diferences in engagement between clickbait and non-clickbait derived from computational models represent other variables such as topic distinctions and assumptions of the system rather than \"clickbaitiness.\" We explore these questions through two experiments and a computational analysis of real-world sharing of news headlines. In the frst experiment, we scraped a series of headlines from reliable sources (the top circulated print media and most watched broadcast media according to Nielsen rating) and unreliable online sources (junk science, conspiracy, and satire sites) [36] and passed them through three automated clickbait detectors-two deep learning models and a traditional machine learning model (Na\u00efve Bayes). Then, we selected headlines classifed as clickbait by all three detectors and that possessed only one of the seven clickbait characteristics (or none). We presented these headlines to participants and asked their likelihood of sharing them with their network and/or clicking the headline to read the article further. In Study 2, we conducted a similar experiment, but to control for potential content efects, we randomly assigned participants to one of eight headlines for the same news story, varying only the clickbait (or not clickbait) characteristic utilized. Participants were then directed to read the article and asked their perceptions of both the headline and the associated article. Finally, in Study 3, we analyzed real-world sharing behavior of a series of headlines classifed as clickbait by four classifers (we used the same three classifers as Study 1 and added one more classifer, resulting in a total of two deep learning models and two traditional machine learning models) and containing one of the seven clickbait characteristics. Methods and results of each study are explained in the following sections.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "STUDY 1 METHOD", "text": "For study 1, we conducted an 8 (Characteristic: 7 Clickbait Characteristics + 1 Non-Clickbait) x 2 (Type of Content: Political vs. Non-Political) mixed method quasi-experiment. The 8 characteristics were a within-subject factor and the type of content was a between-subject factor. We decided on a within-subject factor for the clickbait characteristic because it automatically controls for individual diferences among participants and represents a stronger test of user preference. Furthermore, we included political and nonpolitical headlines as part of our study design because research suggests a diference in engagement between these two types of content, with non-political headlines receiving higher engagement than political headlines [22]. Including both in our manipulation allows us to account for the possibility that clickbait might be more successful for non-political content compared to political content. We gathered headlines for this study by scraping them from reliable and unreliable online sources, as defned by Rony and colleagues [36], and coded them computationally using their model to determine the clickbait characteristic they possess. Then, we passed the headlines through three diferent high-accuracy clickbait detectors that identifed whether a headline is clickbait or not. We passed the headlines through three classifers to increase the robustness of our study. Classifer 1 (93% accuracy) is a deep learning model trained on 32,000 headlines derived from news organizations and coded by three volunteer coders 1 [36]. Classifer 2 (93% accuracy) is a traditional machine learning algorithm (Na\u00efve Bayes), with headlines labeled based on source assumptions 2 [13]. Classifer 3 (90% accuracy) is also a deep learning model, but it is based on a 12,000-headline dataset labeled based on credibility of the source 3 [26]. As a fnal step, we chose two headlines per clickbait characteristic for stimulus sampling purposes. To be selected, a headline had to 1) be identifed as clickbait by the three detectors and 2) use only one of the clickbait characteristics (otherwise we manually modifed) (See Table 2 for fnal list of headlines).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Participants", "text": "Participants for this study were recruited from Amazon Mechanical Turk (N=150). The fnal sample consisted of 149, after deleting one for incomplete data. An a priori power analysis revealed that in order to detect a medium-size efect (.25), with an error of .05 and a power of .80, a sample of 114 participants was needed. Participants' age ranged from 20 to 75 (M= 38.31, SD= 12.84), and 50.3% selfidentifed as female. Participants were predominantly Caucasian (77.2%) and highly educated, with 39.3% having a bachelor's degree and 15.4% a master's degree or higher.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Procedure", "text": "After acknowledging consent, participants were told that we are conducting a study with the purpose of identifying people's preference for diferent types of headlines. During the study, they were presented with a series of headlines through an interaction (See Figure 1). For each headline, participants could click on \"read more\" if they would be likely to click on the headline and \"share\" if they would be likely to share it with their network of friends. They could select one of the options, both, or none. After reading the instructions, participants were provided with a test run so they could have a feel for the interaction before starting the actual study. Participants were randomly assigned to either the political or the non-political condition. Then, they received 8 headlines (one for each clickbait characteristic and non-clickbait) randomized in order of presentation. Each headline was presented as in Figure 1, allowing participants to interact with the headline as they would during their normal course of browsing. Upon interacting with each headline, participants were asked questions about their perceptions of the headline. After participants went through all 8 assigned headlines, they were asked demographic questions and questions about their political orientation.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Measures", "text": "4.3.1 \"Read More\" and \"Sharing\" Intention. To measure participants' likelihood of reading the article associated with the headline and sharing the headline, we created an interaction (See Figure 1) such that participants were provided with the headline and two buttons. Participants were instructed to click on \"read more\" if they would be likely to click on the headline and read the article associated with it (the article was however not displayed to participants). Similarly, participants could click on \"share\" if they would be likely to share the headline with their network of friends on social media. This interaction resulted in two variables, one indicating if the participant clicked on \"read more\" or not, coded as 1=clicked or 0=did not click, and the other indicating if the participant clicked on \"share\" or not, coded as 1= shared or 0 = did not share.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Headline Perception.", "text": "To measure headline perception, after interacting with each headline, we asked participants to indicate how well each word from a list of 23 adjectives describes the headline they just read. Questions were asked on a 1-7 scale. Items were adapted from Sundar's [39] news content perception scale, Ormond et al. 's [32] and Dark and Ritchie's [9] deception scales, and Naylor's curiosity scale [31]. The authors additionally added items to refect the possible entertainment value of headlines. An exploratory factor analysis using Oblimin rotation revealed four factors: credible (e.g., accurate, believable, well-written, persuasive; M= 3.59, SD= 1.51, \u03b1 = .91), deceptive (e.g., dishonest, deceptive, fake, tricky; M= 3.39, SD= 1.41, \u03b1 = .84), entertaining (e.g., humorous, enjoyable, entertaining; M= 3.24, SD= 1.70, \u03b1 = .87), and curiosity arousing (e.g., want to know more, boring (r), intriguing; M= 3.91, SD=1.78 \u03b1 = .85).", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Political Orientation.", "text": "We measured participants' political orientation through four questions proposed by Janof-Bulman and colleagues [20]. Items included, \"Where would you place yourself on a scale from 1 (Strong Democrat) to 7 (Strong Republican)\", and \"Where would you place yourself on a scale from 1 (Very Liberal) to 7 (Very Conservative)\" (M= 3.63, SD= 1.59, \u03b1 = .85).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "STUDY 1 RESULTS", "text": "First, we calculated the distribution of clicks on the \"read more\" and \"share\" buttons by characteristic to get a general idea of the relative success of each characteristic at engaging users. Table 3 reveals that participants clicked on \"read more\" and \"share\" more often for nonclickbait headlines (compared to the seven clickbait characteristics). To test if these diferences were statistically signifcant, we ran two logistic regressions, one using participants' response on \"read more\" button as the dependent variable and the other using participants' response on \"share\" button as the dependent variable. For both regressions, participants' political orientation was entered frst as a control variable, followed by the type of content (political vs. non-political) and headline characteristic as independent variables. The fnal step included an interaction term between the type of content and headline characteristic.\nWhen we entered \"read more\" as the dependent variable, data revealed that headline characteristic was a signifcant predictor of clicking behavior (Wald \u03c7 2 = 18.06, p = .01). Post-hoc pairwise comparison based on odds ratio revealed that the odds of non-clickbait Figure 1: Training interaction provided to participants. As in the training, for the real study, participants could click \"read more,\" \"share,\" both options, or none of them, for each of the eight headlines presented. ). On the other hand, nonclickbait headlines were as successful as demonstrative adjectives, lists, and \"wh\" headlines. This means that non-clickbait headlines performed better than four of the seven clickbait characteristics, but as good as the other three. Furthermore, when comparing between clickbait characteristics, the odds of the \"wh\" headlines receiving a click was 1.70 times higher than modals (p= .03); the odds of listicles receiving a click was 1.83 times higher than modals (p= .01); the odds of listicles receiving a click was 1.63 times greater than questions (p= .04). Results revealed no signifcant main efect of type of content and no interaction efect. When entering \"share\" as the dependent variable, results revealed no signifcant main efects and no signifcant interaction efect. This indicated that all headlines were as likely to be shared regardless of linguistic characteristic or type of content.\nTo test if headlines were perceived diferently by participants as a function of the clickbait characteristics and type of content, we ran a series of 8 (7 Clickbait Characteristics + 1 Non-Clickbait) x 2 (Type of Content: Political vs. Non-Political) repeated-measures analyses of variance using a mixed model approach.\nWhen entering curiosity as the dependent variable, results revealed a signifcant main efect of characteristic, F (7, 1029) = 5.84, p <.0001. Tukey HSD post-hoc comparisons (see Table 4) revealed that non-clickbait headlines elicited more curiosity than all clickbait headlines (the diference is statistically signifcant for all comparisons except for the comparison with demonstrative adjectives and lists). This fnding runs counter to the belief that clickbait headlines induce more curiosity, and thus receive more user engagement.\nThe main efect should be interpreted in light of the interaction efect between characteristic and type of content, F (7, 1029) = 3.17, p =.003. Patterns of the interaction (See Figure 2) reveal that while non-political content written using \"wh\" words and modals were perceived as less curiosity-arousing compared to other clickbait characteristics and non-clickbait, when political content is written using these characteristics, the headline arouses a higher level of curiosity, reaching similar numbers as those of non-clickbait headlines.\nWith credibility as the dependent variable, data revealed a signifcant efect of characteristic, F (7, 1029) = 7.47, p <.0001. Post-hoc comparisons using Tukey HSD diference test are reported in Table 4 and reveal that overall non-clickbait headlines were perceived as more credible than clickbait (the diference was statistically signifcant for all comparisons except for the comparison with demonstrative adjectives and questions). This Note: Vertical means with no lower-case subscript in common difer at p< .05 using Tukey HSD post-hoc comparisons. efect should be interpreted in light of a signifcant interaction SD = 0.12) than non-political headlines (M= 2.87, SD = 0.11), F (1, efect (See Figure 3) between characteristic and type of content, F 147) = 19.52, p <.0001. Results also revealed a signifcant efect of (7, 1029) = 3.75, p <.001, revealing that for all characteristics except characteristic F (7, 1029) = 6.45, p <.0001. Post-hoc pairwise comfor modals, questions and wh, non-political content was perceived parisons using Tukey HSD test (Table 4) indicate that lists, negative as more credible than political content. Importantly, looking at superlatives, and \"wh\" headlines were perceived as more deceitful the interaction efect, non-clickbait content (both political and compared to the other characteristics. Non-clickbait was perceived non-political) were perceived among the most credible headlines.\nas the least deceitful, signifcantly lower than lists and negative When entering perceived deception as the dependent variable, superlatives. Nonetheless, the main efects should be interpreted data revealed a signifcant main efect for type of content such based on a signifcant interaction efect between headline characthat political headlines were perceived as more deceitful (M= 3.62, teristic and type of content, F (7, 1029) = 9.76, p <.0001. The trend of the interaction (See Figure 4) reveals that political content tends to be perceived as more deceitful compared to non-political content, except for headlines using questions. In this case, not only is there no statistically signifcant diference between political and non-political headlines, but political headlines with questions were perceived as less deceptive than political headlines using positive superlatives, lists, and negative superlatives.\nFinally, when we entered entertaining as the dependent variable, data revealed a signifcant main efect of characteristic, F (7, 1029) = 2.99, p = .004. Pairwise post-hoc comparisons using Tukey HSD (See Table 4) revealed that all headlines were perceived as equally entertaining, except for question-based clickbait that ranked least entertaining compared to lists and negative superlatives. Data also showed an interaction efect between characteristic and type of content, F (7, 1029) = 3.76, p < .001 (see Figure 5), such that lists and negative superlatives were perceived as more entertaining in non-political headlines, compared to political headlines. However, political headlines using demonstrative adjectives were perceived as more entertaining than non-political headlines using demonstrative adjectives.", "n_publication_ref": 0, "n_figure_ref": 5}, {"heading": "STUDY 1 DISCUSSION", "text": "In answering RQ1, results of Study 1 reveal that indeed some clickbait characteristics result in more clicks (to read more) than others. Specifcally, headlines using \"wh\" word and using lists received more clicks than those using modals, and headlines using lists received more clicks than those using questions. Despite these differences among clickbait headlines, non-clickbait received more clicks overall than four of the seven clickbait characteristics and performed the same as the remaining three. Furthermore, there were no statistically signifcant diferences in sharing behavior as a function of characteristics. These results indicate that despite the common understanding that clickbait headlines generate more clicks than non-clickbait, this is not the case. Users are as likely (and sometimes more likely) to engage with non-clickbait headlines. The overall preference for non-clickbait headlines can be explained by how users perceived them. For starters, headlines using demonstrative adjectives and lists aroused more curiosity than the other clickbait characteristics. However, non-clickbait was similarly arousing as these two characteristics, and was more arousing than the remining fve clickbait characteristics. This runs counter to the proposition that clickbait headlines will be more arousing than non-clickbait headlines by inducing a curiosity gap [1,6]. Likewise, non-clickbait headlines were perceived by participants as equally entertaining as clickbait headlines. As expected, non-clickbait was also perceived as less deceitful and more credible than clickbait headlines. Interestingly, demonstrative adjectives and questions were perceived as equally credible. It is also important to note the interaction efects between clickbait characteristics and type of content suggesting that some clickbait characteristics might be perceived diferently depending on whether the content is political. Nonetheless, despite these slight variations, the interaction efects still revealed that non-clickbait induce about the same (if not more) curiosity than clickbait, are perceived as equally (if not more) credible and are perceived as equally (or less) deceitful.\nResults of Study 1 provide initial evidence that clickbait headlines might not be as successful as we think. Nonetheless, there are important limitations of this study to consider. First, there are content characteristics that are not accounted for in this study. We utilized headlines scraped from online sources in the interest of ecological validity, but it resulted in headlines pertaining to diferent topics and associated with diferent articles (See Table 2), thus creating  For Study 2, we chose one of the political non-clickbait headlines \"read more, \" which might have reduced participants' desire to click. utilized in Study 1 (Secret Deal Allowing Iran To Expand Nuke Furthermore, our sample consisted of M-Turkers, who might have Program) and systematically changed it to contain one of the seven higher levels of digital literacy or might be more motivated to clickbait characteristics. We chose a political headline because the systematically think about their engagement with headlines (at use of clickbait in this area is of particular concern. Many argue that least in a study context) than the regular user. To account for such the use of clickbait media results in the \"dumbing down of news\" limitations, we conducted Study 2, which is described next.  [22]. The clickbait headlines were written by a former journalist and resulted in a total of eight headlines (7 clickbait + 1 non-clickbait) (See Table 5 for exact headlines).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Participants", "text": "Participants for Study 2 (N=249) consisted of 89 students recruited from communication and information science and technology courses at two US universities (one located in the Northeast and another in the South), as well as 160 participants recruited from Amazon Mechanical Turk. An a priori power analysis revealed that to detect a medium-size efect (.25), with an error of .05 and a power of .80, a sample of 240 participants would be needed. We recruited students and M-Turk participants to account for the possibility that the superior engagement of non-clickbait headlines (vs. clickbait) in Study 1 could be due to the higher digital literacy of M-Turkers. Of our total sample, 61.8% self-identifed as male, 36.9% as female, 1.2% other, and .1% did not report. Our sample was predominantly white (65.5%) and their ages ranged between 18 and 68 years (M= 29.89, SD= 11.42).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Procedure", "text": "Upon consenting to participate in this study, participants were directed to the same instructions and training interaction as in Study 1 (see Figure 1). They were then randomly assigned to one of the 8 conditions (7 clickbait headlines + 1 non-clickbait headline). Once they received their assigned headline, they could click \"read more,\" and/or \"share\" as in Study 1. Upon completing the interaction, participants were redirected to a questionnaire asking about their perceptions of the headline. Participants who clicked on \"read more\" received a prompt saying that before we take them to the story, we would like to ask their quick impressions of the headline they just read, while those who did not click on \"read more\" received a prompt simply stating that we will now ask them about their quick impression of the headline. The diferent prompts were included to assure those that clicked \"read more\" that they will indeed see the story afterwards. The fact that participants received the actual text when clicking \"read more\" addresses the possibility that in Study 1 users did not click \"read more\" for the clickbait headlines because they knew that they would not get to read the story anyway. After completing the questionnaire eliciting their perceptions of the headline, all participants were taken to the story associated with the headline. The story remained constant across conditions, but the headline varied depending on the condition (see Figure 6). After reading the story, participants were directed to a questionnaire asking about their perceptions of the news story, their likelihood of sharing the story with their network of friends, and their elaboration of the content of the story. At the end of the questionnaire, they were asked about their demographics and political orientation.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Measures", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Headline Reading and Sharing Intention.", "text": "To measure participants' likelihood of reading the article associated with the headline and sharing the headline, we followed the same procedure as in Study 1 and created two variables, one indicating if the participant clicked on \"read more\" or not, coded as 1=clicked or 0=did not click, and the other indicating if the participant clicked on \"share\" or not, coded as 1= shared or 0 = did not share.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Headline Perceptions.", "text": "To measure headline perceptions, we utilized a similar scale as in Study 1 and asked participants to indicate how well each word in a list of 24 adjectives refected the headline they just read. The fnal scales were constructed based on an exploratory factor analysis using Oblimin Rotation. We constrained the EFA to four factors to imitate the scales utilized in Study 1 for comparability purposes.   message elaboration [35]. The scale asked participants the degree to which they engaged in a series of behaviors while reading the message, on a 1-7 scale, such as: \"attempting to analyze the issues in the message, unconcerned with the ideas(r), and expending a great deal of cognitive efort\" (M= 4.71, SD= 1.06, \u03b1 = .86).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "STUDY 2 RESULTS", "text": "To assess the efects of the headline characteristics on user engagement with the headline (clicking on \"read more\" and \"share\"), we frst explored descriptive statistics. Figure 7 reveals that, overall, users clicked \"read more\" on non-clickbait at a higher rate, followed by \"wh\" words. When looking at sharing behavior, on the other hand, there is a relatively low number of shares overall. Questions and demonstrative-adjective headlines seem to be shared relatively less than the other characteristics.\nTo test if these diferences in user engagement are statistically signifcant, we conducted two logistic regressions, one for users' click on \"read more\" and one for users' click on \"share. \" We entered participant type (student or M-Turker), political orientation, gender, and age as control variables. Results revealed no signifcant efect of characteristic when entering \"read more\" as the dependent variable (Wald \u03c7 2 = 4.42, p = .73), nor when entering \"share\" as the dependent variable (Wald \u03c7 2 = 2.91, p = .89). This means that user engagement was not diferent across clickbait and non-clickbait headlines.\nTo assess if participants perceived the headlines any diferently, we conducted a series of one-way ANOVAs for each dependent variable of interest. For all analyses, we entered participant type, political orientation, gender, and age as control variables. Results revealed no signifcant diference for any of the four dependent variables of interest: credibility (F (7, 236) = 1.37, p =.22), deceptive (F (7, 236) = 1.14, p =.34), curiosity inducing (F (7, 236) = 1.22, p =.30), entertaining (F (7, 236) = 0.50, p =.84).\nFollowing analyses of the headline, we examined if there were any diferences in how participants perceived the news story, their intention to share the story, and their elaboration as a function of headline characteristic. For this purpose, we ran a series of one-way ANOVAs. Results revealed no statistically signifcant diference on perceived story credibility (F (7, 236) = 088, p =.52), representativeness (F (7, 236) = 0.83, p =.57), quality (F (7, 236) = 0.94, p =.48), liking (F (7, 236) = 1.16, p =.33), users' elaboration of the story (F (7, 235) = 1.66, p =.12), or likelihood of sharing the story (F (7, 236) = 0.24, p =.98).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "STUDY 2 DISCUSSION", "text": "In Study 2, we controlled for the topic of the headlines and included students and M-Turkers in our sample to account for content confounds of Study 1, yet we found no signifcant diferences between clickbait and non-clickbait headlines in terms of likelihood of clicking and sharing. If anything, users clicked more on non-clickbait headlines, although not by a statistically signifcant margin.\nAnalyses also revealed that participants perceived the headline the same regardless of its characteristics. Again, these results run counter to past research suggesting that headline style infuences user perception of the headline and its associated content [15,41]. While data suggest that the headline selected for Study 2 elicited rather high curiosity (M= 4.54), meaning that participants found the article to be interesting, we should note that this study used only one headline. More testing is required for enhancing the external validity of our fndings.\nNonetheless, the null fndings of Study 2 suggests three possibilities, 1) clickbait is not as clickbaity as we think, 2) the \"clickbaitiness\" of clickbait headlines is not solely determined by the characteristics suggested by industry [5] and identifed by the clickbait detector in Rony et al. [36], but by other factors that help create a psychological information gap, or 3) the higher engagement of clickbait headlines (compared to non-clickbait) found in computational analysis [36,44] might not represent engagement with clickbait per se, but third variables (or common-cause variables) attributable to the assumptions of each clickbait detector (e.g., labeling procedure for training data, type of machine learning model). If this is the case, then each detector is operating with a unique and distinct conceptualization of clickbait, which calls into question the validity of clickbait detectors. If the same concept is being captured by the different models, then they should have high agreement in classifying the same set of clickbait headlines. If they do not have high agreement, it would represent a validity issue of content classifcation systems for clickbait detection, with results being confounded with other variables such as topic distinction and system assumptions. We explore these issues in Study 3 through computational analysis of real-world number of shares of headlines scraped from reliable and unreliable sources (political and non-political). This time, we passed the headlines through four diferent clickbait classifers (the same three as in Study 1 plus a new traditional machine-learning classifer) varying in their basic assumptions of clickbait and in the type of machine-learning utilized. We added a fourth classifer in Study 3 in order to have a fully crossed factorial design that can assess diferences in engagement based on the characteristics of the classifers: 2 (Type of Model: Conventional Machine Learning vs. Deep Learning) x 2 (Data: Annotated Data vs. Source Assumptions).", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "STUDY 3 METHOD", "text": "For Study 3, we scraped headlines from reliable and unreliable sources (political and non-political) and computationally identifed the clickbait characteristics utilized by them (see supplemental material for complete list of headlines). Then, we passed the headlines through four diferent clickbait detectors. The frst three detectors were the same as in Study 1. We added a fourth detector -a support vector machine learning model trained on manually annotated data (96% accuracy). This addition resulted in a 2 (Type of Model: Traditional Machine Learning vs. Deep Learning) x 2 (Data: Annotated Data vs. Weak Supervision with Source Assumptions) comparison allowing us to assess diferences in engagement based on the characteristics of the classifers. As a fnal step, we linked the scraped headlines to actual share data retrieved from sharedcount.com. Data from sharedcount.com includes total number of Facebook shares, Facebook comments, Facebook reactions, and number of pins. In total, we ended up with data for 371 headlines after deleting errors and headlines that used other characteristics aside from the seven analyzed in this paper.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "STUDY 3 RESULTS", "text": "First, we analyzed descriptive statistics to assess the classifcation agreement between the four clickbait detectors. Data reveal that the four classifers agreed on the classifcation 47.17% of the times. Of the175 headlines that were classifed similarly by the four classifers, 139 were clickbait classifcations and 36 were non-clickbait. Furthermore, as Figure 8 shows, the level of agreement with the classifcation also varied based on the characteristic used by the headline. For instance, while the four classifers agreed on the clickbait classifcation more times for the negative superlative characteristic (compared to the other six characteristics), the four classifers never agreed on a non-clickbait classifcation for the negative superlative or question characteristics, as illustrated by the absence of the \"non-clickbait agreement\" bar.\nWe then proceeded to compare classifers pairwise; specifcally, the percentage of times that two classifers agreed on either a clickbait or a non-clickbait classifcation. Table 6 reveals that the highest agreement was between detector 1 and 4, both of which used manually annotated data for training.\nTo analyze whether one clickbait characteristic received more engagement than another, as proposed by RQ1, we ran a series of negative binomial regressions with maximum likelihood estimation. This was the most appropriate analysis given overdispersion of the count data. Given the rather low agreement between detectors explained above, we ran a separate analysis for each detector so that we could assess 1) if there is a feature that is more successful at engaging users across all classifers, and 2) if the type of model (classical machine learning vs. deep learning) and the training dataset (annotated data set vs. weak supervision with source assumption) yield diferent results 4 . For all analyses, we entered the type of content (political vs. non-political) and the headline features as the  Note: Percentages of total headlines agreed as non-clickbait (or clickbait) refer to the percentage of the total number of headlines where the two classifers agreed on the non-clickbait or clickbait determination.\nindependent variables, with total engagement (combined number of shares, comments, reactions and pins) as the dependent variable. When analyzing Detector 1 (Deep Learning, Manually Annotated Data), we found a signifcant efect of types of content, such that non-political headlines received more engagement than political headlines, b = 0.55, Wald \u03c7 2 = 4.64, p = .03. There was also a signifcant efect of features, Wald \u03c7 2 =42.59, p < .0001. Post-hoc comparisons using Tukey HSD revealed that demonstrative adjectives, lists, modals, and \"wh\" words received higher engagement than non-clickbait headlines.\nWhen analyzing Detector 2 (Traditional Machine Learning: Na\u00efve Bayes, Weak Supervision), we also found a signifcant effect of features, Wald \u03c7 2 =31.72, p < .0001. Post-hoc comparisons revealed that demonstrative adjectives, lists, and \"wh\" words performed better than non-clickbait headlines. Analysis with Detector 2 additionally yielded signifcant pairwise diferences between clickbait headlines, such that demonstrative adjectives, lists and \"wh\" words received more engagement than positive superlatives.\nWhen analyzing Detector 3 (Deep Learning, Weak Supervision), there was also a signifcant efect of feature, Wald \u03c7 2 =31.99, p < .0001. Again, post-hoc comparisons revealed that demonstrative adjectives, lists, and \"wh\" words performed better than non-clickbait. Data also revealed that demonstrative adjectives and \"wh\" words received more engagement than positive superlatives.\nFinally, analysis of Detector 4 (Traditional Machine Learning: Support Vector Machine, Manually Annotated Data) yielded a signifcant efect of feature, Wald \u03c7 2 =23.15, p = .002. Tukey HSD pairwise comparison revealed that the diference is only between demonstrative adjectives and positive superlatives, with demonstrative adjectives receiving higher engagement.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "STUDY 3 DISCUSSION", "text": "Results of Study 3 are intriguing as it reveals rather low agreement among the four clickbait detectors, despite the detectors individually having high accuracy. This is likely due to the diferent assumptions and characteristics of each detector. For instance, two of the models were trained using annotated data while the other two used source assumptions as ground truth. This means that in the former, clickbait is defned in terms of the perceptions of the volunteers who classifed those headlines and their understanding of what is and what is not clickbait. On the other hand, the other two models relied on source credibility to train the model, such that headlines from sources like the Wall Street Journal are understood as being non-clickbait, while those from outlets like Buzzfeed are assumed to be clickbait. Two of the models utilized a deep learning model, while the other two utilized more classical machine-learning models (Na\u00efve Bayes and Support Vector Machine). Results of Study 3 reveal that the diferences in assumptions made by each model results in a low agreement when comparing the four detectors together and generate diferent results in terms of user engagement. It is worth noting, however, that when examining the pairwise comparison among classifers, the highest total agreement was between the two human-labeled classifers-classifers 1 and 4 (see Table 6).\nRegardless of the low agreement between the clickbait detectors, three out of the four models consistently revealed that clickbait headlines using demonstrative adjectives, lists and \"wh\" words resulted in higher engagement compared to non-clickbait headlines. This means that at least in an uncontrolled environment, these characteristics are more successful in luring users into clicking. Similarly, two of the four models reveal that positive superlatives are not as good as other clickbait characteristics (e.g., demonstrative adjectives) at engaging users. These diferences notwithstanding, it is important to note that some of the headlines classifed as nonclickbait by classifers still contained one of the seven characteristics of clickbait analyzed in this paper (see supplemental material for a list of headlines). This means that it is not the characteristics alone that distinguish a headline as clickbait or not clickbait. It is likely that there are other underlying linguistic properties that may elicit information gap more efectively when read in tandem with a particular characteristic.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "SUMMARY OF FINDINGS", "text": "In sum, we conducted three studies to assess if clickbait headlines are actually \"clickbaity\" (see Table 7 for a comparison of fndings).\nIn Study 1, we found that among the seven clickbait characteristics, users were more likely to click on \"read more\" for \"wh\" words and lists compared to modals. They were more likely to click on \"read more\" for lists compared to questions. However, results revealed an overall preference for non-clickbait. In fact, non-clickbait received signifcantly more clicks than modals, negative superlatives, positive superlatives, and questions, and the same as demonstrative adjectives, lists, and \"wh\" words. Moreover, when assessing user perception of the headlines, non-clickbait headlines elicited more curiosity than all 7 types of clickbait headlines, were perceived as less deceitful and considered more credible. The interaction effects between clickbait characteristic and type of content on user perceptions suggest that the success of clickbait is also contingent on the type of content of the headline. For example, non-political headlines were perceived as more credible than political headlines, except for modals, questions and \"wh\" headlines. Similarly, political headlines were generally perceived as more deceitful than nonpolitical headlines, except for headlines using questions, in which case non-political headlines were perceived as more deceitful.\nIn Study 2, we assessed user preference in a more controlled environment to account for the possibility that the general preference for non-clickbait found in Study 1 could be due to content diferences across the headlines. However, even when we systematically varied the same headline to possess one of the seven characteristics or non-clickbait, we found no statistically signifcant diferences in engagement between them. This means that users were as likely to click on \"read more\" and share for any of the clickbait characteristics or non-clickbait, and their perceptions of the headline (deception, curiosity, entertaining, credible) and news story (credibility, representativeness, quality, liking) were the same regardless of headline type.\nThe fndings of Study 1 and 2 suggested two possibilities for the positive efects of clickbait on engagement found in previous computational studies [36,44]. First, that in these studies clickbait headlines are not \"clickbaity\" because of the characteristics suggested by industry [5] and identifed by the clickbait detector in [36] alone, but by other factors that help create a psychological knowledge gap. Or, that the preference for clickbait is a function of third variables derived from the assumptions of the clickbait detectors. We explored these possibilities in Study 3 by comparing the classifcation of 4 diferent clickbait detectors varying in the nature of training data and the type of machine learning employed. While three of the four classifers suggest that users engage more with headlines using demonstrative adjectives, \"wh\" words, and lists, we found low overall agreement among the 4 classifers, such that the four of them agreed on a clickbait vs. non-clickbait classifcation only 47% of the time. We discuss the implications of our three studies in the next section.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "GENERAL DISCUSSION", "text": "Overall, our fndings suggest that determining whether a headline is clickbait or not is quite complex. It does not simply depend on a few key linguistic characteristics like using a listicle or framing it as a question. It also depends on the nature of the automated classifer used to distinguish clickbaits from non-clickbaits. In fact, results of our study expose the unreliability of clickbait classifers by showing low agreement between them. Like our Study 1 and 2, past studies [29,37] conducted using experimental designs and content analysis fnd that non-clickbait is more engaging than clickbait. Studies using automatic detection [36] reveal the opposite pattern, as did our Study 3. But, in analyzing each detector in isolation, we fnd that the four models only agreed 47% of the time, which indicates the complexity of defning and classifying clickbait computationally. The noise associated with this low level of agreement raises fundamental questions about the validity of clickbait detectors and taxonomies touting objective characteristics that result from clickbait analysis. In reality, clickbait determination could be highly subjective. As the author of one of the clickbait detectors used in this study [13] states \"I know it when I see it.\" However, classifying clickbait might be a difcult task precisely for that reason-we know it when we see it, but we cannot quite defne it operationally. It is possible that clickbait, and more precisely the curiosity gap that it is supposed to generate, represents an abstract concept difcult to defne at the granularity needed for computational detection. The higher engagement of clickbait over non-clickbait found in computational studies might be due to other variables such as topic distinctions and assumptions of the system rather than \"clickbaitiness.\" It is also possible that the results of our study represent the current digital user, one who no longer falls for clickbait headlines due to either their considerable prior experience with this type of headline (including the disappointment and frustration they may have felt) or due to the increasing number of media literacy campaigns in recent years aimed to educating the public. Culturally, we may have reached an infection point where we have come to recognize clickbaits for what they really are and therefore deliberately avoid clicking on them. That said, three classifers agreed that demonstrative adjectives, \"wh\" words and listicles performed better than non-clickbait when using real-world engagement data. These stylistic markers might be essential for clickbait headlines to be \"clickbaity, \" and are worthy of further testing. However, it is worth noting that we did not fnd the same efects in Study 1 and 2. This is important because, in Study 1, we selected 1) clickbait headlines that possessed only one of the seven characteristics, and 2) non-clickbait headlines that did not contain any of the characteristics (or manually adjusted to ensure that this was the case), whereas in Study 2, we further controlled for any possible content diferences by manipulating the same headline to possess one and only one or none of the characteristics. This was not the case for Study 3, where even headlines considered as non-clickbait by the four classifers appeared to have at least traces of some of the linguistic characteristics associated with clickbait headlines. For example, the headlines, \"Greatest Military Coup Ever Could Now Be Underway on U.S. Soil, \" and \"Media Silent as Biggest Protests Since French Revolution Sweep France,\" both use positive superlative words, yet were classifed as non-clickbait by the four classifers. This raises the question whether the characteristics themselves determine the 'clickbaitiness' of a headline or are there other linguistic mechanisms that promote user engagement? Furthermore, in Study 1 and 2, we assessed users' perception of the headline and found a null efect (Study 2) and a slight superiority of non-clickbait headlines (Study 1). As data scientists are well aware [5], analysis of real-world engagement can be rife with confounding variables. For example, it is possible that one type of clickbait headline, more than other types, tends to be placed on the upper part of the news site, generating more clicks partly because of its placement rather than the headline alone. This type of confound could lead to a form of Simpson's Paradox [5], where the direction of an association may be reversed when analyzing the subgroups of that population [21]. This is what we may be witnessing in Study 3. The many third variables associated with real-world engagement data coupled with the low reliability of the detectors found in our study sheds light on the possibility that the greater engagement of clickbait reported in computational studies should be interpreted with caution because the engagement might not be due to actual \"clickbaitiness\" or the knowledge gap hypothesis, but rather due to contextual factors and interface elements unrelated to clickbait.\nOur fndings hold several practical implications. First, our study echoes recent works by other scholars [37] and suggests that news organizations should steer away from utilizing clickbait characteristics in their headline writing, especially for topics that rely on users' perceptions of integrity and objectivity. For starters, headlines written as non-clickbait receive the same or equal engagement compared to clickbait headlines. Moreover, users perceive non-clickbait as more credible and less deceptive. They are also able to induce user curiosity to a higher extent than clickbait, probably because they are processed more deeply by users. All this suggests that respectable news organizations should refrain from resorting to clickbaits to boost engagement with their stories, as it is not only inefective in attracting users but also detrimental to their credibility. In other words, resorting to clickbait is not worth the credibility risk, especially in the news domain where trust in news organizations has already been on a decline. A recent survey of U.S. adults indicated that credibility of news organizations declined in 2020 compared to previous years, \"indicating not only a lack of trust but suggesting that audiences have grown cautious when consuming news\" [45] (para. 1). One way to boost credibility and diferentiate from other types of content online is for news organizations to stick to non-clickbait headlines when presenting content to their audiences.\nOur fndings also raise fundamental doubts about the reliability of computational clickbait detection solutions. Particularly, the low agreement among clickbait detectors reveals that clickbait detection is a rather difcult task. Our results suggest that the success of future detectors should be analyzed beyond their individual performance (the four classifers in this study had high accuracy scores), but also based on their concurrent validity. In other words, detectors should be assessed by comparing them to other clickbait measures to assess their ability to accurately and consistently detect clickbait. One idea can be to test how the detector performs in comparison to human classifers. Insights into those results can provide ideas to improve detection accuracy by assessing potential areas of disagreement. Another approach proposed by scholars studying fake news [28] is for the computational sciences to collaborate with the social sciences when building machine learning models for the detection of abstract concepts such as clickbait. For example, the social sciences can help identify features essential for clickbait detection or other abstract concepts, which can then be used to build algorithms for the detection of such content. This research is not without limitations. First, in our studies, we limited our classifcation of clickbait to seven characteristics. It is possible that other characteristics of clickbait not represented in our research are more successful than the ones we selected. Similarly, we utilized headlines that only possessed one of the seven characteristics. The goal of our research was to identify which of those characteristics is more engaging. However, there could be a potential additive efect that we did not account for. Secondly, in our user studies (Study 1 and 2), we used a sample of M-Turkers and students; though no diference was found across populations, it is not a representative sample of the U.S. population which might be less technologically savvy compared to Turkers and college students. However, while our sample is not representative, college students are as likely to fall for persuasive linguistic strategies such as clickbait, as the normal population. In fact, a study [46] found that college students performed worse than high school students in a reasoning task to evaluate online information. It is also worth noting that the scraped headlines utilized for Study 3 is considerably small, compared to other feld studies conducted to analyze headline engagement. Nonetheless, our fndings align with Scacco and Muddiman [37], whose feld analysis of 5288 headlines revealed higher engagement of non-clickbait headline. Though our Study 3 does reveal a slight preference for a few clickbait characteristics (demonstrative adjectives, \"wh\" words, and listicles), these fndings should also be analyzed with caution given potential confounds of real-world engagement analysis discussed earlier, and the rather low agreement rate among the detectors. Likewise, in Study 1 and 2, we presented headlines that were classifed as clickbait by three classifers, but we found in Study 3 that the classifers have low agreement. The low agreement raises questions about the validity of our Study 1 and Study 2. Nonetheless it is worth noting that the null efect of clickbait headlines is consistent with fndings in other experimental studies [29,30,37].\nThese limitations notwithstanding, our research adds to our current understanding of users' engagement with clickbait headlines by exploring possible reasons for the mixed fndings regarding their relative superiority over non-clickbait headlines. Our studies reveal that indeed clickbait is not as \"clickbaity\" as we tend to think, and that results might vary depending on the conceptual defnition of clickbait. For example, in our study, the four clickbait detectors yielded somewhat diferent engagement results, and agreed on a classifcation only about 47% of the time. Moreover, three of the four detectors suggest user preference for demonstrative adjectives, lists, and \"wh\" words. However, these fndings were not consistent when we tested this premise in a controlled and semi-controlled setting, suggesting that other factors pertaining to content of the story may indeed play a signifcant role in attracting clicks from users. Furthermore, other content genres, such as gossip, advertisements, and warnings, may fare better with clickbaits compared to straight news headlines, which many online users may see as inherently unworthy of clicks.\nIn conclusion, this paper exposes the unreliability of clickbait detecting classifers by demonstrating low agreement between them. While results of this research reveal that some characteristics (demonstrative adjectives, \"wh\" words, and listicles) attract more clicks when analyzing real-world engagement data, the noise associated with the low level of agreement among classifers raises fundamental questions about the validity of these systems. It is possible that the higher engagement of clickbait in computational studies (such as our Study 3) is driven by other third variables that go beyond the linguistic features of the headline, e.g., topic diferences and article placement within the website that may have nothing to do with \"clickbaitiness\" or curiosity efects. It is also possible that the modest efects of clickbaits are due perhaps to greater media literacy and user wariness arising from their prior experience with clickbait headlines.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGMENTS", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "8 amazing secrets for getting more clicks\": Detecting clickbaits in news streams using article informality", "journal": "", "year": "2016", "authors": "Prakhar Biyani; Kostas Tsioutsiouliklis; John Blackmer"}, {"title": "Click bait: Forwardreference as lure in online news headlines", "journal": "J. Pragmat", "year": "2015", "authors": "Jonas Nygaard Blom; Kenneth Reinecke Hansen"}, {"title": "Sharing the news: Efects of informational utility and opinion leadership on online news sharing", "journal": "Journal. Mass Commun. Q", "year": "2015", "authors": "Piotr S Bobkowski"}, {"title": "Contextual prerequisites for understanding: Some investigations of comprehension and recall", "journal": "J. Verbal Learn. Verbal Behav", "year": "1972-12", "authors": "John D Bransford; Marcia K Johnson"}, {"title": "You'll never guess how chartbeat came up with the greatest headline", "journal": "", "year": "2015-08-03", "authors": "Chris Breaux"}, {"title": "Stop clickbait: detecting and preventing clickbaits in online news media", "journal": "", "year": "2016-08", "authors": "Abhijnan Chakraborty; Bhargavi Paranjape; Sourya Kakarla; Niloy Ganguly"}, {"title": "Misleading online content: Recognizing clickbait as \"false news", "journal": "", "year": "2015-11", "authors": "Yimin Chen; Niall J Conroy; Victoria L Rubin"}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "journal": "", "year": "2014", "authors": "Junyoung Chung; Caglar Gulcehre; Kyunghyun Cho; Yoshua Bengio"}, {"title": "The defensive consumer: Advertising deception, defensive processing, and distrust", "journal": "J. Mark. Res", "year": "2007", "authors": "R Peter; Robin J B Darke;  Ritchie"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv181004805", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Efects of comprehension on retention of prose", "journal": "J. Exp. Psychol", "year": "1971-05", "authors": "D ; James Dooling; Roy Lachman"}, {"title": "On newspaper headlines as relevance optimizers", "journal": "J. Pragmat", "year": "2003-01", "authors": "Daniel Dor"}, {"title": "Clickbait classifer", "journal": "", "year": "2020-08-10", "authors": "Peter Downs"}, {"title": "Too good to be true, too good not to share: the social utility of fake news", "journal": "Inf. Commun. Soc", "year": "2019-06", "authors": "Andrew Dufy; Edson Tandoc; Rich Ling"}, {"title": "The efects of subtle misinformation in news headlines", "journal": "J. Exp. Psychol. Appl", "year": "2014", "authors": "K H Ullrich; Stephan Ecker;  Lewandowsky"}, {"title": "Framing: Toward clarifcation of a fractured paradigm", "journal": "J. Commun", "year": "1993", "authors": "Robert M Entman"}, {"title": "Social clicks: What and who gets read on Twitter? ACM SIGMET-RICS Perform", "journal": "Eval. Rev", "year": "2016-06", "authors": "Maksym Gabielkov; Arthi Ramachandran; Augustin Chaintreau; Arnaud Legout"}, {"title": "Shortcut learning in deep neural networks. arXiv200407780", "journal": "", "year": "2020", "authors": "Robert Geirhos; J\u00f6rn-Henrik Jacobsen; Claudio Michaelis; Richard Zemel; Wieland Brendel; Matthias Bethge; Felix A Wichmann"}, {"title": "Newspaper headlines and relevance: Ad hoc concepts in ad hoc contexts", "journal": "J. Pragmat", "year": "2009-04", "authors": "Elly Ifantidou"}, {"title": "Mapping moral motives: Approach, avoidance, and political orientation", "journal": "J. Exp. Soc. Psychol", "year": "2008-07", "authors": "Ronnie Janof-Bulman; Sana Sheikh; Kate G Baldacci"}, {"title": "Simpson's paradox in psychological science: a practical guide", "journal": "Front. Psychol", "year": "2013-08", "authors": "Rogier Kievit; Willem Eduard Frankenhuis; Lourens Waldorp; Denny Borsboom"}, {"title": "What clicks actually mean: Exploring digital news user practices", "journal": "Journal. Lond. Engl", "year": "2017-01", "authors": "Tim Groot Kormelink; Irene Costera Meijer"}, {"title": "5 sources of clickbaits you should know! using synthetic clickbaits to improve prediction and distinguish between bot-generated and human-written headlines", "journal": "ACM", "year": "2019", "authors": "Thai Le; Kai Shu; Maria D Molina; Dongwon Lee; S Shyam Sundar; Huan Liu"}, {"title": "Academic clickbait: articles with positively-framed titles, interesting phrasing, and no wordplay get more attention online. The Winnower", "journal": "", "year": "2016-06", "authors": "Gwilym Lockwood"}, {"title": "The psychology of curiosity: A review and reinterpretation", "journal": "Psychol. Bull", "year": "1994-07", "authors": "George Loewenstein"}, {"title": "Clickbait-detector", "journal": "", "year": "2019-08", "authors": "Saurabh Mathur"}, {"title": "Relevance and goal-focusing in text processing", "journal": "Educ. Psychol. Rev", "year": "2006", "authors": "T Matthew; Gregory Mccrudden;  Schraw"}, {"title": "Fake news\" is not simply false information: A concept explication and taxonomy of online content", "journal": "Am. Behav. Sci", "year": "2019-10", "authors": "Maria D Molina; S S Sundar; D Le;  Lee"}, {"title": "Aggregation, Clickbait and their efect on perceptions of journalistic credibility and quality", "journal": "Journal. Pract", "year": "2020-04", "authors": "Logan Molyneux; Mark Coddington"}, {"title": "The (null) efects of clickbait headlines on polarization, trust, and learning", "journal": "Public Opin. Q", "year": "2020", "authors": "Kevin Munger; Mario Luca; Jonathan Nagler; Joshua Tucker"}, {"title": "A state-trait curiosity inventory", "journal": "Aust. Psychol", "year": "1981-03", "authors": "F D Naylor"}, {"title": "Perceived deception: Evaluating source credibility and self-efcacy", "journal": "J. Inf. Priv. Secur", "year": "2016-12", "authors": "Dustin Ormond; Merrill Warkentin; Allen C Johnston; Samuel C Thompson"}, {"title": "Sharing of sponsored advertisements on social media: A uses and gratifcations perspective", "journal": "Inf. Syst. Front", "year": "2018-06", "authors": "J Cherniece; Emma L Plume;  Slade"}, {"title": "The clickbait challenge 2017: Towards a regression model for clickbait strength", "journal": "", "year": "2018", "authors": "Martin Potthast; Tim Gollub; Matthias Hagen; Benno Stein"}, {"title": "A validation test of a message elaboration measure", "journal": "Commun. Res. Rep", "year": "1997-05", "authors": "Rodney A Reynolds"}, {"title": "Diving deep into clickbaits: Who use them to what extents in which topics with what efects?", "journal": "", "year": "2017", "authors": "Md Main Uddin Rony; Naeemul Hassan; Mohammad Yousuf"}, {"title": "The curiosity efect: Information seeking in the contemporary news environment", "journal": "New Media Soc", "year": "2020", "authors": "Joshua M Scacco; Ashley Muddiman"}, {"title": "Lies, damn lies and viral content", "journal": "", "year": "2015", "authors": "Craig Silverman"}, {"title": "Exploring receivers' criteria for perception of print and online news", "journal": "Journal. Mass Commun. Q", "year": "1999-06", "authors": "S Shyam Sundar"}, {"title": "Journalism is twerking? How web analytics is changing the process of gatekeeping", "journal": "New Media Soc", "year": "2014-05", "authors": "C Edson;  Tandoc"}, {"title": "The efect of headlines on the interpretation of news stories", "journal": "Journal. Q", "year": "1953", "authors": "H Percy;  Tannenbaum"}, {"title": "What drives virality (sharing) of online digital content? The critical role of information, emotion, and brand prominence", "journal": "J. Mark", "year": "2019-04", "authors": "Gerard J Tellis; Deborah J Macinnis; Seshadri Tirunillai; Yanwei Zhang"}, {"title": "What prompts users to click and comment: A longitudinal study of online news", "journal": "Journalism", "year": "2015", "authors": "Ori Tenenboim; Akiba A Cohen"}, {"title": "Clickbaits: Curious hypertexts for news narratives in the digital medium", "journal": "", "year": "2017-07", "authors": "Lasya Venneti; Aniket Alam"}, {"title": "Credibility of major news organizations in the United States from 2017 to 2020", "journal": "", "year": "", "authors": "Amy Watson"}, {"title": "Evaluating information: The cornerstone of civic online reasoning", "journal": "", "year": "", "authors": "Sam Wineburg; Sarah Mcgrew; Joel Breakstone; Teresa Ortega"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Aroused curiosity as a function of characteristic and type of content", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Perceived credibility as a function of characteristic and type of content", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Perceived deceitfulness as a function of characteristic and type of content", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Perceived entertainment as a function of characteristic and type of content", "figure_data": ""}, {"figure_label": "6", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Sample story stimulus: demonstrative adjective condition (left), negative superlative condition (right). The story remained constant, but the headline difered based on the condition to which participants were assigned (eight total).", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Percentage of participants who clicked on \"Read more\" (left) or \"Share\" (right) by condition.", "figure_data": ""}, {"figure_label": "8", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Percentage of times the clickbait detectors agreed on a classifcation by characteristic.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Defnition of Clickbait characteristics", "figure_data": "Clickbait CharacteristicDefnitionQuestionsAn interrogative or inquiry left open-ended and assumed to be answered within the associatedarticle.ListA statement in list format based on a particular theme. The expectation is that the reader willencounter a series of facts, examples, or tips about that theme upon reading the article.Wh WordsFunction words such as what, which, when, where, who, whom, whose, why, whether and how.Diferent than question headlines, this clickbait characteristic does not ask an actual question orinquiry when utilizing such function words.Demonstrative AdjectivesDemonstrative adjectives (e.g. this or that) serve to indicate an entity being referred to and help todistinguish that entity from other entities.Positive SuperlativesWhen several entities are compared, the positive superlative refers to the entity that is at thehighest limit of the group in a particular characteristic (e.g. best, closest).Negative SuperlativesWhen several entities are compared, the negative superlative refers to the entity that is at thelowest limit of the group in a particular characteristic (e.g. worst, least).ModalsAn auxiliary verb that expresses possibility, suggestion, or obligation (e.g. could, must).clickbait headlines. These diferences in approaches to assemblingtraining data represent fundamental diferences in the conceptualdefnition of clickbait. While human coding represents independentcoders' understanding of what clickbait is (or what criteria were"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Stimulus sampling headlines for Study 1", "figure_data": "Political"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Number of clicks on \"read more\" and \"share\" by characteristic", "figure_data": "Read MoreShareDemonstrative Adjective7118List7722Modals5516Negative Superlative6222Positive Superlative6517Question5913Wh7418Non-Clickbait8423Total547149headlines receiving a click was 2.22 times greater than modal head-lines (p <.001), 1.81 times greater than negative superlatives (p=.01),1.68 times greater than positive superlatives (p= .03), and 1.97times greater than questions (p= .003"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Perceptual diferences as a function of characteristics.", "figure_data": "CuriosityCredibilityDeceptionEntertainingDemonstrative AdjectiveM = 4.12 abM = 3.74 abM = 3.16 bcM = 3.55 abSE=0.14SE=0.12SE=0.12SE=0.13ListM = 4.01 abcM = 3.52 bcdM = 3.62 aM = 3.62 aSE=0.14SE=0.12SE=0.12SE=0.13ModalsM = 3.51 cM = 3.25 dM = 3.05 cM = 3.38 abSE=0.14SE=0.12SE=0.12SE=0.13Negative SuperlativeM = 3.72 bcM = 3.34 cdM = 3.50 abM = 3.62 aSE=0.14SE=0.12SE=0.12SE=0.13Positive SuperlativeM = 3.86 bcM = 3.55 bcdM = 3.11 cM = 3.55 abSE=0.14SE=0.12SE=0.12SE=0.13QuestionM = 3.71 bcM = 3.66 abcM = 3.15 bcM = 3.22 bSE=0.14SE=0.12SE=0.12SE=0.13WhM = 3.92 bcM = 3.60 bcdM = 3.32 abcM = 3.43 abSE=0.14SE=0.12SE=0.12SE=0.13Non-ClickbaitM = 4.48 aM = 4.02 aM = 3.03M = 3.27 abSE=0.14SE=0.12SE=0.12 cSE=0.13"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "List of headlines for Study 2", "figure_data": "CharacteristicHeadlineDemonstrative AdjectiveThis Secret Deal Allows Iran to Make Nukes in Half the TimeListFour Things to Know about Secret Nuke Deal with IranModalsSecret Deal Expanding Iran's Nuke Program that you Should KnowNegative SuperlativeThe Worst Secret Deal: Allows Iran to Expand Nuke ProgramPositive SuperlativeSecret deal is the Best Thing that Happened to Iran Nuke ProgramQuestionWant to Know the Secret Behind Iran's Nuclear Deal?\"Wh\"What you did not Know About Iran's Secret Deal to Build NukesNon-clickbaitSecret Deal Allows Iran to Expand Nuke Program"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Pairwise comparison agreement between detectors", "figure_data": "ComparisonPercentage of Total HeadlinesPercentage of TotalTotal AgreementAgreed as ClickbaitHeadlines Agreed asNon-ClickbaitDetector 1 &. Detector 259.57%14.02%73.59%Detector 1 & Detector 350.94%18.87%69.81%Detector 1 & Detector 454.72%23.18%77.90%Detector 2 & Detector 349.06%15.36%64.42%Detector 2 & Detector 448.52%15.36%63.88%Detector 3 & Detector 447.17%27.49%74.66%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Summary of signifcant main efects of clickbait characteristic by study", "figure_data": "Study 1OutcomeComparisonRead moreNon-clickbait > modal; Non-clickbait > negative superlatives; Non-clickbait > positive superlatives;Non-clickbait > questions; Non-clickbait = demonstrative adjectives; Non-clickbait = demonstrativeadjectives; Non-clickbait = lists; Non-clickbait = \"wh\"; Wh > modals; Lists >modals; List > questionsCuriosityNon-clickbait >modals; Non-clickbait >negative superlatives; Non-clickbait >positive superlatives;Non-clickbait >questions; Non-clickbait >\"wh\"CredibilityNon-clickbait > list; Non-clickbait > modal; Non-clickbait > negative superlative; Non-clickbait >positive superlative; Non-clickbait > \"wh\"DeceptionList >demonstrative adjective; List>modals; List >positive superlatives; List> question; List >Non-clickbait; Negative superlative >modals; Negative superlative >positive superlatives; Negativesuperlative >no-clickbaitEntertainingList > question; Negative superlative > questionStudy 2OutcomeComparisonHeadline engagement (read moreNo signifcant diferences& share)Headline perceptions (deception,No signifcant diferencescuriosity, entertaining, credible)Story perceptions (credibility,No signifcant diferencesrepresentativeness, quality,liking)Elaboration of the storyNo signifcant diferencesLikelihood of sharing the storyNo signifcant diferencesStudy 3 -Detector 1 (Deep Learning, trained on Manually Annotated Data)OutcomeComparisonSharesDemonstrative adjectives > non-clickbait; Lists > non-clickbait; \"Wh\" > non-clickbait; Modals >non-clickbaitStudy 3 -Detector 2 (Traditional Machine Learning: Na\u00efve Bayes, Weak Supervision)OutcomeComparisonSharesDemonstrative adjectives > non-clickbait; Lists > non-clickbait; \"Wh\" > non-clickbait; Demonstrativeadjectives > positive superlative; Lists > positive superlatives; \"Wh\" > positive superlativesStudy 3 -Detector 3 (Deep Learning, Weak Supervision)OutcomeComparisonSharesDemonstrative adjectives > non-clickbait; Lists > non-clickbait; \"Wh\" > non-clickbait; Demonstrativeadjectives > positive superlative; \"Wh\" words > positive superlativesStudy 3 -Detector 4 (Traditional Machine Learning: Support Vector Machine, trained on Manually Annotated Data)OutcomeComparisonSharesDemonstrative adjectives > positive superlative"}], "doi": "10.1145/3411764.3445753"}