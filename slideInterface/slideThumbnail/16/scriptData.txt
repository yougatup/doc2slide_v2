
 This study stemmed from a larger project looking  at the detection of misinformation. One of the characteristics commonly found in false news  is that they are written using clickbait to lure users into engaging with content. So as we  started engaging with this literature we realized
 there were a lot of mixed findings. On one hand,  some studies indicate clear user preference for
 clickbait but on the other hand, other studies  suggest clickbait is not more effective than
 non-clickbait. So in this study, we wondered  why is this the case why the mixed findings.

 And there's a few possible reasons. The first one  is the different operationalizations of clickbait
 across studies. For example, some studies define  clickbait as the use of questions but others as
 the use of lists. So it is possible that some  clickbait characteristics are more effective than others. Yet a second possibility has to do  with the different methodological approaches.
 It is possible that the null  results from experimental studies
 is due to the low external validity of the  method but it is also possible that the engagement captured through computational methods  do not represent engagement with clickbait per se
 but third variables embedded in the assumptions of  each clickbait detector. While some computational
 models are trained with data labeled by human  coders other models utilize source assumptions.
 Likewise some detectors are built using  traditional machine learning while others
 utilize more complex deep learning models. So we  explored these possibilities via three studies
 and in the first study participants were asked to  engage with seven different clickbait headlines

 and a non-clickbait headline and we scraped  headlines by following the following procedure.
 First we scraped headlines from reliable and  unreliable sources we coded them computationally
 to determine the clickbait characteristic  and we passed the headlines by three highly

 accurate clickbait detectors and we only used  headlines that were classified as clickbait by the three classifiers. So how did the  interaction look like for participants. For
 each headline participants had the option to click  on read more if they would be likely to read the article further or share if they would be likely  to share the article with their network of friends and the same interaction was completed for the 7  clickbait and non-clickbait headlines. We found
 that non-clickbank received significantly more  reads than 5 of the clickbait characteristics and the same as the other three. Non-clickbait was  also perceived as more credible and less deceptive and more curiosity arousing but we utilized  headline scraped from online sources in the
 interest of ecological validity but it resulted  in headlines pertaining to different topics and associated with different articles creating a  content confound when comparing the various types of headlines so we conducted study number two and  here we selected only one headline and a trained

 journalist systematically vary the headline to  contain only one of the clickbait characteristics

 and again no significant differences. So the null  findings of study 2 suggest two possibilities

 one clickbait is not as click-baity as we think  or the higher engagement of clickbait headlines found in computational analysis might represent  third variables attributable to the assumptions
 of each clickbait detector. So we run a third  study. Once again we scraped headlines from
 reliable and non-reliable sources but this  time we linked them to actual shared data
 we also added another classifier  which resulted in a two by two design

 And what did we find? We find  that three of the four classifiers
 suggest that users engage more with  headlines using demonstrative adjectives wh
 words and lists. So at least in a real-world  scenario these three characteristics seem to generate better results than non-clickbait. But  these findings should be taken with caution.
 The four classifiers agreed on the classification  only 47 percent of the times. Of the 370 headlines 139 were classified by the 4 as clickbait  and 36 as non-clickbait. So overall our
 findings suggest that determining whether a  headline is clickbait or not is quite complex.
 It does not simply depend on a few key linguistic  characteristics. It also depends on the nature of the automated classifier used to distinguish  clickbait from non-clickbait. The low agreement
 among classifiers also raises fundamental  doubts about the reliability of computational
 clickbait detectors. This suggests that the  success of future detectors should be analyzed beyond their individual performance with a few  suggestions outlined in our paper. Thank you.

