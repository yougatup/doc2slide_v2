• Human-centered computing → Collaborative and social computing; • Computing methodologies → Machine learning approaches.
Human centered data science, human centered machine learning, Critical computing
Michael Muller, Christine T. Wolf, Josh Andres, Zahra Ashktorab, Narendra Nath Joshi, Michael Desmond, Aabhas Sharma, Kristina Brimijoin, Qian Pan, Evelyn Duesterwald, and Casey Dugan. 018. Designing Ground Truth and the Social Life of Labels. In CHI 2021, May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3411764.3445402
There is no ”the truth,” ”a truth” – truth is not one thing, or even a system. It is an increasing complexity. – Adrienne Rich [84]
The demand for Machine Learning (ML) models continues to grow as companies aim to beneft from cognitive services that harness large amounts of data in providing data-driven machine action [78, 121]. ML models arise in all areas of our lives, from optimizing our commute paths, assisting in work decision-making practices, to recommending entertainment options and suggesting dating partners e.g.,[3, 78, 99]. Building ML models involves data practices in a number of data phases along the data science pipeline [5, 114]. These data practices require efort and care [120]. Data science involves craftful, engineering practices, and has been explored in a number of dimensions [30, 69, 114, 116, 123].
However, ML models don’t work without human engagement [7, 43, 91]. As we think about organizations’ needs for models, we must also think about the people who do the work to make the models work [4, 6, 57, 72], and to engage in critical examinations of data science work-practices and discussions [30, 31, 68, 70, 122]. In this paper, we focus on an area of human work in modeling that has received relatively little attention - the collaborative workpractices to organize the work of labeling data, or more precisely, labeling ground truth data for use in those ML models. We show the difcult and collaborative work of data science workers to manage Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI ’21, May 8–13, 2021, Yokohama, Japan © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00 https://doi.org/10.1145/3411764.3445402
the design of labels that work for ML projects. We examine workpractices that are involved in the social production of ground truth. While labeling may seem straightforward, a closer look reveals its situated and emergent character [41].
In most accounts of supervised (machine) learning, the ground truth is considered to be the “dependent variable” that is predicted by a collection of features (independent variables) [26, 58, 78]. Each record contains the features along with the value of the ground truth. The value of the ground truth in each record is referred to as a “label” (in machine learning projects) or an “annotation” (in linguistic and medical projects). We will say “label” for simplicity.
Ground truth is an important and usually human contribution to the dataset. It is important to understand how ground truth is added to a dataset, and therefore we need to understand how humans collectively make that contribution. We will show how humans organize their work to design and create ground truth labels. We will describe their collaborations, their concerns about quality, and their mitigations of those concerns. Through a grounded analysis of interviews with 15 data science workers, we describe three major patterns in this work:
• The Principled design of ground truth as an unfolding of a planned process. • The Iterative design of ground truth as a series of anticipated clarifcations and redefnitions. • The Improvisational design of ground truth as a series of surprises and repairs.
Throughout these descriptions, we highlight the diligence, responsibility, and ingenuity of the humans who do this work.
As the use of machine learning has grown, so has the need for labeled data. The process of labeling data to train supervised machine learning algorithms1 is often an expensive and labor-intensive process, requiring humans to analyze and annotate large amounts of data in a repetitive manner [82]. Labeling is part of the preparation of data for analysis, and includes cleaning the data, removing outliers, transforming quantitative features to strengthen statistical analyses, and making non-linear combinations of features to represent additional data-derived concepts [85]. Workers in data science have described data preparation - often called ”data wrangling” - as laborious, taking up to 80% of the time and efort of a data science project [40, 46, 53, 82, 107]. Sutton et al. referred to the problem of extensive data-preparation work as “death by a thousand wranglings” [107]. We study the labeling of ground truth data because it is part of this time-consuming process. Improvements in labeling would reduce the ”thousand wranglings” down to a more manageable workload, and would make projects easier to perform, with better use of humans’ time.
1Supervised learning is a machine learning model built to make a prediction, given an unforeseen input instance. A supervised learning algorithm takes a known set of input dataset and its known responses to the data (output) to learn the machine learning model.
2.1.1 Crowdsourcing. Crowdsourcing has emerged as one way to address this challenge, by distributing labeling tasks to an army of
anonymous low-paid digital workers – what Gray and Suri [43] call "ghost work." Crowdsourcing is widely used in the feld of artifcial intelligence to obtain labeled data (e.g., [48, 50, 61, 75]), and has been utilized to solve diferent kinds of problems such as authoring tools [13], evaluation of search results [17], library cataloguing and translation [23] and document relevance assessment [42].
2.1.2 Domain Experts. However when labeling requires more specialized knowledge, or when label quality is a priority, organizations have obtained labels from domain experts – also called Subject Matter Experts (SMEs) – to provide higher-quality labels. These Subject Matter Experts must sometimes even undergo training which is both time consuming and expensive, in order to label accurately [33, 91]. Some applied research projects, e.g., in the medical domain, have shown that expert discussions following labeling yield more accurate results [32]. Aslan et. al showed that a human expert labeling approach could improve the accuracy of the student engagement measurement model they develop and describe [7]. Similarly Woitek et al. showed the benefts of expert musicologist annotations of audio samples [119].
Organizations need to fnd ways to use SMEs’ time wisely and efectively, given that SMEs have limited time, and their time is usually more costly than crowdworkers. There have been attempts to augment crowdsourced labels with expert-generated labels to increase the quality of the data. obtained [24, 52, 60, 81, 98, 112]. However, it is widely agreed that SME-labeled data is the "gold standard" data source for high quality labeled data for specialized tasks.
While there may be specialist applications to do fully-automatic ground truth labeling (e.g., [1, 2, 71]), much of the work of labeling is done by people. And yet, there is a tradition that presents data science as a rational and “data-driven” way of “discovering” truths about patterns in the world [49, 80, 111] - a world that sometimes seems to be eerily unpopulated by human beings [69]. People, of course, bring their own strengths and weaknesses to the work of data science [115, 121], and they may assert diverse defnitions of what ”rational” can mean [127].
2.2.1 Human Centered Data Science. The emerging feld of Human Centered Data Science (HCDS) helps us to understand the values and peculiarities of humans as they pursue individual and collective goals in data science [4, 6, 57, 72], and to engage in critical examinations of data science work-practices and discussions [30, 31, 68, 70, 122]. Aragon et al. described HCDS as “an emerging feld at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science” [6]. Together, these perspectives can be selectively mixed and recombined to inform both the design of tools for labelers, and also our conceptions of the activity of labeling.
In a discussion of HCDS, Yapchaian describes human work in data science as a series of observations and refnements [122]:
Data is not always received in an organized and tidy package ready to be acted on. Data scientists, working with subject matter experts (SME), begin their efort
by understanding the data available to them. (Including the asset(s) generating data.) From this initial view, they can determine what additional data and quantity of it is needed.
Beyond recognizing a need for more data - or for more ”tidy” data - data science workers often have to negotiate the data and the potentially multiple meanings of the data. Elaborating on the theme of ”tidy” data - and its converse, ”messy” data - Passi and Jackson reported,
[W]e describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantifcation, but also on negotiation and translation.
Thus, humans necessarily intervene in the acquisition and preparation of data for analysis [30, 31, 69, 96, 97].
2.2.2 Design of Data. In this paper, we will focus on such human interventions and work-practices around one particular aspect of messy and negotiable data - namely, the collaborative and critical processes around labeling ground truth data. As we will show, these work practices often involve collaborative activities such as shared assessments of available data, collective search for additional data, negotiation over meaning, and in some cases management of trust. The collective work of labeling is important to data science, because labeling is initially visible to the labeler and their administrator, and is thus a form of articulation work [92] - i.e., in the words of Hampson and Junor, "work that is necessary for the work to proceed" [47].
Subsequently, labeling work becomes "invisible to rationalized models of work" [102], such as when a data scientist constructs a model using an entire dataset which is now treated as a form of infrastructure – including not only the dataset itself, but also the human work that was required to produce that dataset [101]. If the labeling work becomes invisible, then it is no longer available for inspection in the event of problems later in the data science cycle. As Gerson and Star observed, "To the degree to which articulation is tacit and deleted from representations, requirements developed from those representations will be inadequate for local contingencies" [35]. In the context of this paper, labeling can be considered, by analogy to Hampson and Junor [47], ”the invisible work that allows data science and data scientists to work.”
Our analysis is informed by several papers that examined the nature of ”data” in data science work. Muller and colleagues described fve ways in which diligent professional data science workers necessarily intervene between a dataset and the modeling work that is usually the focus of data science projects [69]. In their grounded analysis, data may be discovered, captured, curated, designed, and/or
created. For this paper, we focus on the fourth and ffth of those interventions, the design of data and the creation of ground truth.
If data are never ”raw” [38], then it is important to understand how responsible professionals prepare their data for analysis.2 The frequent use of phrases like ”data preparation” and ”data wrangling” indicate that we already know that data must be organized [69], disciplined [65], and transformed [30, 31, 96, 97] for use in data science.
Separately, both Feinberg [30, 31] and Seidelin [97] described the every-day data-design practices in use among data science workers. Feinberg analyzes ”data infrastructure [which] involves the selection and defnition of entities, attributes, and value parameters, plus the processes established to facilitate data creation” ([30]; see also [77]). Noting Redström’s description of design as including an implicit conception of anticipated usage [83], Feinberg claimed that a data infrastructure does not so much determine data; rather, it provides an environment in which people create data so that those data will become ft for purpose - e.g., ft for analysis, and potentially ft for a particular type of analysis (see also [109]).
Seidelin considered data in an organizational context as a design material, where it is both acted on by a collection of humans and algorithms, and where it also acts on other parts of a data and analytic assemblage [97]. Data act by becoming part of the computational infrastructure (which includes Feinberg’s concept of a data infrastructure). Once placed in that infrastructure, data develop a quality of authority and objectivity [38, 44]. Data thus become powerful actors. Seidelin argues that data should be considered a potential design material for co-design activities among the organizational stakeholders who are afected by those data [96]. Tanweer extended this analysis to include collaborative preparation of data: ”Data are inextricable from the assemblages in which they are embedded” - an activity of collective sense-making [109]. Data infrastructure becomes a design material [30], and humans work diligently to design data within that environment [69].
However, once data have been shaped and molded through this design process, the evidence of those processes tends to disappear. One reason for the disappearance is that data scientists usually focus on the dataset as an entirety, rather than on its components. In Star’s descriptions of infrastructure, she says that we lose focus on the contents of the infrastructure while we focus on what we can do with the infrastructure as a service or a tool ([101]; see also [31]). A second reason for the disappearance is that lack of documentation about the labeling process - which is one aspect of the general dearth of documentation in data science [77, 86, 124]. Of course, data science workers engage in complex and social labeling processes that produce the labels in the dataset. However, they do not leave any traces of the thoughtful human work that produces those labels. Labels take on a quality of being an infrastructural part of the data of the dataset, even though they were designed and added to that dataset through human work-processes.
2We prefer not to say that data are ”cooked” because of the negative connotations of that word. However, other less-negative phrases from food preparation may be appropriate, such as ”cleaning,” ”slicing,” and ”arranging.” Certain aspects of quality and integrity may be ”baked in.” When serving meal, we may attend to its ”presentation” on the serving plate.
CHI ’21, May 8–13, 2021, Yokohama, Japan
2.2.3 Summary and a Look Forward. In this paper, we are concerned with understanding the particular and organizationally powerful data type of ground truth labels while they are being designed (e.g., [30, 97]) and created [69], and before they slip unobserved into the infrastructure [101] of the modeling work that dominates thinking in data science. While the phrase ”ground truth” implies a kind of objectivity and authority [38], we show that ground truth labels are the designed outcomes [30, 97] of both intentional and improvisatory collective design processes, embedded in both strategized and reactive work-practices.
We show that the experience of labeling needs to be improved - for the sake of the labelers, and of the labels. We then describe how people collaboratively defne and apply labels. Aspects of that collaborative work reveal weaknesses in the quality of some labeling work-practices and the quality of the labels that people create through those work-practices. In response to these weaknesses, we describe how responsible data science workers mitigate some of those problems. Finally, we describe three approaches to the collective design of ground truth: Principled design; Iterative design; and Improvisational design. In the Discussion, we collect these ideas into proposed new understandings, and we carry ideas and understandings into implications for future technologies and workpractices.
We conducted our research in IBM Research and customer-facing parts of IBM’s consulting organizations, primarily in North American locations. IBM provides computer hardware, software, and services to global customers. Most of the interviews were conducted online; four were face-to-face. All interviews were recorded and transcribed.
We interviewed 15 people who worked in several diferent data science roles. Because we are interested in collaborative labeling of ground truth, we restricted our analysis to people who worked in teams. 47% of the informants were women and 53% were men. Table 1 shows summary quantitative information about the interviews. Table 2 provides information about each informant.
Trovato and Tobin, et al.
With one exception, informants worked on teams in the IBM Research organization, or in customer-facing teams that were closely aligned to the research organization; the one exception was a data science researcher who was working on an algorithmic project that involved labels (I-02).
Because of the structure and business relations of IBM as a business-to-business company, we were able to interview only the researchers, team leads and managers who organized the labeling work - and not the labelers themselves. To understand labeling practices within the broader time-course of a project, we always asked informants to discuss a completed project. Labelers among IBM’s customers were employees of the customers, and we could not ask for their time. In other cases, I-05 and I-06 worked with physicians as labelers, and had completed payments for their participation; the same was true for the fnancial analysts in I-08’s project. I-03 reported her own team’s difculty to persuade domain experts to provide detailed information. I-01 described labeling by a team of three interns; however, those interns were no longer to be interviewed. We therefore note that this paper provides a useful but partial view into the specifcally collaborative work of labeling. A more complete view will emerge over time, when it is possible to interview people in other roles, including both domain experts and clients.
We analyzed the interview transcripts using ground theory methodology, based on a combination of practices from Charmaz [18] and from Corbin and Strauss [22], as adapted for HCI [67]. Grounded theory is an inductive method that builds theory ”up” from data - i.e., the ”grounded” in grounded theory is ”grounded in the data.” Grounded theory is usually applied to qualitative data. As a deliberately and self-consciously rigorous methodology, grounded theory includes a series of analytic practices that have been developed since the foundational book by Glaser and Strauss, originally published in 1967 [39]. For example, grounded theory researchers engage in constant comparison of data with data, and of data with theory - i.e.,
the theory that the researchers are iteratively constructing from the data [18, 22]. A second analytic practice example is theoretical sampling, in which researchers builds a temporary theory, and then collect more data to ”test” that theory - typically at its weakest points [16, 19, 34]. When the iteratively-developed theory passes all of these tests, then researchers feel it is strong enough to publish. Thus, although a result developed through grounded theory has not been evaluated through statistical testing, it has undergone a series of rigorous qualitative tests which tend to revise and expand the power of the result over successive iterations within the qualitative analysis protocol.
Working with the transcripts (Table 1) and following grounded theory practices as adapted for HCI [67], we began with descriptive codes for items and events in the data (”open codes”), and then we looked for related items [22]. One researcher organized this work and conducted most of the coding. Two additional researchers reviewed those codes, discussing changes as needed until we reached consensus.3 We then put those related items into structured collections of concepts or categories (”axial codes”). Through iterations of coding and re-coding (i.e., constant comparison and theoretical sampling), certain stronger and more generalizable concepts began to emerge. In this paper, those concepts are the experiences of labeling, and especially the collaborative work of teams while labeling, including multiple patterns in their design of ground truth labels. Figure 1 provides a high-level summary of the emergent codes from our analysis; these emergent codes become the basis for our Results in Section 4.
Papers in the CHI conference tradition have successfully applied grounded theory to analyze interview data for sample sizes ranging from as few as six interviewees in two papers [8, 79] to a high of 74 interviewees in a market-oriented study [105]. Our sample size of 15 informants fts comfortably within this range. We note that appropriate sample size is not really a settled matter in the broader literature on grounded theory [14, 66]. In a policy editorial in Archives of Sexual behavior, Dworkin surveyed grounded theory interview papers, and reported advices of minimal sample sizes that ranged from fve to 50 informants [27]. In a fascinating paper, Baker and Edwards asked 15 well-respected experts on grounded theory research about sample sizes [9]. Most of the experts discussed the complexities of making such a determination, including the scope of the research project, the experience of the researcher, and the practical constraints of budget and time.
Multiple of their expert informants recommended other criteria for assessing a grounded theory analysis. An important quality of rigor in grounded theory methodology is called saturation, or sometimes saturation of categories or saturation of data [45, 66]. Saturation is generally understood to mean that the researcher is no longer fnding new information about each important concept [45]. However, as Morse stated, while "saturation is the key to excellent qualitative work," there are nonetheless "no published guidelines... for estimating the sample size required to reach saturation" [66].
Stern discussed the concept of saturation in terms of being "bored" by interviews that added no additional analytic insight [104]. Guest et al. performed a rare, detailed analysis of saturation
3Our use of grounded theory was interpretive, and therefore measures of inter-rater reliability were inappropriate [64].
in terms of how many new codes were learned in each interview [45]. When the number of new codes dropped to zero, saturation had been achieved. In their study, the frst 12 of 60 informants accounted for the vast majority of codes. Using a similar saturation criterion, Majid et al. also found that saturation occurred after 12 interviews [62]. Within HCI, Muller described saturation in terms of curiosity and surprise, echoing Stern’s position [104] that saturation occurs when the researcher is no longer surprised by information in each interview [67]. In this paper, we describe concepts that were reported in multiple interviews as our evidence of saturation - i.e., a multi-informant form of the no-new-codes criterion of Guest et al. [45] and Majid et al. [62]. If we are hearing again about the same concept from multiple informants, then we have saturated that concept.
We begin the Results section with some brief orientation material (Sections 4.1 and 4.2) to prepare for our detailed fndings . We then present informants’ views on the technologies that their teams use (Section 4.3). We next consider the experiences of labeling, which become a motivation for improving existing technologies and workpractices (Section 4.4). We consider informants’ use of collaboration practices (Section 4.5), and potential quality issues that may arise (Section 4.6). Finally, we interrogate the design of data (Section 4.7), analyzing these practices into three categories: Principled design, Iterative design, and Improvisational design. Figure 1 summarizes our interpretation, and states the major Axial codes in our grounded theory analysis.
As we described above, some additional codes (not shown) occurred only once, and we therefore excluded them from further analysis because there was no saturation on those codes.
In labeling work, the labeler classifes each record in terms of the category that it belongs to. The category is generally referred to as ”ground truth.” The goal of most machine learning projects is to predict the ground truth on the basis of multiple predictor variables, often referred to as ”features.” A familiar tutorial example is the Iris dataset, in which measurements of plant parts (features) are used to predict the species of each iris fower (ground truth) [74, 108].
In the preceding paragraph, we stated that “the labeler classifes each record...” But who or what is the ”labeler”? Labels may be applied to records by human labelers, or by algorithms (e.g., [21]), or by ”human-and-AI-in-the-loop” hybrid systems [24, 81, 98, 112, 126]. In this paper, we focus on human-centered labeling practices as a path toward a hybrid future.
If our focus is on humans and their work-practices, then we need to know more about how they organize their work. In general, labeling teams had a hierarchical structure. Usually, there was a team-lead or a project-lead who organized the work of the labelers. In tightlystructured hierarchies, the labelers were recognized domain experts who were sometimes assigned from other teams. In one set of projects, the domain experts were external authorities, paid for a day of labeling work (e.g., physicians who worked with I-04 and I-05,
and fnancial analysts who worked with I-08). In loosely-structured hierarchies, the team-lead had to recruit labelers from within the team - and sometimes encountered reluctance from team members (see Section 4.4). We ofer this overview of team confgurations as an orientation to the results that follow. Further details on team structures and their consequences may be found in Section 4.5 and especially Section 4.5.2.
Informants used diverse labeling tools. To avoid critical comments on specifc products, we will list them together in this paragraph. The most frequently used tool was an unmodifed spreadsheet. Some projects used more specialized tools such as CrowdFlower (previously FigureEight).4 Projects that focused on both entities and their relationships used more complex linguistic tools such as brat.5. A few projects had unusual needs that led them to try multiple commercial tools, and then to build their own in-house tools (similar to e.g. [87]).
Informants described their considerations when selecting the labeling tools according to the task they were performing, for example, I-12 commented about the usability of the tools in relation to multiple labelers, stating that “the challenging piece here is the... usability of those tools and ability to distribute in [an] efective manner across several people, which is... time-consuming.” I-15 commented on the challenges of using external labelers “We defnitely needed to trust the labeler and on the external tool you cannot trust the labelers.” This kind of difculty became part of the motivation to focus on team members and/or domain experts, who performed their
4https://visit.fgure-eight.com/People-Powered-Data-Enrichment_T 5https://brat.nlplab.org/index.html
labeling work under more controlled and partially-accountable conditions.
I-08 highlighted that some commercial tools can support multiple labelers’ labeling practices in a sophisticated fashion, "You have multiple people working on it and they have like a very sophisticated interface where they can click on [the] labels they want to attribute.” She also described some of the consensus practices used to agree on label disagreement ofered in the tool, “And then at the end of the task, they actually have even a verifcation test set because it’s multiple people that work on that to say yes or no, this labeler made sense or not.”
In contrast to using commercial tools, other labelers took a less formal approach. I-05 stated “I may start of with not really even a true annotation tool or a very, you know, just building up a system just trying it out.” This approach was echoed by I-02 in relation to the act of labeling in one of their projects, “We did kind of a brute force approach. Everybody on the team got their 100 or 200 examples to label and they just had to choose a class label for every single example”.
Interestingly, labeler task creators and labelers refected on the tools and artifacts they created when assembling the labeling task and conducting labeling. I-11 stated “We’ve kind of came up with a set of rules and defnitions that I think ended up in creating fowcharts at one point, to help us guide our labeling.”. This quote followed with comments about what the fowcharts enabled the labelers to do. I-12 mentioned “We started creating spreadsheets with the label data... my colleague and I ended up getting pretty close, we got into each other’s heads enough to where we were doing this fairly reliably”. Some informants also wished for a tool that could support a broad range of labeling tasks, I-01 said
”If you were building a custom labeling tool for one customer I would never say, ‘Building a spreadsheet.’ But, if your goal is to have not just like an [open source] labeling tool for a specifc domain but a truly general or general fexible labeling framework... I wouldn’t even say the whole thing is like one smart spreadsheet but the notion that you can import and export into spreadsheets ... and then you plug your spreadsheet in and it gets sucked into this system...”
As we will show, below, the labeling projects were quite diverse. Teams chose their labeling tool based on the nature of the labeling problem that they needed to solve, the characteristics of their ground truth, and conveniences or limitations in available budgets, timeframes, and tools.
Amongst informants, there was a general sentiment that labeling is a thankless and burdensome task. Labeling "tends to alternate between mind-numbingly boring and excruciatingly painful," as I-01 noted. They explained that the painful part, in their project, was dealing with unclear or ambiguous instances to label: "You’ve got to be kidding me. How am I supposed to label it?" As I-ll fatly put it: "labeling is this inherently boring task."
Orienting one’s self to the labeling task can be challenging, as I-12 shared:"labeling was not something which... they did day to day. That was very new for them..." (I-12). I-06 spoke of the "resistance" of team members when asked to perform labeling, "it’s not like really a fun job to do." I-14 said that he had to resort to "bugging people to do it when they had time..." I-12 reported of his team that "in most cases they had tried to stay away" from labeling sessions. I-1 tried incentives, such as "we have what we call a tag party. You know, we got pizza and we got 13 people to sit in a conference room and start tagging [i.e., labeling] posts." However, even in this case, "people were getting really hung up" on labeling disagreements, which points to the situated and emergent nature of labeling data (see Section 4.7).
4.4.1 Contrasting Automation and Human Expertise. Given that labeling is unpleasant and difcult work, many see it as an opportunity for automation technologies to ease the repetitive and numbing nature of labeling for humans [3]. Yet, in most cases, human knowledge remains an essential dimension in generating a high-quality, labeled training set. Cutting-edge ML paradigms, like reinforcement learning, enable models to gradually improve accuracy by measuring user behavior and interactions with model outputs. But even in this type of approach, the model needs human-labeled inputs to provide a "warm start," as I-03 described. Their project, which centered around a reinforcement learning model to build a chatbot, needed a starting set of labels to provide a reasonably accurate starting place for the system’s early users (e.g., routing service requests to the appropriate channel). I-03 provided this warm start by generating some labeled data, applying her own best guess as to what category each user request fell into. She was conscious that she was creating these labels in her role as data engineer, and she said, "think of it as almost like user labeling." The actual user-derived labels would be incorporated into the model and refned via reinforcement learning during end-use.
In the healthcare domain, I-05 described the necessity of involving medical experts when labeling the contents of an electronic health record (EHR) because it "is not always complete or contains outdated information." Further, an EHR contains both general and specifc information. She explained, "So you may have a concept that shortness of breath which is also the name is dyspnea which is all the same difculty breathing just diferent way to express it." In a project to categorize the social infuence of outbound social media posts from IBM’s marketing organization, I-1 said that "there are attributes in a social media post that would be impossible to code for... is it entertaining?, funny?, does it relate to an audience, and the timing of the post."
4.4.2 Summary. Thus, the tedium and difculty of labeling call out for automated assistance. However, human knowledge remains critical to train that automated assistance. We explore further complexities of human labeling work-practices in the following sections.
Collaboration followed multiple patterns among our informants. Several informants reported that they asked at least two people - and as many as fve people - to label the same record. Except for extreme cases (e.g., 1-2 labelers in a team), these replications of labels did not appear to be related to team size, but rather to the intended degree of label quality. I-04 said that ”as a team we pick 20 patients each... and where we doubled on a patient with somebody else.” In an image-labeling project, I-14 said that ”most of the images that have - would be labeled by at least 3 diferent people.” I-09 reported that ”every [labeling] task has fve people.” The general rationale for asking multiple labelers to work on the same record was accuracy (sometimes expressed as "verifcation"); however, informants did not report a rationale for the larger numbers of labelers.
4.5.1 Dealing with Disagreements. If the multiple-labelers agreed on a label, then of course this was the ideal outcome. I-01 reported that a requirement for ”consensus” was helpful but potentially timeconsuming. If a pair of group of co-labelers disagreed, then an additional labeler might be called in as a ”tie-breaker” (I-09, I-1).
For other informants, there was a hierarchy of authoritative labelers. I-09 reported that ”We can actually let everything go to the crowd and only edit. And we ask the expert to process [it]” - i.e., an SME would examine cases of disagreements among frst-pass labelers. In several healthcare projects, the labelers were themselves SMEs (physicians). They were encouraged to resolve disagreements via ”some kind of consensus voting” (I-04). However, if discussion did not lead to consensus, then I-05, a project team member with medical training, would apply her combination of healthcare expertise and data science expertise to resolve the issue:
We’ll do an hour of annotation. Then we’ll regroup. Maybe it’s lunchtime. And while they do lunch ... I’m like changing things to try to ft in what they’re saying and capture the nuances they’re pointing out...
Failures in consensus on labels for a chatbot, required that I-01 examine the labels from each of 3-5 labelers, as recorded in a spreadsheet. Both I-01 and I-11 complained that comparing multiple labels in a single row of a spreadsheet could be problematic - ”horizontal scrolling is awful” (I-11).
4.5.2 Division-of-Labor. Informants who organized the work of labelers often used diverse forms of division-of-labor. One type of division of labor was mentioned in the preceding sub-section: I-04, I-05, and I-09 described the management of disagreements in terms of a hierarchy of labeler-expertise (e.g., SMEs). Less formally, I-1 worked with a team of three interns. For the most part, the interns resolved their own labeling disputes, but occasionally ”they’d like raise something to me, what’s the tie-breaker, what’s your thought on this?”
Another form of division-of-labor occurred during initial training. I-06 told us that their team
mostly worked by themselves, but in the beginning especially we kind of worked together just trying to fgure out what things are like. But yes, general rule of thumb it was like by ourselves.
I-02 also reported that labelers worked in isolation, ”we were kind of trying to do the simplest thing where none of our labelers could interact with any other labelers.” I-02’s motivation was primarily to divide necessary work among people who were reluctant to do the work: ”We did kind of a brute force approach. Everybody on the team got their 100 or 00 examples to label.” However, this principle could be applied diferently for some teams when labelers disagreed. Echoing the expertise-based distinctions from the previous subsection, I-11 said
We also considered various methods of... sharing the label example from ones who were high-quality labelers... For example, if you label something and it was an outlier compared to your peer group... highlighting that and saying, hey, four out of fve labelers actually chose this... You may want to consider - reconsider your choice.
I-01 applied a diferent form of division-of-labor among a team of chatbot developers. In many chatbot architectures, the front-end dialog is used to route the user’s request to an appropriate backend service (called a ”user intent”) [29]. Members of I-01’s team had organizational relationships with teams that built some of the back-end services - ”a sense of ownership,” according to I-01. For this team, the division-of-labor called for each labeler to look for user requests that would match their back-end service-of-interest:
Some of the companies have - individual people [who] were responsible for diferent subsets of the classes and the model.... "I’m responsible for the legal questions and you’re responsible for the technical questions..."
Finally, we observe that there may be multiple stakeholder groups with an interest in the labels. While these groups may not have agreed on an explicit division-of-labor, they nonetheless must manage their multiple sets of labels as part of their multiple stakes. In customer facing work, I-08 reported that ”We also have labels from their side that we use for the extrapolation of labels... And then of course the labels that we created or taxonomies, then we would present that to them.” I-08 elaborated:
...The labels we attribute to [their data] - of course we check them with them. We have regular [meetings] where we go over the results and say, "Hey, we found this, this, and this. Does this makes sense?" And then
they can say yes [or] no... so there’s a... running back and forth there.
4.5.3 Summary. Collaborations took diverse forms. Some teams isolated each labeler from other labelers, resulting in a real but minimalist dyadic collaboration between team-lead or researcher, and each labeler. A common theme was to collect multiple label-values for each record, to increase accuracy or verifcation for the label or labels that would ultimately be assigned to that record, and then become the basis of modeling. Informants and their teams structured their collaborations to detect and resolve potential disagreements, employing strategies that ranged from tie-breaking to consensus. Often these strategies made use of diferences in expertise among the labelers, assigning greater responsibility and infuence to labelers with greater expertise (e.g., SMEs). However, other organizing strategies were also observed, such as taking advantage of organizational relationships to assign each labeler to fnd records that matched their organizational stakes, and to assign the same labels to all of those records. Finally, some teams consulted with their clients over labeling decisions. As we will see in the next section, various collaboration and division-of-labor strategies had implications for the quality of the assigned labels.
Informants were candid about the compromises that they had to engage in, to make the labeling process ”work” in the context of their projects. We say "to make the labeling process ’work’ ” in the sense of articulation theory, which is often characterized as the ”work to make work work” [92]. In this section, we describe how the informants worked to make the work of labeling successful in practical circumstances.
4.6.1 Reconsidering the Qality of Labels. Informants acknowledged that they were working in a research engineering environment, and that engineering is often a matter of trade-ofs. The quality of labels had to be considered in relation to resource limitations, such as available staf and available time. Working on a chatbot project, I-03 acknowledged that her labels were ”not a ground truth [in the sense of] which one is the absolute best,” but rather a series of approximations that would work enough for a ”warm start” of her chatbot service, which would then improve the quality of its labels through reinforcement learning. I-06, who worked on a diferent chatbot project, stated the situation more bluntly: ”Not super scientifc, but it works.”
These issues were not limited to chatbot projects. In a healthcare project, I-04 acknowledged that ”there’s lots of confusability,” Working on a diferent aspect of the same project, I-05 spoke of the need for administrative control in the labeling project: ”everyone has an idea of what a problem is but we need to tell them what [the] problem is... because everyone has a diferent idea.” I-06 agreed about the ambiguities of the general situation of the labeling task: ”Depending on who you [are] or depending on the time of the day, or the week, you could probably label it two diferent things.” In the words of I-05, there was sometimes a need to ”kind of clean the ground truth.”
I-04 told us that some of the difculties with label quality occurred ”when an expert... may not have been available...” and noted
that in some situations, ”medical expertise was less important as compared to just word knowledge or linguistic input.” I-03 also reported difculty to access domain experts and their knowledge: ”that’s very rare that we work with them [domain experts]. It’s kind of just like a conversation. They give us their skills and we build stuf on top of them typically.” In some projects, domain experts provided documentation for the labeling team, but declined to label directly. I-08 said that ”where we have documents, we have no help from their side so it was done in-house - yes.”
4.6.2 Situated Data Science Projects. The practical circumstances of contextualized project work imposed several types of constraints. Working on a chatbot project, I-03 described a set of client cases that raised statistical distribution issues: ”One of your [labels]... covers a bunch of diferent topics. So we want to split this into several [labels].” In this project, it was not possible to change the statistical distribution by obtaining more data, so an existing label had to be split into additional new labels in order to improve the downstream modeling performance. I-06’s team also ”limited the possible labels” based on pragmatic constraints. I-08 described a customer-facing project in which the business goal was to demonstrate the value of IBM’s approach to the customer, within the period of a limited customer-engagement: ”You try to do your best, but it’s sometimes complicated because of the [time limits].” Therefore, ”It’s really more of... how to adapt or attack a business problem or how to get value out of it... by using machine learning as efectively as possible.”
In the health sciences domain, I-05 acknowledged that the scoping of the items to be labeled could be limited by resources and timeframes, resulting in limitations on what her domain experts should label:
We said clearly... we don’t want some of the... chronic problems, you want acute problems are happening right now. You don’t want anything that is resolved. But then there’s the question ’okay what if the problem is resolved but there are some residuals?’ ... Or another thing we tell them we want recurrent problems.
4.6.3 Mitigating Qality Issues. Earlier, we quoted I-05’s call for administrative control over the labeling process. Informants exercised control in three principal ways. The frst strategy was to use multiple labelers on each record, as described in detail in Section 4.5, further strengthened through hierarchies of expertise and authority as described in Section 4.5.2. We describe the two other strategies in this section: applying tacit knowledge of data and humans; and the use of guidelines.
Tacit Knowledge. Informants’ second type of administrative control was less formal, and sometimes under-acknowledged as a form of quality-control. Informants often had intimate knowledge of multiple formal aspects of their projects, and this knowledge allowed them to apply their expertise in diverse ways. As Schön has argued articulately, designers often enter into ”a refective conversation with [their] materials” ([93]; see also [10]). Muller et al. showed that this concept could be applied to the analysis of data science activities [69], through the data science worker’s intimate knowledge of the data and the tools that manipulate the data.
Informants’ accounts revealed this kind of intimate and sometimes actionable knowledge of their data. While working on a
sampling algorithm, I-02 remembered, ”One thing that we realized was that when we as humans looked at the experts some conficts just looked bad to us in ways that none of our automatic metrics were capturing.” Later in describing this aspect of his work, I-02 added, ”There’s something fshy going on... When I see this example I don’t really know how to fx it, but I do agree that there’s something wrong going on.” When I-02 talks about ”some conficts just looked bad” and ”there’s something fshy,” these are examples of using intimate knowledge of the data to perceive problems that are not detectable by formal algorithms. I-05 spoke of using her medical knowledge of physicians’ note-writing practices to detect issues, ”as a physician reading the note, you know, their thoughts are organized in a certain way.” In all of these cases, informants used their tacit knowledge to address quality issues.
Informants also used a diferent kind of tacit knowledge - their knowledge of humans as labelers. I-02 considered human limitations and algorithmic limitations in selecting which records should be labeled by human labelers:
So if you give somebody an example to label that’s incredibly hard to label and they take 20 minutes to fgure out what the right label is, that might not be as good as if you were able to give them 50 diferent examples at the same time, and learn even more from the diferent examples, because they were easier to label, even though that one example is the most valuable single example in your data set.
I-05 also had to navigate limitations of human cognitive load vs. the needs of modelers, “It was always kind of a back and forth between... what the researchers and engineers want versus what the human is capable of doing...”
In these examples, we can see informants as knowledgeable and intuitive practitioners, seeing into their data and their colleagues. We described their knowledge as ”tacit,” which implies that the informants may not always be aware that they are using these kinds of insights. We hope that these observations may encourage researchers to explore tools to encourage and capture refective practice (e.g., [56].
Guidelines. Informants exercised a third type of administrative control through multiple forms of guidance and documentation for labelers. Fort has made a strong argument for extensive ”Annotation Guides” - i.e., documents that explain the labeling process in detail [33]. Informants did not typically provide the rich detail recommended by Fort. However, they often provided as much guidance as initially appeared to be needed for labelers. I-09 stated the goal as follows: “When we’re perform[ing] labeling tasks for the product[-use] that is a very precise defnition... So labeling for everything is actually consistent.” For complex labeling requirements, I-12 spoke of a “codebook” and I-11 made reference to “fowcharts.” Guidelines may or may not require a formal document. Working on an image-labeling project to model trafc congestion, I-14 described a set of lightweight guidelines in the form of onscreen reference photographs: ”We had 6 or 8 diferent pictures on the main screen showing what we would consider to be congested.”
Guidelines can help to reduce the need for double-labeling, thus saving time and costs. I-04 reported that she would ”try to defne the annotation rules before starting the actual annotation process...
Once the rules have been defned and something has been made more (deterministic) [for] an expert... it’s typically not double annotated.”
However, some of the projects found their initial guidelines to be insufcient. I-05 began with the intention of the guidelines - ”usually it should be if your guidelines are good or the way the test is set up it should be very clearly right or wrong” - and then described potential problems: ”But there is always a gray area...” The problems could become an occasion for further refnement of the labeling vocabulary and hence the guidelines: ”rather than forcing the gray area into a right or wrong a lot of times what we do is we try to qualify it.” Guidelines that are prepared in advance may require revision when used ”in the feld,” according to I-04, who added ”but you arrive at that point often it’s confusing and it... leads to a lot of disagreement.” What is required for adequate guidelines?
4.6.4 Summary. Informants sometimes struggled to make difcult trade-ofs between the pragmatic needs of their projects, and the formal needs of high-quality labels for subsequent analysis. They described problems of labeler confusion, and many issues of resource constraints. Informants also reported on their eforts to improve or restore label quality, including the use of their own well-informed tacit knowledge, and diverse forms of guidelines for labelers and other members of the team.
In work mentioned earlier, Fort advocated a process that could take as long as six months to develop a set of extensive guidelines [33]. However, several of the informants described projects whose entire duration was less than three months. We assert that there is evidence both of the value of guidelines, and also of the practical diffculties of creating those guidelines [55, 77, 86, 124]. We hope that researchers will work on future print-and/or-online tools that can provide more robust support to labelers, and we believe that these tools may also provide labor-saving and consistency-enhancing recommendations based on team needs and individual preferences (see Section 4.3).
The principal fndings of our paper are our account in this subsection of how the work-practices in the design of ground truth labels. In grounded theory terms, this subsection constitutes our Selective Code, and corresponds to the lower portion of Figure 1. We describe three appraoches to the design of ground truth: Principled design; Iterative design; and Improvisational design.
4.7.1 Principled Design of Ground Truth. The strategies in this subsection refect both a solid planning process and, importantly, an absence of substantive surprises. Several projects began with a wellplanned procedure for defning labels and applying those labels to data records. In one of the healthcare projects, I-04 discussed a preselected medical vocabulary for labels: ”In a medical context it might be problems like diseases, conditions, symptoms, medication... So these would be diferent labels that would be assigned to the same [textpassage] and it would persist.” Her research partner, I-05, reported similar to ”build a lexicon” for a diferent study, ”We had tasks where you were identifying a single concept for example medication, or a lab test, or even a lab test and its corresponding value and, you know, billing together.” In a chatbot project, I-06 described a plan to
constrain the labeling vocabulary to ”30 altogether topics that were very limited, so we didn’t choose anything outside that 30.”
Of course, ambiguities frequently arise in labeling edge cases. Projects taking a "principled design" approach were also distinguished by a plan to resolve ambiguities. I-05 provided an example of resolving ambiguous terms online through the Unifed Medical Language System (UMLS), an online medical database, where ”by linking.... [to] UMLS which has all the various medical language terms and is linked to, you know, it provide[s] additional information.”
Another strategy for "principled design" was described in Section 4.5.2, in which I-01’s chatbot team divided their labor by assigning each team member to look for user-requests that corresponded to that team member’s respective back-end service.
4.7.2 Iterative Design of Ground Truth. However, what happens when plans do not go ”as planned?” Informants on a second group of projects knew that they could not make a principled plan for all of their labeling protocols and stafng. They accepted this uncertainty, and built their plans around disciplined, iterative approaches to resolving the uncertainty.
One strategy was to build iterations into the labeling plan. In I-05’s healthcare project, her team intentionally developed certain labels through a series of refnements: “I may start of with not really even a true annotation tool or a very, you know, just building up a system just trying it out.” In I-04’s related healthcare projects, there was a series of steps in defning and refning labels, “the input from the expert annotator is only at certain points and done with certain... resources that might help this process, but not in terms of actually creating that entire question answering dataset.”
A related theme came from I-11’s and I-12’s work on a project to estimate conversational quality in chatbot sessions. They knew that they would need to work to fnd a solid set of metrics for conversational quality. I-11 described their iterative approach: “We’d grab like 20 conversations, we would cold-label [the] conversations, evaluate what we did, discuss what went well, what went poorly... and see how we were doing, And then go back and do it again.” I-12 concurred: ”There’s a lot of discussion, lot of iteration on the rules of the code sets.”
Beyond planning in general for iterations, some informants described iterating on combinations of human activity and computer activity. In I-10’s project on structured documents,
“the tool learns an initial set of rules and then it continuously refnes this by asking the user for concrete examples. So, the tool generates examples that the system is uncertain the labels should be... And then, after additional labeling from the user the system refnes the rules.”
Similarly, I-06 worked with a more product-oriented chatbot tool, and said,
“we were trying... to gather the utterances, [because the tool] requires some utterances to be trained so that they can respond to, and they can fgure out what topic it’s talking about and then they can start the dialogue from there.”
In both of these projects, the teams’ development plans provided for step-by-step development of the competence of the chatbot system through iterations of human and computer label-development.
4.7.3 Improvisational Design of Ground Truth. For some teams, there were many surprises, which took them away from any possibility of a planned development of labels. Other teams seemed to approach the problem of ground truth labeling as an area of open explorations into an unknown domain. In these cases, ground truth emerged from a series of improvisations.
Informants acknowledged that limited time, resources, and labelerknowledge could impact the labels that were assigned. I-1 was concerned that his very busy team of reluctant labelers had ”too little time commitment, too little thought commitment,” and he eventually re-assigned the work of labeling to three interns, who could be assigned to create labels as a full-time activity. Earlier in Section 4.6.1, we quoted I-03 describing her chatbot project as ” It’s not a ground truth [in the sense of] which one is the absolute best.” I-06 reached a similar conclusion in her chatbot project, ”we don’t have to be like extensive but we have to be representative enough so that it will fgure out that these are going to be labeled into one and the others are going to be labeled into the other.” I-02 described her labelers as ”learning as they go.”
When teams work intensively together, they may be able to apply good labels. I-ll reported that ”we got into each other’s heads enough to where we were doing this fairly reliably.” However, more diverse teams may encounter unexpected difculties. I-1 remembered,
”So strategists, creatives, not predominantly data analysts or subject matter experts or anything of that nature... What we ran into immediately was some of the categories were subjective in nature, like what constitutes the exit post versus a promotional post. And people were getting really hung up on those defnitions. And so each person... was coding things diferently.”
We previously noted I-06’s complaint in Section 4.6.1, ”depending on who you’re doing or depending on the time of the day, or the week, you could probably label it two diferent things.” Even successful labeling eforts could encounter early problems. I-05 described her early labeling work as ”mostly we had a huge dump of an HTML with all the text - all the notes for a patient that... can be sorted in diferent ways, can be fltered in diferent ways, and you just have to navigate your way through.” Teams had to stop, reconsider, negotiate, and put considerable work into aligning their labels with one another. This mutual re-alignment was of course possible, but it added time and efort to what was already an efortful task.
Other problems emerged from pragmatic limitations and relationships. We learned about two categories of pragmatic challenges, including issues intrinsic to the project, and challenges that primarily addressed issues extrinsic to the project, including concerns that arose from relationships with clients.
Intrinsic Challenges. Some projects faced ”massive amount of topics... I don’t think we tried much to actually harmonize them or evaluate how diferent they were” (I-06). Facing a similarly formidable number of topics, I-07 said ”we ended up doing the simpler ones, because... this is more straightforward, more explainable, less time-consuming.” Like I-07, I-05 also had to ”change the name of the label going forward if the label is confusing to [SMEs].” Sometimes it
was necessary, according to I-05, to ”make up our labels... because... there is no single word that represents what we mean.”
The requirements of projects seem to have powerful efects on the names and defnitions of ground truth labels.
Extrinsic Challenges. When teams faced limited time or resources, they often had to make compromises on how they managed their labels. I-08 and his team worked on quick cycles to show value to customers. They could not label extensively, but rather ”for most of the cases I guess we are working with single label just because of the nature of the POC [proof of concept].” I-08 and I-13 reported that customer-reactions drove part of their labeling protocols and outcomes. I-08: ”we try... to help them adopt our line of product and show them the value in that by sharing them with the very specifc use cases how we actually create value.” and I-13: ”we managed to implement both methods and, yes, okay results. Client was happy. Obviously, if we had more time, more data, more annotations, we could have done a lot better. we usually work on six-week engagements.”
In the preceding examples, the relationship with the client led to restrictions in the types of labels and the thoroughness of labeling. I-07 worked with an inverse of this challenge while using labels on outbound social media posts to control the relative volumes of posts. I-07 distinguished between frequent posts that made assertions vs. rare posts that showed products in action, ”there’s very few of those ’show’ posts, but we wanted to highlight our - showing a product in action versus telling people.”
Of course, relationships are important, and it makes sense to adjust labels and labeling protocols. to support those relationships. From the perspective of ”ground truth labels,” we see again that both labels and processes are contingent on both intrinsic and extrinsic relationship opportunities. Ground truth appears to be grounded in data, and also in networks of social and business ties.
These accounts imply that label names and meanings are not fxed in advance, but are malleable, changeable, and negotiable depending on who is applying them. Labels become accountable beyond simple defnitions - accountable to pragmatic limitations, to persons, and to corporate entities. Labels thus perform multiple types of work for us. Labels describe the world. Labels mediate defnitions of the world. Labels arise from the needs of relationship partners. ”Ground truth” begins to look less like a formal or ”objective” truth, and more like a worthwhile social accomplishment.
We began this paper with Rich’s observation that “truth” is or becomes an “increasing complexity” [84]. Informants have helped us to see how, when we examine the simple concept of “ground truth,” we discover the complexity of their collective practices, and the strategies of their thought processes. These complexities lead to implications which may be of interest to
Through discussions with informants, we propose potential improvements to the tools of labeling, and also the work-practices that can be supported through those tools.
5.1.1 Smarter Labeling UIs. Future labeling UIs could include a number of features that could be responsive to the labeler, and that could provide assistance. For example, I-02 noted ”there’s something fshy going on” in her data. Over time, an intelligent labeling tool could learn the defnition of ”fshy,” and could help labelers to set ”fshy thresholds” that could determine which records were automatically brought to their attention as potential problems.
We may learn that entire data science teams can agree on their local defnition of “fshy;” or we may learn that data science workers in diferent roles focus on diferent data attributes, and thus have their own distinct and role-specifc defnitions of “fshy.” It seems likely that a domain expert might apply a diferent set of criteria for “fshiness” as contrasted with a statistician. How can humans teach their skills to an AI agent, so that the agent can bring role-specifc “fshy” candidates to the human for inspection? Does the AI agent need to have a model of the roles of the humans, so that each ”fshy” instance can be brought to an appropriate human for discernment?
In addition to providing insights into “fshiness,” I-02 also mentioned the problem of a labeler’s cognitive exhaustion. An intelligent labeling UI could contain a private user model for each labeler, possibly including the labeler’s history with other labeling tasks. The UI could help the labeler to track their own personal performance, recommending breaks or other refreshing activities when there was evidence of exhaustion. A more sophisticated possibility is that the labeling UI could actively adjust the degree of difculty of the next item-to-be-labeled, based on an analysis of the labeler’s cognitive state and the labeler’s prior history with similar items.
5.1.2 Social Labeling UIs. A socially-intelligent labeling tool could provide awareness services related to other labelers - something that is seldom done in crowdmarkets other than in the work of Von Ahn [113]. These services could include an indication of the individual labeler’s comparative performance, which could be anonymized so as to show that labeler’s performance in relation to unnamed other labelers, following principles of social translucence (e.g., [28, 37, 63]). This kind of design would allow self-calibration in relation to peers, or in more dramatic applications, it could become a means of gamifcation.
A second type of social awareness could be implemented for cases in which two or more labelers work on the same record, so as to achieve higher quality through comparisons of their labels. In this case, early detection of conficts in labels could be moved immediately to a queue of items to be resolved between or among the labelers who had contributed labels to the same record [20, 51, 90, 117, 118]. With good user modeling, it may be possible to predict conficts before they actually occur, and to help labelers consult with one another to prevent or “pre-resolve” conficts [25].
5.1.3 Labeling Provenance. We have shown the evolving complexities of ground-truth labeling. However, these complexities leave few marks on the data [122]. Labels seldom contain their own histories or provenances.6 Further, we know that data science workers tend 6In this paper, we use the word provenance in its conventional or arts/museum/Indigeneous meaning, which emphasizes maintaining a trace of human ownership and human action related to an artifact [36, 94, 110]. We are not using this word in the more specialized sense of software-script provenance, such as in noWorkfow and YesWorkfow [76], ReproduceMeGit [89], and Provbook [88]. Those technical usages generally do not involve a history of human actors, whereas the human actions are crucial for our intended meaning in this paper.
not to document their work as they are doing it [55, 86], and that measurement plans (which might shape or the labeling process) often go unrecorded [77] - despite Fort’s advice to develop and continually use a detailed set of labeling guidelines [33]. As Kery et al. showed, some of the obstacles to documentation appear to be based in the tools themselves [55], while other obstacles appear to be due to the exploratory work-practices that characterize important parts of data science practice [86].
Not all of these provenance problems can be solved through technology, but some features could surely help. Once applied, labels appear in a dataset without label-metadata. Similarly to Kery et al.’s Verdant versioning-support tool [54], we propose that the preservation of labeling histories could help to reconstruct how a particular label was applied to a particular record. Simple metadata could include a timestamp and a link to the labeling vocabulary that was available at the time of labeling - i.e., the labeling-structuralcontext that governed the labeling activity. An equally valuable type of metadata would be the identity of the labeler, who might be able to provide insights about how they assigned that label - i.e., the labeling-social-context.
Disagreements among labelers could also be recorded with their resolutions. These metadata could also be used analytically when needed, to examine other labels that were assigned during the same timeframe, or that were assigned by the same labeler. These and other label-metadata would help to reveal what might be called informally ”the truth beneath the ground truth”7 - i.e., the located accountabilities (e.g., [73, 106]) around the creation and refnement of labels. In much contemporary labeling practice, these accountabilities would involve humans as actors. With enhanced “human-andAI-in-the-loop” algorithms [24, 125] these accountabilities could also involve computational actors (e.g., [25, 95]).
In most data science projects, one or more people create groundtruth labels through a process that subsequently fades into invisibility. Because the term of art implies these labels are neutral descriptions of reality - i.e., as “ground truth” - we tend to forget that there was a series of human actions that led to creating what now appears self-evidently as “truth.” The labels stand as rationalized data attributes, while the informal articulation work that was done to produce them, becomes less and less visible [47, 92, 100, 101].
Earlier, we introduced Feinberg’s work on the design of data ([30]; see also the work of Seidelin [96, 97] and Tanweer [109]). Muller et al. showed that their concepts of design-of-data could be applied to the analysis of data science activities as a kind of craft work, in which the data science worker uses tools analogous to weavers’ looms and potters’ wheels to engage with and shape the data [69]. If, as Feinberg, Seidelin, and Tanweer argue, data can be designed, then the analysis of Muller et al. suggests that we should pay attention to the tools through which "data designers" (i.e., data science workers) pursue their under-acknowledged crafting of data. In this regard, we may want to revisit Schön’s work on "designing
7Elaborating on Rich’s characterization, we acknowledge that any claims to “truth” should be viewed as a layered complexity, with “truths beneath truths beneath truths” - see also Bateson’s concept of an “infnite regress of relationships” [11].
as refective conversation with the materials of a design situation" [93] - or, in the succinct summary of Bean and Rosner, "materials matter" [12].
Feinberg also provided a useful commentary in her paper on ”material vision,” in which she pointed out that we can choose how we approach data and related concepts ([31]; see also [72, 103]). She described how we change the conceptual signifcance of an item by changing the characteristics that we attribute to it. Applying her ideas to the labeling process, we propose that:
• For a labeler, each record presents a new challenge. The labeler takes a record-centric view, and solves each labeling challenge. • Subsequently, for a modeler, each dataset presents a new challenge. The modeler takes a dataset-centric view, and solves each modeling challenge.
Applying the concepts of Feinberg’s analysis, the labeler chooses to focus on labels as the materials that they work with (see also [96, 97]), while the modeler chooses to focus on datasets as the materials that they work with. In the transition of the work from labeler to modeler, focus is a key choice.8 As the focus of the data science team shifts from one activity to the next, the perceptual status of the labels changes from foreground to background, from structure to infrastructure [31], from visible to invisible [100]. Correspondingly, the articulation practices that we have described here, also fade from foreground to background [102]. As the modeler continues with their work - and as the client waits impatiently for the results of the model - it is easy to forget that someone sweated the labels.
The shift in perspective that Feinberg described, may also be analyzed in Star’s infrastructure approach as a shift in analytic focus (e.g., [101]), which in our case translates into a shift in both data granularity and work practices required to work at that level of granularity. As Star notes, when a concept enters an infrastructure, it can easily become a ”boring thing” [100]. For a team of data scientists who are intent on producing a functioning model or pipeline, ground truth labels and the work-practices that produce them, tend to disappear into the data infrastructure [30] – along with other ”boring things" that are necessary, but unexciting. We recall I-01 and I-11, who reported that labeling was ”inherently boring” (I-11) and indeed ”mind-numbingly boring” (I-01).
If we want to bring attention and accountability to the processes resulting in ground truth, we need to fnd ways of again shifting focus, so that labels become a worthy topic of analysis and of human efort. Bowker described a similar analytic strategy as infrastructural inversion [15]. Our paper has attempted a similar shift in analytic focus, showing how collaboration and division-of-labor make important contributions to the development of ground truth, raising questions of data and process quality, and focusing on three distinct classes of processes in the design of ground truth labels (Section 4.7). Based on the difcult and thoughtful work reported by informants, we assert not only that labels are ”interesting things,”
8Laudan has made a similar argument about scientifc progress as a matter of solving - i.e., attending to - specifc aspects of scientifc problems [59].
but that the people and practices that give rise to labels are also interesting sites of our scholarly attention.
In the context of machine learning pipelines, labeling work tends to occur as a series of acts of layering, in which each layer renders the layer below it invisible. Star and Strauss eloquently made the general case for invisibility in their paper "Layers of silence, arenas of voice" [102], and we apply the silences part of their argument here, as a series of consequences of shifts in our focus when we work in data science. The original work of discovering or capturing a dataset [69] tends to become invisible as we strive to make the data ft for purpose - i.e., ready for analysis. We do this through acts of designing the data [30], applying ground truth labels to the records (this paper), and curating records in the dataset [65]. Each of these necessary actions adds another layer of value ”on top” of the original dataset. However each layer reduces the visibility of the original data, and crucially, each layer reduces the visibility of the activities of the layer beneath it. If problems arise subsequently in the analysis, the multiple invisibilities imposed by these layers may interfere with our ability to repair problems that we inadvertently introduced in the earlier steps of our analysis, as we argued above in Section 5.1.3 on “Labeling Provenance.”
This paper describes research in a single company in a single national culture. Of necessity in interview studies, we considered fewer than 20 projects, and we considered those projects at a moment-in-time in the year 2020, during a period of rapid development of data science concepts, work-practices, and technologies.
We also note that the projects had more of a research emphasis than a development emphasis. Nonetheless, even in the research projects, we found that pragmatic considerations imposed constraints on how ground truth was created and refned. We hypothesize that pragmatic constraints may be even more impactful in a development, product, or service context. We note that informants who were working closer to a “client-facing” project, had a tendency to report less complete labels and less complete labeling strategies. In a small-n interview study, we do not want to claim that this was a causal relationship. We mention it as a possibility for future research about how organizational settings may afect ground truth practices.
Finally, we revisit the issue of the partial view. In Section 3.2, we noted that, like other industry research settings, IBM’s structure and business model prevented us from interviewing the labelers themselves. Thus, this paper provides only one view into labeling phenomena and practices. In the future, we hope to fnd ways to gain direct access to the labelers, for a more complete view.
In this paper, we explored the collaborative work-practices of teams that engage in the important work of ground-truth labeling for machine learning. We believe our principal contributions are as follows;
• Experiences of ground-truth labeling • Collaborative practices in ground-truth labeling, especially focusing on division-of-labor during the creation of labels
and the administration of diverse labeling processes and teams • Quality issues that arise during labeling, and mitigations through collaboration, division-of-labor, and the administrative control through fxed labeling vocabularies and documentation • Three practices in the collaborative design of ground truth • Integration with work in Human Centered Data Science, focusing on human practices with data • Proposals for technology developments that may aid labelers and their organizations
We shape our data science analyses, and the impact of our analyses on the world, through our design of data, including ground-truth labels. We hope that this paper will help us to understand our shared human accountability in the increasing complexities that we create.
We thank Werner Geyer and Darrell Reimer for advice and recommendations to fnd labeling informants.
[1] José Manuel Álvarez and Antonio Lopez. 2008. Novel index for objective evalu-
ation of road detection algorithms. In 2008 11th International IEEE Conference on Intelligent Transportation Systems. IEEE, 815–820. [2] Amol Ambardekar, Mircea Nicolescu, and Sergiu Dascalu. 2009. Ground truth verifcation tool (GTVT) for video surveillance systems. In 2009 Second International Conferences on Advances in Computer-Human Interactions. IEEE, 354–359. [3] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014. Power to the people: The role of humans in interactive machine learning. Ai Magazine 35, 4 (2014), 105–120. [4] Theresa Dirndorfer Anderson and Nicola Parker. 2019. Keeping the human in the data scientist: Shaping human-centered data science education. Proceedings of the Association for Information Science and Technology 56, 1 (2019), 601–603. [5] Josh Andres, Christine T Wolf, Sergio Cabrero Barros, Erick Oduor, Rahul Nair, Alexander Kjærum, Anders Bech Tharsgaard, and Bo Schwartz Madsen. 2020. Scenario-based XAI for Humanitarian Aid Forecasting. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems Extended Abstracts. 1–8. [6] Cecilia Aragon, Clayton Hutto, Andy Echenique, Brittany Fiore-Gartland, Yun Huang, Jinyoung Kim, Gina Nef, Wanli Xing, and Joseph Bayer. 2016. Developing a research agenda for human-centered data science. In Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion. ACM, 529–535. [7] Sinem Aslan, Sinem Emine Mete, Eda Okur, Ece Oktay, Nese Alyuz, Utku Ergin Genc, David Stanhill, and Asli Arslan Esme. 2017. Human expert labeling process (HELP): towards a reliable higher-order user state labeling process and tool to assess student engagement. Educational Technology (2017), 53–59. [8] Catherine M Baker, Lauren R Milne, and Richard E Ladner. 2019. Understanding the Impact of TVIs on Technology Use and Selection by Children with Visual Impairments. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–13. [9] Sarah Elsie Baker and Rosalind Edwards. 2012. How many qualitative interviews is enough? Expert voices and early career refections on sampling and cases in qualitative research. (2012). [10] Shaowen Bardzell, Daniela K Rosner, and Jefrey Bardzell. 2012. Crafting quality in design: integrity, creativity, and public sensibility. In Proceedings of the Designing Interactive Systems Conference. 11–20. [11] Gregory Bateson. 2000. Steps to an ecology of mind: Collected essays in anthropology, psychiatry, evolution, and epistemology. University of Chicago Press. [12] Jonathan Bean and Daniela Rosner. 2012. Old hat: craft versus design? interactions 19, 1 (2012), 86–88. [13] Michael S Bernstein, Greg Little, Robert C Miller, Björn Hartmann, Mark S Ackerman, David R Karger, David Crowell, and Katrina Panovich. 2010. Soylent: a word processor with a crowd inside. In Proceedings of the 23nd annual ACM symposium on User interface software and technology. 313–322. [14] Daniel Bertaux. 1981. From the life-history approach to the transformation of sociological practice. Biography and society: The life history approach in the social sciences (1981), 29–45.
[15] Geofrey C Bowker, C Geofrey, W Bernard Carlson, et al. 1994. Science on the run: Information management and industrial geophysics at Schlumberger, 1920-1940. MIT press. [16] Jenna Breckenridge and Derek Jones. 2009. Demystifying theoretical sampling in grounded theory research. Grounded Theory Review 8, 2 (2009). [17] Vitor R Carvalho, Matthew Lease, and Emine Yilmaz. 2011. Crowdsourcing for search evaluation. In ACM Sigir forum, Vol. 44. ACM New York, NY, USA, 17–22. [18] Kathy Charmaz. 2014. Constructing grounded theory. sage. [19] Kathy Charmaz and Antony Bryant. 2011. Grounded theory and credibility.
Qualitative research 3 (2011), 291–309. [20] Veronika Cheplygina and Josien PW Pluim. 2018. Crowd disagreement about
medical images is informative. In Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis. Springer, 105–111. [21] Ming Cheung, James She, and Xiaopeng Li. 2015. Non-user generated annotation on user shared images for connection discovery. In 2015 IEEE International Conference on Data Science and Data Intensive Systems. IEEE, 204–209. [22] Juliet Corbin and Anselm Strauss. 2014. Basics of qualitative research: Techniques and procedures for developing grounded theory. Sage publications. [23] Jonathan Corney, Andrew Lynn, Carmen Torres, Paola Di Maio, William Regli, Graeme Forbes, and Lynne Tobin. 2010. Towards crowdsourcing translation tasks in library cataloguing, a pilot study. In 4th IEEE International Conference on Digital Ecosystems and Technologies. IEEE, 572–577. [24] Frederick J Damerau, David E Johnson, and Martin C Buskirk Jr. 2004. Automatic labeling of unlabeled text data. US Patent 6,697,998. [25] Michael Desmond, Kristina Brimijoin, Evelyn Duesterwald, Narendra Nath Joshi, Michael Muller, Zahra Ashktorab, Aabhas Sharma, Casey Dugan, and Qian Pan. 2020. AI=Assisted Data Labeling. Demo at NeurIPS 2020. [26] Christian Dietz and Michael R Berthold. 2016. KNIME for open-source bioimage analysis: a tutorial. In Focus on Bio-Image Informatics. Springer, 179–197. [27] Shari L Dworkin. 2012. Sample size policy for qualitative studies using in-depth interviews. [28] Thomas Erickson and Wendy A Kellogg. 2003. Social translucence: using minimalist visualisations of social activity to support collective interaction. In Designing information spaces: The social navigation approach. Springer, 17–41. [29] Hao Fang, Hao Cheng, Maarten Sap, Elizabeth Clark, Ari Holtzman, Yejin Choi, Noah A Smith, and Mari Ostendorf. 2018. Sounding board: A user-centric and content-driven social chatbot. arXiv preprint arXiv:1804.10202 (2018). [30] Melanie Feinberg. 2017. A design perspective on data. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 2952–2963. [31] Melanie Feinberg. 2017. Material Vision. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. 604–617. [32] P. M. Ferreira, T. Mendonça, J. Rozeira, and P. Rocha. 2012. An Annotation Tool for Dermoscopic Image Segmentation. In Proceedings of the 1st International Workshop on Visual Interfaces for Ground Truth Collection in Computer Vision Applications (Capri, Italy) (VIGTA ’12). Association for Computing Machinery, New York, NY, USA, Article 5, 6 pages. https://doi.org/10.1145/2304496.2304501 [33] Karën Fort. 2016. Collaborative Annotation for Reliable Natural Language Processing: Technical and Sociological Aspects. John Wiley & Sons. [34] Susan Gasson and Jim Waters. 2013. Using a grounded theory approach to study online collaboration behaviors. European Journal of Information Systems 22, 1 (2013), 95–118. [35] Elihu M Gerson and Susan Leigh Star. 1986. Analyzing due process in the workplace. ACM Transactions on Information Systems (TOIS) 4, 3 (1986), 257– 270. [36] Patty Gerstenblith. 2020. Provenience and Provenance Intersecting with International Law in the Market for Antiquities. NCJ Int’l L. 45 (2020), 457. [37] Eric Gilbert. 2012. Designing social translucence over social networks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2731–2740. [38] Lisa Gitelman. 2013. Raw data is an oxymoron. MIT press. [39] Barney G Glaser and Anselm L Strauss. 2017. Discovery of grounded theory:
Strategies for qualitative research. Routledge. [40] Michele Goetz. 2017. 3 Ways Data Preparation Tools Help You
Get Ahead Of Big Data. "https://go.forrester.com/blogs/15-02-173_ways_data_preparation_tools_help_you_get_ahead_of_big_data/". [41] Charles Goodwin. 2000. Practices of color classifcation. Mind, culture, and activity 7, 1-2 (2000), 19–36. [42] Catherine Grady and Matthew Lease. 2010. Crowdsourcing document relevance assessment with mechanical turk. In Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazon’s mechanical turk. Association for Computational Linguistics, 172–179. [43] Mary L Gray and Siddharth Suri. 2019. Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass. Eamon Dolan Books. [44] Ben Green. 2018. Data science as political action: grounding data science in a politics of justice. arXiv preprint arXiv:1811.03435 (2018).
[45] Greg Guest, Arwen Bunce, and Laura Johnson. 2006. How many interviews are enough? An experiment with data saturation and variability. Field methods 18, 1 (2006), 59–82. [46] Philip J Guo, Sean Kandel, Joseph M Hellerstein, and Jefrey Heer. 2011. Proactive wrangling: Mixed-initiative end-user programming of data transformation scripts. In Proceedings of the 24th annual ACM symposium on User interface software and technology. 65–74. [47] Ian Hampson and Anne Junor. 2005. Invisible work, invisible skills: interactive customer service as articulation work. New Technology, Work and Employment 20, 2 (2005), 166–181. [48] Kotaro Hara, Vicki Le, and Jon Froehlich. 2013. Combining crowdsourcing and google street view to identify street-level accessibility problems. In Proceedings of the SIGCHI conference on human factors in computing systems. 631–640. [49] Tony Hey, Stewart Tansley, Kristin Tolle, et al. 2009. The fourth paradigm: data-intensive scientifc discovery. Vol. 1. Microsoft research Redmond, WA. [50] Humayun Irshad, Eun-Yeong Oh, Daniel Schmolze, Liza M Quintana, Laura Collins, Rulla M Tamimi, and Andrew H Beck. 2017. Crowdsourcing scoring of immunohistochemistry images: Evaluating performance of the crowd and an automated computational method. Scientifc reports 7 (2017), 43286. [51] Narendra Nath Joshi, Aabhas Sharma, , Michael Muller, Qian Pan, Michael Desmond, Kristina Brimijoin, Zahra Ashktorab, Evelyn Duesterwald, and Casey Dugan. 2020. Fast and Automatic Visual Label Confict Resolution. Demo at NeurIPS 2020. [52] Hiroshi Kajino, Yuta Tsuboi, Issei Sato, and Hisashi Kashima. 2012. Learning from crowds and experts. In Workshops at the Twenty-Sixth AAAI Conference on Artifcial Intelligence. [53] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jefrey Heer. 2011. Wrangler: Interactive visual specifcation of data transformation scripts. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 3363–3372. [54] Mary Beth Kery, Bonnie E John, Patrick O’Flaherty, Amber Horvath, and Brad A Myers. 2019. Towards Efective Foraging by Data Scientists to Find Past Analysis Choices. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–13. [55] Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. The story in the notebook: Exploratory data science using a literate programming tool. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1–11. [56] Rafal Kocielnik, Lillian Xiao, Daniel Avrahami, and Gary Hsieh. 2018. Refection companion: a conversational system for engaging users in refection on physical activity. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2, 2 (2018), 1–26. [57] Marina Kogan, Aaron Halfaker, Shion Guha, Cecilia Aragon, Michael Muller, and Stuart Geiger. 2020. Mapping Out Human-Centered Data Science: Methods, Approaches, and Best Practices. In Companion of the 2020 ACM International Conference on Supporting Group Work. 151–156. [58] Scott Krig. 2016. Ground truth data, content, metrics, and analysis. In Computer Vision Metrics. Springer, 247–271. [59] Larry Laudan. 1978. Progress and its problems: Towards a theory of scientifc growth. Vol. 282. Univ of California Press. [60] Dong-Hyun Lee. 2013. Pseudo-label: The simple and efcient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, Vol. 3. 2. [61] Diana Lynn MacLean and Jefrey Heer. 2013. Identifying medical terms in patient-authored text: a crowdsourcing-based approach. Journal of the american medical informatics association 20, 6 (2013), 1120–1127. [62] Mohd Alif Abdul Majid, Mohhidin Othman, Siti Fatimah Mohamad, and Sarina Abdul Halim Lim. 2018. Achieving data saturation: evidence from a qualitative study of job satisfaction. Social and Management Research Journal 15, 2 (2018), 66–77. [63] David W McDonald, Stephanie Gokhman, and Mark Zachry. 2012. Building for social translucence: a domain analysis and prototype system. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 637–646. [64] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–23. [65] Helena M Mentis, Ahmed Rahim, and Pierre Theodore. 2016. Crafting the image in surgical telemedicine. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing. 744–755. [66] Janice M Morse. 1995. The signifcance of saturation. [67] Michael Muller. 2014. Curiosity, creativity, and surprise as analytic tools:
Grounded theory method. In Ways of Knowing in HCI. Springer, 25–48. [68] Michael Muller, Melanie Feinberg, Timothy George, Steven J Jackson, Bonnie E
John, Mary Beth Kery, and Samir Passi. 2019. Human-Centered Study of Data Science Work Practices. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, W15. [69] Michael Muller, Ingrid Lange, Dakuo Wang, David Piorkowski, Jason Tsay, Q Vera Liao, Casey Dugan, and Thomas Erickson. 2019. How Data Science
Workers Work with Data: Discovery, Capture, Curation, Design, Creation. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–15. [70] Gina Nef, Anissa Tanweer, Brittany Fiore-Gartland, and Laura Osburn. 2017. Critique and contribute: A practice-based framework for improving critical data studies and data science. Big data 5, 2 (2017), 85–97. [71] Naveen Onkarappa and Angel D Sappa. 2015. Synthetic sequences and groundtruth fow feld generation for algorithm validation. Multimedia Tools and Applications 74, 9 (2015), 3121–3135. [72] Samir Passi and Steven Jackson. 2017. Data vision: Learning to see through algorithmic abstraction. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. 2436–2447. [73] Samir Passi and Steven J Jackson. 2018. Trust in Data Science: Collaboration, Translation, and Accountability in Corporate Data Science Projects. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–28. [74] Kanu Patel, Jay Vala, and Jaymit Pandya. 2014. Comparison of various classifcation algorithms on iris datasets using WEKA. Int. J. Adv. Eng. Res. Dev.(IJAERD) 1, 1 (2014). [75] Sharoda A Paul, Lichan Hong, and Ed H Chi. 2011. What is a question? Crowdsourcing tweet categorization. In Workshop on Crowdsourcing and Human Computation at the Conference on Human Factors in Computing Systems (CHI). [76] João Felipe Pimentel, Saumen Dey, Timothy McPhillips, Khalid Belhajjame, David Koop, Leonardo Murta, Vanessa Braganholo, and Bertram Ludäscher. 2016. Yin & Yang: demonstrating complementary provenance from noWorkfow & YesWorkfow. In International Provenance and Annotation Workshop. Springer, 161–165. [77] Kathleen H Pine and Max Liboiron. 2015. The politics of measurement and action. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. 3147–3156. [78] Ivens Portugal, Paulo Alencar, and Donald Cowan. 2018. The use of machine learning algorithms in recommender systems: A systematic review. Expert Systems with Applications 97 (2018), 205–227. [79] Alisha Pradhan, Ben Jelen, Katie A Siek, Joel Chan, and Amanda Lazar. 2020. Understanding Older Adults’ Participation in Design Workshops. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–15. [80] Krishna Rajan. 2013. Informatics for materials science and engineering: datadriven discovery for accelerated experimentation and application. ButterworthHeinemann. [81] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. 2020. Snorkel: Rapid training data creation with weak supervision. The VLDB Journal 29, 2 (2020), 709–730. [82] Tye Rattenbury, Joseph M Hellerstein, Jefrey Heer, Sean Kandel, and Connor Carreras. 2017. Principles of data wrangling: Practical techniques for data preparation. " O’Reilly Media, Inc.". [83] Johan Redström. 2008. RE: Defnitions of use. Design studies 29, 4 (2008), 410–423. [84] Adrienne Rich. 1995. On lies, secrets, and silence: Selected prose 1966-1978. WW Norton & Company. [85] Yuji Roh, Geon Heo, and Steven Euijong Whang. 2019. A survey on data collection for machine learning: a big data-ai integration perspective. IEEE Transactions on Knowledge and Data Engineering (2019). [86] Adam Rule, Aurélien Tabard, and James D Hollan. 2018. Exploration and explanation in computational notebooks. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. 1–12. [87] Manaswi Saha, Michael Saugstad, Hanuma Teja Maddali, Aileen Zeng, Ryan Holland, Steven Bower, Aditya Dash, Sage Chen, Anthony Li, Kotaro Hara, et al. 2019. Project sidewalk: A web-based crowdsourcing tool for collecting sidewalk accessibility data at scale. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–14. [88] Sheeba Samuel and Birgitta König-Ries. 2018. ProvBook: Provenance-based Semantic Enrichment of Interactive Notebooks for Reproducibility.. In International Semantic Web Conference (P&D/Industry/BlueSky). [89] Sheeba Samuel and Birgitta König-Ries. 2020. ReproduceMeGit: A Visualization Tool for Analyzing Reproducibility of Jupyter Notebooks. arXiv preprint arXiv:2006.12110 (2020). [90] Mike Schaekermann, Graeme Beaton, Minahz Habib, L. I.M. Andrew, Kate Larson, and L. A.W. Edith. 2019. Understanding expert disagreement in medical data analysis through structured adjudication. , 23 pages. https://doi.org/10. 1145/3359178 [91] Mike Schaekermann, Carrie J Cai, Abigail E Huang, and Rory Sayres. 2020. Expert Discussions Improve Comprehension of Difcult Cases in Medical Image Assessment. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–13. [92] Kjeld Schmidt. 2002. Remarks on the complexity of cooperative work. Revue d’intelligence artifcielle 16, 4-5 (2002), 443–483. [93] Donald A Schön. 1992. Designing as refective conversation with the materials of a design situation. Knowledge-based systems 5, 1 (1992), 3–14.
CHI ’21, May 8–13, 2021, Yokohama, Japan
[94] Philipp Schorch. 2020. Sensitive Heritage: Ethnographic Museums, Provenance Research, and the Potentialities of Restitutions. Museum and Society 18, 1 (2020), 1–5. [95] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna de Vreede, Gert-Jan De Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah Oeste-Reiß, Nils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in team collaboration. Information & management 57, 2 (2020), 103174. [96] Cathrine Seidelin, Yvonne Dittrich, and Eric Grönvall. [n.d.]. Co-designing data experiments. ([n. d.]). (in preparation). [97] Cathrine Seidelin, Yvonne Dittrich, and Erik Grönvall. 2018. Data Work in a Knowledge-Broker Organisation: How Cross-Organisational Data Maintenance Shapes Human Data Interactions. In Proceedings of the 32nd International BCS Human Computer Interaction Conference (Belfast, United Kingdom) (HCI ’18). BCS Learning & Development Ltd., Swindon, GBR, Article 14, 12 pages. https: //doi.org/10.14236/ewic/HCI2018.14 [98] Burr Settles. 2009. Active learning literature survey. Technical Report. University of Wisconsin-Madison Department of Computer Sciences. [99] Ayush Singhal, Pradeep Sinha, and Rakesh Pant. 2017. Use of deep learning in modern recommendation system: A summary of recent works. arXiv preprint arXiv:1712.07525 (2017). [100] Susan Leigh Star. 1999. The ethnography of infrastructure. American behavioral scientist 43, 3 (1999), 377–391. [101] Susan Leigh Star and Karen Ruhleder. 1996. Steps toward an ecology of infrastructure: Design and access for large information spaces. Information systems research 7, 1 (1996), 111–134. [102] Susan Leigh Star and Anselm Strauss. 1999. Layers of silence, arenas of voice: The ecology of visible and invisible work. Computer supported cooperative work (CSCW) 8, 1-2 (1999), 9–30. [103] Stephanie B Steinhardt and Steven J Jackson. 2015. Anticipation work: Cultivating vision in collective practice. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing. 443–453. [104] P.N. Stern. 2007. Properties for growing grounded theory. In The Sage handbook of grounded theory, A. Bryant and K. Charmaz (Eds.). Sage, Thousand Oaks, CA, USA. [105] Miriam Sturdee, John Hardy, Nick Dunn, and Jason Alexander. 2015. A public ideation of shape-changing applications. In Proceedings of the 2015 International Conference on Interactive Tabletops & Surfaces. 219–228. [106] Lucy Suchman. 2002. Located accountabilities in technology production. Scandinavian journal of information systems 14, 2 (2002), 7. [107] Charles Sutton, Timothy Hobson, James Geddes, and Rich Caruana. 2018. Data dif: Interpretable, executable summaries of changes in distributions for data wrangling. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2279–2288. [108] Madhusmita Swain, Sanjit Kumar Dash, Sweta Dash, and Ayeskanta Mohapatra. 2012. An approach for iris plant classifcation using neural network. International Journal on Soft Computing 3, 1 (2012), 79. [109] Anissa Tanweer. 2018. Data science of the social: How the practice is responding to ethical crisis and spreading across sectors. Ph.D. Dissertation. [110] Natalia Tognoli and José Augusto Chaves Guimarães. 2020. Provenance as a Knowledge Organization Principle. KO KNOWLEDGE ORGANIZATION 46, 7 (2020), 558–568. [111] Wil MP Van der Aalst. 2014. Data scientist: The engineer of the future. In Enterprise interoperability VI. Springer, 13–26. [112] Jesper E Van Engelen and Holger H Hoos. 2020. A survey on semi-supervised learning. Machine Learning 109, 2 (2020), 373–440. [113] Luis Von Ahn. 2008. Human computation. In 2008 IEEE 24th international conference on data engineering. IEEE, 1–2. [114] Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. Human-AI Collaboration in Data Science. Proceedings of the ACM on HumanComputer Interaction 3, CSCW (Nov 2019), 1–24. https://doi.org/10.1145/ 3359313 [115] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. Human-AI Collaboration in Data Science: Exploring Data Scientists’ Perceptions of Automated AI. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–24. [116] Daniel Karl I Weidele, Justin D Weisz, Erick Oduor, Michael Muller, Josh Andres, Alexander Gray, and Dakuo Wang. 2020. AutoAIViz: opening the blackbox of automated artifcial intelligence with conditional parallel coordinates. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 308–312. [117] Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference. 2035–2043. [118] Andrea Wiggins, Greg Newman, Robert D Stevenson, and Kevin Crowston. 2011. Mechanisms for data quality and validation in citizen science. In 2011
Trovato and Tobin, et al.
IEEE Seventh International Conference on e-Science Workshops. IEEE, 14–19. [119] Peter Woitek, Paul Bräuer, and Holger Grossmann. 2010. A Novel Tool for Cap-
turing Conceptualized Audio Annotations (AM ’10). Association for Computing Machinery, New York, NY, USA, Article 15, 8 pages. https://doi.org/10.1145/ 1859799.1859814 [120] Christine T. Wolf. 2019. Conceptualizing Care in the Everyday Work Practices of Machine Learning Developers. In Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion (San Diego, CA, USA) (DIS ’19 Companion). Association for Computing Machinery, New York, NY, USA, 331–335. https://doi.org/10.1145/3301019.3323879 [121] Christine T Wolf. 2020. AI Models and Their Worlds: Investigating Data-Driven, AI/ML Ecosystems Through a Work Practices Lens. In International Conference on Information. Springer, 651–664. [122] Matthew Yapchain. 2018. Human-Centered Data Science: A New Paradigm for Industrial IoT. In Ethnographic Praxis in Industry Conference Proceedings, Vol. 2018. Wiley Online Library, 53–61. [123] Amy X Zhang, Michael Muller, and Dakuo Wang. 2020. How do Data Science Workers Collaborate? Roles, Workfows, and Tools. arXiv preprint arXiv:2001.06684 (2020). [124] Amy X. Zhang, Michael Muller, and Dakuo Wang. 2020. How do Data Science Workers Collaborate? Roles, Workfows, and Tools. In Proc. ACM Hum.-Comput. Interact. Article 22. Issue CSCW1. [125] Lei Zhang, Yan Tong, and Qiang Ji. 2008. Active image labeling and its application to facial action labeling. In European Conference on Computer Vision. Springer, 706–719. [126] Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. (2002). [127] Laszlo Zsolnai. 1998. Rational choice and the diversity of choices. The Journal of Socio-Economics 27, 5 (1998), 613–622.
